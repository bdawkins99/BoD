\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces {\bf  Imposters vs true neighbors in the presence of interactions with three variables}. Attributes A, B, and C have no main effect. (\textbf  {\fontfamily  {ptm}\selectfont  I}) Scatter plot of simulated irrelevant Attribute C with a functional Attribute A. (\textbf  {\fontfamily  {ptm}\selectfont  II}) Scatter plot of Attributes A and B, which interact through differential correlation. Computing nearest neighbors with irrelevant attributes (\textbf  {\fontfamily  {ptm}\selectfont  I}) or lower dimensions leads to imposter nearest neighbors and degrades the ability of Relief-based methods to identify interaction effects. Computing distances in only these two dimensions leads to an imposter false miss (FM) for the nearest neighbor from the opposite outcome class for target instance X. This imposter leads to Attribute A predicting closer projected distances for misses than hits (H), which incorrectly indicates that A is a poor discriminator (yellow boxes in \textbf  {\fontfamily  {ptm}\selectfont  I}). (\textbf  {\fontfamily  {ptm}\selectfont  III-IV}) Computing nearest neighbors in higher dimensions or with the correct interaction partner leads to imposter nearest neighbor (FM) being replaced by the true nearest miss neighbor (TM) for target instance X, which correctly leads to Attribute A predicting closer projected distances for hits (H) than misses, which is an indication that Attribute A is a good discriminator (yellow boxes \textbf  {\fontfamily  {ptm}\selectfont  II}).\relax }}{2}{figure.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ABC}{{1}{2}{{\bf Imposters vs true neighbors in the presence of interactions with three variables}. Attributes A, B, and C have no main effect. (\textbf {\fontfamily {ptm}\selectfont I}) Scatter plot of simulated irrelevant Attribute C with a functional Attribute A. (\textbf {\fontfamily {ptm}\selectfont II}) Scatter plot of Attributes A and B, which interact through differential correlation. Computing nearest neighbors with irrelevant attributes (\textbf {\fontfamily {ptm}\selectfont I}) or lower dimensions leads to imposter nearest neighbors and degrades the ability of Relief-based methods to identify interaction effects. Computing distances in only these two dimensions leads to an imposter false miss (FM) for the nearest neighbor from the opposite outcome class for target instance X. This imposter leads to Attribute A predicting closer projected distances for misses than hits (H), which incorrectly indicates that A is a poor discriminator (yellow boxes in \textbf {\fontfamily {ptm}\selectfont I}). (\textbf {\fontfamily {ptm}\selectfont III-IV}) Computing nearest neighbors in higher dimensions or with the correct interaction partner leads to imposter nearest neighbor (FM) being replaced by the true nearest miss neighbor (TM) for target instance X, which correctly leads to Attribute A predicting closer projected distances for hits (H) than misses, which is an indication that Attribute A is a good discriminator (yellow boxes \textbf {\fontfamily {ptm}\selectfont II}).\relax }{figure.1}{}}
\citation{npdr}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Interacting features A and B in the presence of noise. (\textbf  {I}) Standardized beta coefficients of interacting features A and B and irrelevant features plotted vs the total number of features $p$. As more irrelevant features are added to the simulated interacting features A and B, beta coefficients decrease quickly. At about 200 features (198 irrelevant), A and B have beta coefficients close to 0. (\textbf  {II}) P values corresponding to standardized beta coefficients of interacting features A and B and irrelevant features plotted vs the total number of features $p$. As more irrelevant features are added, P values quickly approach 1.\relax }}{3}{figure.2}\protected@file@percent }
\newlabel{fig:ABC_plus_noise}{{2}{3}{Interacting features A and B in the presence of noise. (\textbf {I}) Standardized beta coefficients of interacting features A and B and irrelevant features plotted vs the total number of features $p$. As more irrelevant features are added to the simulated interacting features A and B, beta coefficients decrease quickly. At about 200 features (198 irrelevant), A and B have beta coefficients close to 0. (\textbf {II}) P values corresponding to standardized beta coefficients of interacting features A and B and irrelevant features plotted vs the total number of features $p$. As more irrelevant features are added, P values quickly approach 1.\relax }{figure.2}{}}
\citation{urbanowicz17}
\citation{mckinney13}
\citation{stir}
\@writefile{toc}{\contentsline {section}{\numberline {1}Neighborhood methods}{4}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Derivation of expected $k$ for MultiSURF neighborhoods}{4}{section.2}\protected@file@percent }
\newlabel{sec:expected-k}{{2}{4}{Derivation of expected \texorpdfstring {$k$}{} for MultiSURF neighborhoods}{section.2}{}}
\newlabel{eq:D}{{1}{4}{Derivation of expected \texorpdfstring {$k$}{} for MultiSURF neighborhoods}{equation.2.1}{}}
\newlabel{eq:diff}{{2}{4}{Derivation of expected \texorpdfstring {$k$}{} for MultiSURF neighborhoods}{equation.2.2}{}}
\citation{urbanowicz17}
\newlabel{eq:N}{{3}{5}{Derivation of expected \texorpdfstring {$k$}{} for MultiSURF neighborhoods}{equation.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Predicted number of neighbors in the MultiSURF $\alpha $ neighborhood}{5}{subsection.2.1}\protected@file@percent }
\newlabel{eq:binomial_average}{{7}{5}{Predicted number of neighbors in the MultiSURF \texorpdfstring {$\alpha $}{} neighborhood}{equation.2.7}{}}
\newlabel{eq:q_prob}{{8}{5}{Predicted number of neighbors in the MultiSURF \texorpdfstring {$\alpha $}{} neighborhood}{equation.2.8}{}}
\citation{lareau15}
\citation{lareau15}
\newlabel{eq:kbar}{{9}{6}{Predicted number of neighbors in the MultiSURF \texorpdfstring {$\alpha $}{} neighborhood}{equation.2.9}{}}
\newlabel{fig:gaussPlot}{{3}{6}{Predicted number of neighbors in the MultiSURF \texorpdfstring {$\alpha $}{} neighborhood}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Data simulations}{6}{section.3}\protected@file@percent }
\newlabel{sec:sim_methods}{{3}{6}{Data simulations}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Interaction effects}{6}{subsection.3.1}\protected@file@percent }
\citation{lareau15}
\citation{leek2007}
\newlabel{eq:b_int}{{10}{7}{Interaction effects}{equation.3.10}{}}
\newlabel{eq:null_case_ctrl}{{11}{7}{Interaction effects}{equation.3.11}{}}
\newlabel{eq:full_interaction_data}{{12}{7}{Interaction effects}{equation.3.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Main effects}{7}{subsection.3.2}\protected@file@percent }
\newlabel{eq:lin-mod}{{13}{7}{Main effects}{equation.3.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Mixed effects: interactions and main effects}{8}{subsection.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Algorithm for simulating mixed effects data with interactions and main effects. \textbf  {(1)} Random network is generated, whose degree distribution is either Erd\H {o}s-R\'{e}nyi or Scale-free. \textbf  {(2)} Adjacency matrix ($A$) and degree vector ($v_d$) corresponding to the random network are computed and functional features ($F$) are randomly selected from those with positive degree. \textbf  {(3)} Correlation matrices are generated for cases and controls. In the control group, high ($\rho ^\text  {hi}$) and low ($\rho ^\text  {lo}$) correlations are assigned to connected ($A_{ij}=1$) and non-connected ($A_{ij}=0$) feature pairs, respectively. In the case group, differential correlation ($b^\text  {int}$) is applied to functional connections. \textbf  {(4)} Upper triangular Cholesky factors are computed for case/control correlation matrices. \textbf  {(5)} Standard normal random data matrices ($X^\text  {ctrl}$ and $X^\text  {case}$) are given correlation structure associated with case and control groups and combined into full data matrix with interaction effects ($X$). \textbf  {(6)} Main effects simulated with effect sizes randomly sampled from $\mathcal  {N}(0,b^\text  {main})$. \textbf  {(8)} Interactions ($X^\text  {int}$) and main effects ($X^\text  {main}$) combined $X$.\relax }}{8}{figure.4}\protected@file@percent }
\newlabel{fig:mixed_sim}{{4}{8}{Algorithm for simulating mixed effects data with interactions and main effects. \textbf {(1)} Random network is generated, whose degree distribution is either Erd\H {o}s-R\'{e}nyi or Scale-free. \textbf {(2)} Adjacency matrix ($A$) and degree vector ($v_d$) corresponding to the random network are computed and functional features ($F$) are randomly selected from those with positive degree. \textbf {(3)} Correlation matrices are generated for cases and controls. In the control group, high ($\rho ^\text {hi}$) and low ($\rho ^\text {lo}$) correlations are assigned to connected ($A_{ij}=1$) and non-connected ($A_{ij}=0$) feature pairs, respectively. In the case group, differential correlation ($b^\text {int}$) is applied to functional connections. \textbf {(4)} Upper triangular Cholesky factors are computed for case/control correlation matrices. \textbf {(5)} Standard normal random data matrices ($X^\text {ctrl}$ and $X^\text {case}$) are given correlation structure associated with case and control groups and combined into full data matrix with interaction effects ($X$). \textbf {(6)} Main effects simulated with effect sizes randomly sampled from $\mathcal {N}(0,b^\text {main})$. \textbf {(8)} Interactions ($X^\text {int}$) and main effects ($X^\text {main}$) combined $X$.\relax }{figure.4}{}}
\citation{R}
\citation{mckinney13}
\citation{urbanowicz17}
\@writefile{toc}{\contentsline {section}{\numberline {4}Optimizing $k$ for detecting effects}{9}{section.4}\protected@file@percent }
\newlabel{sec:optim-k}{{4}{9}{Optimizing \texorpdfstring {$k$}{} for detecting effects}{section.4}{}}
\citation{npdr}
\bibstyle{unsrt}
\bibdata{BoD-rsfmri}
\bibcite{urbanowicz17}{1}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Variable-Wise Optimized $k$ (VWOK) method for choosing optimal values of $k$ in nearest-neighbor feature selection. For each attribute $a$ and each value of $k=1,2,\dots  ,m-1$, a score or weight is computed with a nearest-neighbor feature selection method like Relief-Based Algorithms (RBAs) or another similar method. Each feature is assigned the value of $k$ that maximizes its importance score. Winning scores are then sorted in decreasing order and some fraction of the top scoring attributes are chosen for further analysis. In our analysis, we use standardized NPDR beta coefficients to score attributes, where adjusted pseudo P values are used to filter out insignificant attributes.\relax }}{10}{figure.5}\protected@file@percent }
\newlabel{fig:vwok}{{5}{10}{Variable-Wise Optimized $k$ (VWOK) method for choosing optimal values of $k$ in nearest-neighbor feature selection. For each attribute $a$ and each value of $k=1,2,\dots ,m-1$, a score or weight is computed with a nearest-neighbor feature selection method like Relief-Based Algorithms (RBAs) or another similar method. Each feature is assigned the value of $k$ that maximizes its importance score. Winning scores are then sorted in decreasing order and some fraction of the top scoring attributes are chosen for further analysis. In our analysis, we use standardized NPDR beta coefficients to score attributes, where adjusted pseudo P values are used to filter out insignificant attributes.\relax }{figure.5}{}}
\bibcite{mckinney13}{2}
\bibcite{stir}{3}
\bibcite{lareau15}{4}
\bibcite{leek2007}{5}
\bibcite{R}{6}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Area Under the Precision-Recall Curve (auPRC) as a function of $k$ for simulated data. Each point on the plot is the average auPRC from 30 simulated data sets with interactions or main effects. Each data set has $m=p=100$ instances and attributes. Attribute scores were computed with NPDR\nobreakspace  {}\cite  {npdr}. The auPRC monotonically increases with increasing $k$ and is maximized at $k=m-1=99$ for main effects. For interactions, auPRC increases between $k=1$ and $k=18$ and remains relatively constant until $k=38$. After $k=38$, auPRC decreases monotonically between $k=39$ and $k=99$.\relax }}{11}{figure.6}\protected@file@percent }
\newlabel{fig:auPRC-vs-k}{{6}{11}{Area Under the Precision-Recall Curve (auPRC) as a function of $k$ for simulated data. Each point on the plot is the average auPRC from 30 simulated data sets with interactions or main effects. Each data set has $m=p=100$ instances and attributes. Attribute scores were computed with NPDR~\cite {npdr}. The auPRC monotonically increases with increasing $k$ and is maximized at $k=m-1=99$ for main effects. For interactions, auPRC increases between $k=1$ and $k=18$ and remains relatively constant until $k=38$. After $k=38$, auPRC decreases monotonically between $k=39$ and $k=99$.\relax }{figure.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{11}{section.5}\protected@file@percent }
\bibcite{npdr}{7}
\newlabel{LastPage}{{}{12}{}{page.12}{}}
\xdef\lastpage@lastpage{12}
\xdef\lastpage@lastpageHy{12}
