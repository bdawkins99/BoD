\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces {\bf  Imposters vs true neighbors in the presence of interactions with three variables}. Attributes A, B, and C have no main effect. (\textbf  {\fontfamily  {ptm}\selectfont  I}) Scatter plot of simulated irrelevant Attribute C with a functional Attribute A. (\textbf  {\fontfamily  {ptm}\selectfont  II}) Scatter plot of Attributes A and B, which interact through differential correlation. Computing nearest neighbors with irrelevant attributes (\textbf  {\fontfamily  {ptm}\selectfont  I}) or lower dimensions leads to imposter nearest neighbors and degrades the ability of Relief-based methods to identify interaction effects. Computing distances in only these two dimensions leads to an imposter false miss (FM) for the nearest neighbor from the opposite outcome class for target instance X. This imposter leads to Attribute A predicting closer projected distances for misses than hits (H), which incorrectly indicates that A is a poor discriminator (yellow boxes in \textbf  {\fontfamily  {ptm}\selectfont  I}). (\textbf  {\fontfamily  {ptm}\selectfont  III-IV}) Computing nearest neighbors in higher dimensions or with the correct interaction partner leads to imposter nearest neighbor (FM) being replaced by the true nearest miss neighbor (TM) for target instance X, which correctly leads to Attribute A predicting closer projected distances for hits (H) than misses, which is an indication that Attribute A is a good discriminator (yellow boxes \textbf  {\fontfamily  {ptm}\selectfont  II}).\relax }}{2}{figure.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:ABC}{{1}{2}{{\bf Imposters vs true neighbors in the presence of interactions with three variables}. Attributes A, B, and C have no main effect. (\textbf {\fontfamily {ptm}\selectfont I}) Scatter plot of simulated irrelevant Attribute C with a functional Attribute A. (\textbf {\fontfamily {ptm}\selectfont II}) Scatter plot of Attributes A and B, which interact through differential correlation. Computing nearest neighbors with irrelevant attributes (\textbf {\fontfamily {ptm}\selectfont I}) or lower dimensions leads to imposter nearest neighbors and degrades the ability of Relief-based methods to identify interaction effects. Computing distances in only these two dimensions leads to an imposter false miss (FM) for the nearest neighbor from the opposite outcome class for target instance X. This imposter leads to Attribute A predicting closer projected distances for misses than hits (H), which incorrectly indicates that A is a poor discriminator (yellow boxes in \textbf {\fontfamily {ptm}\selectfont I}). (\textbf {\fontfamily {ptm}\selectfont III-IV}) Computing nearest neighbors in higher dimensions or with the correct interaction partner leads to imposter nearest neighbor (FM) being replaced by the true nearest miss neighbor (TM) for target instance X, which correctly leads to Attribute A predicting closer projected distances for hits (H) than misses, which is an indication that Attribute A is a good discriminator (yellow boxes \textbf {\fontfamily {ptm}\selectfont II}).\relax }{figure.1}{}}
\citation{npdr2}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Interacting features A and B in the presence of noise. (\textbf  {I}) Standardized beta coefficients of interacting features A and B and irrelevant features plotted vs the total number of features $p$. As more irrelevant features are added to the simulated interacting features A and B, beta coefficients decrease quickly. At about 200 features (198 irrelevant), A and B have beta coefficients close to 0. (\textbf  {II}) P values corresponding to standardized beta coefficients of interacting features A and B and irrelevant features plotted vs the total number of features $p$. As more irrelevant features are added, P values quickly approach 1.\relax }}{3}{figure.2}\protected@file@percent }
\newlabel{fig:ABC_plus_noise}{{2}{3}{Interacting features A and B in the presence of noise. (\textbf {I}) Standardized beta coefficients of interacting features A and B and irrelevant features plotted vs the total number of features $p$. As more irrelevant features are added to the simulated interacting features A and B, beta coefficients decrease quickly. At about 200 features (198 irrelevant), A and B have beta coefficients close to 0. (\textbf {II}) P values corresponding to standardized beta coefficients of interacting features A and B and irrelevant features plotted vs the total number of features $p$. As more irrelevant features are added, P values quickly approach 1.\relax }{figure.2}{}}
\citation{npdr2}
\citation{urbanowicz17}
\citation{mckinney13}
\newlabel{eq:diff_outcome}{{1}{4}{Introduction}{equation.0.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Nearest-neighbor Projected Distance Regression for binary response (case-control)\nobreakspace  {}\cite  {npdr2}. For each attribute $a \in \mathcal  {A}$, where $\mathcal  {A}$ is the collection of all attributes, we compute standardized beta coefficients for a generalized linear model, where the predictors ($\text  {d}_{ij}(a) = |X_{ia} - X_{ja}|$) are one-dimensional projected distances (diffs) with respect to a particular attribute ($a$). The argument $\text  {p}^\text  {miss}_{ij}$ is the probability that instances ($i,j$) are in different phenotype classes. The logit function models the binary hit-miss phenotype projection (Eq.\nobreakspace  {}\ref  {eq:diff_outcome}). Significant adjusted pseudo P values ($p_\text  {adj}$) lead to the rejection of the null hypothesis $\beta _a \leq 0$. The set $\mathcal  {N}(k)$ is the collection of all neighbor ordered pairs such that each target instances neighborhood has exactly $k$ nearest neighbors.\relax }}{4}{figure.3}\protected@file@percent }
\newlabel{fig:npdr_finished}{{3}{4}{Nearest-neighbor Projected Distance Regression for binary response (case-control)~\cite {npdr2}. For each attribute $a \in \mathcal {A}$, where $\mathcal {A}$ is the collection of all attributes, we compute standardized beta coefficients for a generalized linear model, where the predictors ($\text {d}_{ij}(a) = |X_{ia} - X_{ja}|$) are one-dimensional projected distances (diffs) with respect to a particular attribute ($a$). The argument $\text {p}^\text {miss}_{ij}$ is the probability that instances ($i,j$) are in different phenotype classes. The logit function models the binary hit-miss phenotype projection (Eq.~\ref {eq:diff_outcome}). Significant adjusted pseudo P values ($p_\text {adj}$) lead to the rejection of the null hypothesis $\beta _a \leq 0$. The set $\mathcal {N}(k)$ is the collection of all neighbor ordered pairs such that each target instances neighborhood has exactly $k$ nearest neighbors.\relax }{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Neighborhood methods}{4}{section.1}\protected@file@percent }
\citation{stir}
\@writefile{toc}{\contentsline {section}{\numberline {2}Derivation of expected $k$ for MultiSURF neighborhoods}{5}{section.2}\protected@file@percent }
\newlabel{sec:expected-k}{{2}{5}{Derivation of expected \texorpdfstring {$k$}{} for MultiSURF neighborhoods}{section.2}{}}
\newlabel{eq:D}{{2}{5}{Derivation of expected \texorpdfstring {$k$}{} for MultiSURF neighborhoods}{equation.2.2}{}}
\newlabel{eq:diff}{{3}{5}{Derivation of expected \texorpdfstring {$k$}{} for MultiSURF neighborhoods}{equation.2.3}{}}
\newlabel{eq:N}{{4}{5}{Derivation of expected \texorpdfstring {$k$}{} for MultiSURF neighborhoods}{equation.2.4}{}}
\citation{urbanowicz17}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Predicted number of neighbors in the MultiSURF $\alpha $ neighborhood}{6}{subsection.2.1}\protected@file@percent }
\newlabel{eq:alpha-radius}{{6}{6}{Predicted number of neighbors in the MultiSURF \texorpdfstring {$\alpha $}{} neighborhood}{equation.2.6}{}}
\newlabel{eq:binomial_average}{{8}{6}{Predicted number of neighbors in the MultiSURF \texorpdfstring {$\alpha $}{} neighborhood}{equation.2.8}{}}
\newlabel{eq:q_prob}{{9}{6}{Predicted number of neighbors in the MultiSURF \texorpdfstring {$\alpha $}{} neighborhood}{equation.2.9}{}}
\newlabel{eq:kbar}{{10}{6}{Predicted number of neighbors in the MultiSURF \texorpdfstring {$\alpha $}{} neighborhood}{equation.2.10}{}}
\citation{lareau15}
\citation{lareau15}
\citation{lareau15}
\newlabel{fig:gaussPlot}{{4}{7}{Predicted number of neighbors in the MultiSURF \texorpdfstring {$\alpha $}{} neighborhood}{figure.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Data simulations}{7}{section.3}\protected@file@percent }
\newlabel{sec:sim_methods}{{3}{7}{Data simulations}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Interaction effects}{7}{subsection.3.1}\protected@file@percent }
\newlabel{eq:b_int}{{11}{7}{Interaction effects}{equation.3.11}{}}
\citation{leek2007}
\newlabel{eq:null_case_ctrl}{{12}{8}{Interaction effects}{equation.3.12}{}}
\newlabel{eq:full_interaction_data}{{13}{8}{Interaction effects}{equation.3.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Main effects}{8}{subsection.3.2}\protected@file@percent }
\newlabel{eq:lin-mod}{{14}{8}{Main effects}{equation.3.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Mixed effects: interactions and main effects}{8}{subsection.3.3}\protected@file@percent }
\citation{mckinney13}
\citation{urbanowicz17}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Algorithm for simulating mixed effects data with interactions and main effects. \textbf  {(1)} Random network is generated, whose degree distribution is either Erd\H {o}s-R\'{e}nyi or Scale-free. \textbf  {(2)} Adjacency matrix ($A$) and degree vector ($v_d$) corresponding to the random network are computed and functional features ($F$) are randomly selected from those with positive degree. \textbf  {(3)} Correlation matrices are generated for cases and controls. In the control group, high ($\rho ^\text  {hi}$) and low ($\rho ^\text  {lo}$) correlations are assigned to connected ($A_{ij}=1$) and non-connected ($A_{ij}=0$) feature pairs, respectively. In the case group, differential correlation ($b^\text  {int}$) is applied to functional connections. \textbf  {(4)} Upper triangular Cholesky factors are computed for case/control correlation matrices. \textbf  {(5)} Standard normal random data matrices ($X^\text  {ctrl}$ and $X^\text  {case}$) are given correlation structure associated with case and control groups and combined into full data matrix with interaction effects ($X$). \textbf  {(6)} Main effects simulated with effect sizes randomly sampled from $\mathcal  {N}(0,b^\text  {main})$. \textbf  {(8)} Interactions ($X^\text  {int}$) and main effects ($X^\text  {main}$) combined $X$.\relax }}{9}{figure.5}\protected@file@percent }
\newlabel{fig:mixed_sim}{{5}{9}{Algorithm for simulating mixed effects data with interactions and main effects. \textbf {(1)} Random network is generated, whose degree distribution is either Erd\H {o}s-R\'{e}nyi or Scale-free. \textbf {(2)} Adjacency matrix ($A$) and degree vector ($v_d$) corresponding to the random network are computed and functional features ($F$) are randomly selected from those with positive degree. \textbf {(3)} Correlation matrices are generated for cases and controls. In the control group, high ($\rho ^\text {hi}$) and low ($\rho ^\text {lo}$) correlations are assigned to connected ($A_{ij}=1$) and non-connected ($A_{ij}=0$) feature pairs, respectively. In the case group, differential correlation ($b^\text {int}$) is applied to functional connections. \textbf {(4)} Upper triangular Cholesky factors are computed for case/control correlation matrices. \textbf {(5)} Standard normal random data matrices ($X^\text {ctrl}$ and $X^\text {case}$) are given correlation structure associated with case and control groups and combined into full data matrix with interaction effects ($X$). \textbf {(6)} Main effects simulated with effect sizes randomly sampled from $\mathcal {N}(0,b^\text {main})$. \textbf {(8)} Interactions ($X^\text {int}$) and main effects ($X^\text {main}$) combined $X$.\relax }{figure.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Optimizing $k$ for detecting effects}{9}{section.4}\protected@file@percent }
\newlabel{sec:optim-k}{{4}{9}{Optimizing \texorpdfstring {$k$}{} for detecting effects}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Attribute-Wise Optimized $k$ (AWOK) method for choosing optimal values of $k$ in nearest-neighbor feature selection. For each attribute $a$ and each value of $k=1,2,\dots  ,m-1$, a score or weight is computed with a nearest-neighbor feature selection method like Relief-Based Algorithms (RBAs) or another similar method. Each feature is assigned the value of $k$ that maximizes its importance score. Winning scores are then sorted in decreasing order and some fraction of the top scoring attributes are chosen for further analysis. In our analysis, we use standardized NPDR beta coefficients to score attributes, where adjusted pseudo P values are used to filter out insignificant attributes.\relax }}{10}{figure.6}\protected@file@percent }
\newlabel{fig:vwok}{{6}{10}{Attribute-Wise Optimized $k$ (AWOK) method for choosing optimal values of $k$ in nearest-neighbor feature selection. For each attribute $a$ and each value of $k=1,2,\dots ,m-1$, a score or weight is computed with a nearest-neighbor feature selection method like Relief-Based Algorithms (RBAs) or another similar method. Each feature is assigned the value of $k$ that maximizes its importance score. Winning scores are then sorted in decreasing order and some fraction of the top scoring attributes are chosen for further analysis. In our analysis, we use standardized NPDR beta coefficients to score attributes, where adjusted pseudo P values are used to filter out insignificant attributes.\relax }{figure.6}{}}
\newlabel{eq:recall}{{15}{10}{Optimizing \texorpdfstring {$k$}{} for detecting effects}{equation.4.15}{}}
\newlabel{eq:precision}{{16}{10}{Optimizing \texorpdfstring {$k$}{} for detecting effects}{equation.4.16}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Example of precision and recall for attribute scores}}{11}{table.caption.3}\protected@file@percent }
\newlabel{tab:auPRC}{{1}{11}{Example of precision and recall for attribute scores}{table.caption.3}{}}
\citation{npdr2}
\citation{R}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces NPDR standardized beta coefficients and Area Under the Precision-Recall Curve (auPRC) as a function of $k$ for simulated data}}{12}{figure.7}\protected@file@percent }
\newlabel{fig:auPRC-vs-k}{{7}{12}{NPDR standardized beta coefficients and Area Under the Precision-Recall Curve (auPRC) as a function of $k$ for simulated data}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Nested cross-validation algorithm for simultaneously tuning parameters and selecting models. The data is split into training and test folds, where each outer training fold is further split into training and validation folds in order to tune parameters and select features. Parameters and features are chosen in the inner folds to optimize validation accuracy. Selected features and tuned parameters from the inner loop are used to train on the full outer training fold. Features and tuned parameters corresponding with the highest outer test accuracy are ultimately selected.\relax }}{13}{figure.8}\protected@file@percent }
\newlabel{fig:nCV}{{8}{13}{Nested cross-validation algorithm for simultaneously tuning parameters and selecting models. The data is split into training and test folds, where each outer training fold is further split into training and validation folds in order to tune parameters and select features. Parameters and features are chosen in the inner folds to optimize validation accuracy. Selected features and tuned parameters from the inner loop are used to train on the full outer training fold. Features and tuned parameters corresponding with the highest outer test accuracy are ultimately selected.\relax }{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Comparison of Area Under the Precision-Recall Curve (auPRC) for three methods on simulated data with main effects or interactions. Each point on any particular boxplot represents the max auPRC for one of 30 simulated data sets, where auPRC is maximized as a function of $k$. We compared auPRC for each data set with respect to Global fixed $k$, Attribute-Wise Optimized $k$ (AWOK), and nested cross-validation (nested-CV). AWOK has the best performance overall and the lowest variance in auPRC. Unlike the other methods, AWOK auPRC is balanced between interactions and main effects because each attribute has its own best $k$.\relax }}{14}{figure.9}\protected@file@percent }
\newlabel{fig:auPRC-compare-global-vwok-nCV}{{9}{14}{Comparison of Area Under the Precision-Recall Curve (auPRC) for three methods on simulated data with main effects or interactions. Each point on any particular boxplot represents the max auPRC for one of 30 simulated data sets, where auPRC is maximized as a function of $k$. We compared auPRC for each data set with respect to Global fixed $k$, Attribute-Wise Optimized $k$ (AWOK), and nested cross-validation (nested-CV). AWOK has the best performance overall and the lowest variance in auPRC. Unlike the other methods, AWOK auPRC is balanced between interactions and main effects because each attribute has its own best $k$.\relax }{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Comparison of Area Under the Precision-Recall Curve (auPRC) for three methods on simulated data with main effects or interactions. Each point on any particular boxplot represents the max auPRC for one of 30 simulated data sets, where auPRC is maximized as a function of $k$. We compared auPRC for each data set with respect to Attribute-Wise Optimized $k$ (AWOK), adaptive radius method MultiSURF (MSURF), and predicted MultiSURF fixed $k$ (MSURF $k$). AWOK has the best performance overall and the lowest variance in auPRC. Unlike the other methods, AWOK auPRC is balanced between interactions and main effects because each attribute has its own best $k$.\relax }}{15}{figure.10}\protected@file@percent }
\newlabel{fig:auPRC-compare-vwok-msurf-predk}{{10}{15}{Comparison of Area Under the Precision-Recall Curve (auPRC) for three methods on simulated data with main effects or interactions. Each point on any particular boxplot represents the max auPRC for one of 30 simulated data sets, where auPRC is maximized as a function of $k$. We compared auPRC for each data set with respect to Attribute-Wise Optimized $k$ (AWOK), adaptive radius method MultiSURF (MSURF), and predicted MultiSURF fixed $k$ (MSURF $k$). AWOK has the best performance overall and the lowest variance in auPRC. Unlike the other methods, AWOK auPRC is balanced between interactions and main effects because each attribute has its own best $k$.\relax }{figure.10}{}}
\citation{urbanowicz17}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Summary of average fixed $k$ for three different methods. Using AWOK (Figure\nobreakspace  {}\ref  {fig:vwok}), we computed the average optimized $k$ for interactions ($k^\text  {int}_\text  {ave}$), main effects ($k^\text  {int}_\text  {ave}$), and noise attributes ($k^\text  {noise}_\text  {ave}$). We also applied Global best $k$ to optimize auPRC ($k^\text  {auPRC}_\text  {ave}$). The average MultiSURF $k$ ($k^\text  {msurf}_\text  {ave}$) is given for comparison. The high correlation parameter ($\rho ^\text  {hi}$, Fiure\nobreakspace  {}\ref  {fig:mixed_sim}\textbf  {(3)}) was varied within $\{0.3,0.6,0.9\}$ to explore low, medium, and high interaction effect sizes. Mix Ratio represents the ratio of main-to-interaction effects among functional attributes, where 0.2 means 20\% main effects and 80\% interactions.\relax }}{16}{table.caption.4}\protected@file@percent }
\newlabel{tab:summary-k}{{2}{16}{Summary of average fixed $k$ for three different methods. Using AWOK (Figure~\ref {fig:vwok}), we computed the average optimized $k$ for interactions ($k^\text {int}_\text {ave}$), main effects ($k^\text {int}_\text {ave}$), and noise attributes ($k^\text {noise}_\text {ave}$). We also applied Global best $k$ to optimize auPRC ($k^\text {auPRC}_\text {ave}$). The average MultiSURF $k$ ($k^\text {msurf}_\text {ave}$) is given for comparison. The high correlation parameter ($\rho ^\text {hi}$, Fiure~\ref {fig:mixed_sim}\textbf {(3)}) was varied within $\{0.3,0.6,0.9\}$ to explore low, medium, and high interaction effect sizes. Mix Ratio represents the ratio of main-to-interaction effects among functional attributes, where 0.2 means 20\% main effects and 80\% interactions.\relax }{table.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{16}{section.5}\protected@file@percent }
\citation{leek2007}
\bibstyle{unsrt}
\bibdata{BoD-rsfmri}
\bibcite{npdr2}{1}
\bibcite{urbanowicz17}{2}
\bibcite{mckinney13}{3}
\bibcite{stir}{4}
\bibcite{lareau15}{5}
\bibcite{leek2007}{6}
\bibcite{R}{7}
\newlabel{LastPage}{{}{18}{}{page.18}{}}
\xdef\lastpage@lastpage{18}
\xdef\lastpage@lastpageHy{18}
