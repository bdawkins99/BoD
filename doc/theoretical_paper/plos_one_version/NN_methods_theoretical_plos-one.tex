% Template for PLoS
% Version 3.5 March 2018
%
% % % % % % % % % % % % % % % % % % % % % %
%
% -- IMPORTANT NOTE
%
% This template contains comments intended 
% to minimize problems and delays during our production 
% process. Please follow the template instructions
% whenever possible.
%
% % % % % % % % % % % % % % % % % % % % % % % 
%
% Once your paper is accepted for publication, 
% PLEASE REMOVE ALL TRACKED CHANGES in this file 
% and leave only the final text of your manuscript. 
% PLOS recommends the use of latexdiff to track changes during review, as this will help to maintain a clean tex file.
% Visit https://www.ctan.org/pkg/latexdiff?lang=en for info or contact us at latex@plos.org.
%
%
% There are no restrictions on package use within the LaTeX files except that 
% no packages listed in the template may be deleted.
%
% Please do not include colors or graphics in the text.
%
% The manuscript LaTeX source should be contained within a single file (do not use \input, \externaldocument, or similar commands).
%
% % % % % % % % % % % % % % % % % % % % % % %
%
% -- FIGURES AND TABLES
%
% Please include tables/figure captions directly after the paragraph where they are first cited in the text.
%
% DO NOT INCLUDE GRAPHICS IN YOUR MANUSCRIPT
% - Figures should be uploaded separately from your manuscript file. 
% - Figures generated using LaTeX should be extracted and removed from the PDF before submission. 
% - Figures containing multiple panels/subfigures must be combined into one image file before submission.
% For figure citations, please use "Fig" instead of "Figure".
% See http://journals.plos.org/plosone/s/figures for PLOS figure guidelines.
%
% Tables should be cell-based and may not contain:
% - spacing/line breaks within cells to alter layout or alignment
% - do not nest tabular environments (no tabular environments within tabular environments)
% - no graphics or colored text (cell background color/shading OK)
% See http://journals.plos.org/plosone/s/tables for table guidelines.
%
% For tables that exceed the width of the text column, use the adjustwidth environment as illustrated in the example table in text below.
%
% % % % % % % % % % % % % % % % % % % % % % % %
%
% -- EQUATIONS, MATH SYMBOLS, SUBSCRIPTS, AND SUPERSCRIPTS
%
% IMPORTANT
% Below are a few tips to help format your equations and other special characters according to our specifications. For more tips to help reduce the possibility of formatting errors during conversion, please see our LaTeX guidelines at http://journals.plos.org/plosone/s/latex
%
% For inline equations, please be sure to include all portions of an equation in the math environment.  For example, x$^2$ is incorrect; this should be formatted as $x^2$ (or $\mathrm{x}^2$ if the romanized font is desired).
%
% Do not include text that is not math in the math environment. For example, CO2 should be written as CO\textsubscript{2} instead of CO$_2$.
%
% Please add line breaks to long display equations when possible in order to fit size of the column. 
%
% For inline equations, please do not include punctuation (commas, etc) within the math environment unless this is part of the equation.
%
% When adding superscript or subscripts outside of brackets/braces, please group using {}.  For example, change "[U(D,E,\gamma)]^2" to "{[U(D,E,\gamma)]}^2". 
%
% Do not use \cal for caligraphic font.  Instead, use \mathcal{}
%
% % % % % % % % % % % % % % % % % % % % % % % % 
%
% Please contact latex@plos.org with any questions.
%
% % % % % % % % % % % % % % % % % % % % % % % %

\documentclass[10pt,letterpaper]{article}
\usepackage[top=0.85in,left=2.75in,footskip=0.75in]{geometry}

% amsmath and amssymb packages, useful for mathematical formulas and symbols
\usepackage{amsmath,amssymb}

% Use adjustwidth environment to exceed column width (see example table in text)
\usepackage{changepage}

% Use Unicode characters when possible
\usepackage[utf8x]{inputenc}

% textcomp package and marvosym package for additional characters
\usepackage{textcomp,marvosym}

% cite package, to clean up citations in the main text. Do not remove.
\usepackage{cite}

% Use nameref to cite supporting information files (see Supporting Information section for more info)
\usepackage{nameref,hyperref}

% line numbers
\usepackage[right]{lineno}

\usepackage{float}

% ligatures disabled
\usepackage{microtype}
\DisableLigatures[f]{encoding = *, family = * }

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% adjust width of tikz tables or figures
\usepackage{adjustbox}

\usepackage{tikz}

% bold math symbols package
\usepackage{bm}

% create "+" rule type for thick vertical lines
\newcolumntype{+}{!{\vrule width 2pt}}

% create \thickcline for thick horizontal lines of variable length
\newlength\savedwidth
\newcommand\thickcline[1]{%
  \noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
  \cline{#1}%
  \noalign{\vskip\arrayrulewidth}%
  \noalign{\global\arrayrulewidth\savedwidth}%
}

% \thickhline command for thick horizontal lines that span the table
\newcommand\thickhline{\noalign{\global\savedwidth\arrayrulewidth\global\arrayrulewidth 2pt}%
\hline
\noalign{\global\arrayrulewidth\savedwidth}}


% Remove comment for double spacing
%\usepackage{setspace} 
%\doublespacing

% Text layout
\raggedright
\setlength{\parindent}{0.5cm}
\textwidth 5.25in 
\textheight 8.75in

% Bold the 'Figure #' in the caption and separate it from the title/caption with a period
% Captions will be left justified
\usepackage[aboveskip=1pt,labelfont=bf,labelsep=period,justification=raggedright,singlelinecheck=off]{caption}
\renewcommand{\figurename}{Fig}

% Use the PLoS provided BiBTeX style
\bibliographystyle{plos2015}

% Remove brackets from numbering in List of References
\makeatletter
\renewcommand{\@biblabel}[1]{\quad#1.}
\makeatother

% define theorem and definition environments commands
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]


% Header and Footer with logo
\usepackage{lastpage,fancyhdr,graphicx}
\usepackage{epstopdf}
%\pagestyle{myheadings}
\pagestyle{fancy}
\fancyhf{}
%\setlength{\headheight}{27.023pt}
%\lhead{\includegraphics[width=2.0in]{PLOS-submission.eps}}
\rfoot{\thepage/\pageref{LastPage}}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrule}{\hrule height 2pt \vspace{2mm}}
\fancyheadoffset[L]{2.25in}
\fancyfootoffset[L]{2.25in}
\lfoot{\today}

%% Include all macros below

\newcommand{\lorem}{{\bf LOREM}}
\newcommand{\ipsum}{{\bf IPSUM}}

%% END MACROS SECTION


\begin{document}
\vspace*{0.2in}

% Title must be 250 characters or less.
\begin{flushleft}
{\Large
\textbf\newline{Theoretical properties of nearest-neighbor distance distributions and novel metrics for high dimensional bioinformatics data} % Please use "sentence case" for title and headings (capitalize only the first word in a title (or heading), the first word in a subtitle (or subheading), and any proper nouns).
}
\newline
% Insert author names, affiliations and corresponding author email (do not include titles, positions, or degrees).
\\
Bryan A. Dawkins\textsuperscript{1},
Brett A. McKinney\textsuperscript{2,3*},
Trang T. Le\textsuperscript{4}
\\
\bigskip
\textbf{1} Genes and Human Disease, Oklahoma Medical Research Foundation, Oklahoma City, \\
\hphantom{1} Oklahoma, USA
\\
\textbf{2} Department of Mathematics, University of Tulsa, Tulsa, OK 74104, USA
\\
\textbf{3} Tandy School of Computer Science, University of Tulsa, Tulsa, OK 74104, USA
\\
\textbf{4} Department of Biostatistics, Epidemiology and Informatics, University of \\
\hphantom{4} Pennsylvania, Philadelphia, PA 19104
\\
\bigskip

% Insert additional author notes using the symbols described below. Insert symbol callouts after author names as necessary.
% 
% Remove or comment out the author notes below if they aren't used.
%
% Primary Equal Contribution Note
%\Yinyang These authors contributed equally to this work.

% Additional Equal Contribution Note
% Also use this double-dagger symbol for special authorship notes, such as senior authorship.
%\ddag These authors also contributed equally to this work.

% Current address notes
%\textcurrency Current Address: Dept/Program/Center, Institution Name, City, State, Country % change symbol to "\textcurrency a" if more than one current address note
% \textcurrency b Insert second current address 
% \textcurrency c Insert third current address

% Deceased author note
%\dag Deceased

% Group/Consortium Author Note
%\textpilcrow Membership list can be found in the Acknowledgments section.

% Use the asterisk to denote corresponding authorship and provide email address in note below.
* brett.mckinney@gmail.com

\end{flushleft}
% Please keep the abstract below 300 words
\section*{Abstract}
	The performance of nearest-neighbor feature selection and prediction methods depends on the metric for computing neighborhoods and the distribution properties of the underlying data. The effects of the distribution and metric, as well as the presence of correlation and interactions, are reflected in the expected moments of the distribution of pairwise distances. We derive general analytical expressions for the mean and variance of pairwise distances for $L_q$ metrics for normal and uniform random data with $p$ attributes and $m$ instances. We use extreme value theory to derive results for metrics that are normalized by the range of each attribute (max - min). In addition to these expressions for continuous data, we derive similar analytical formulas for a new metric for genetic variants (categorical data) in genome-wide association studies (GWAS). The genetic distance distributions account for minor allele frequency and transition/transversion ratio. We introduce a new metric for resting-state functional MRI data (rs-fMRI) and derive its distance properties. This metric is applicable to correlation-based predictors derived from time series data. Derivations assume independent data, but empirically we also consider the effect of correlation. These analytical results and new metrics can be used to inform the optimization of nearest neighbor methods for a broad range of studies including gene expression, GWAS, and fMRI data. The summary of distribution moments and detailed derivations provide a resource for understanding the distance properties for various metrics and data types.


% Please keep the Author Summary between 150 and 200 words
% Use first person. PLOS ONE authors please skip this step. 
% Author Summary not valid for PLOS ONE submissions.   
%\section*{Author summary}
%Lorem ipsum dolor sit amet, consectetur adipiscing elit. Curabitur eget porta erat. Morbi consectetur est vel gravida pretium. Suspendisse ut dui eu ante cursus gravida non sed sem. Nullam sapien tellus, commodo id velit id, eleifend volutpat quam. Phasellus mauris velit, dapibus finibus elementum vel, pulvinar non tellus. Nunc pellentesque pretium diam, quis maximus dolor faucibus id. Nunc convallis sodales ante, ut ullamcorper est egestas vitae. Nam sit amet enim ultrices, ultrices elit pulvinar, volutpat risus.

\linenumbers

% Use "Eq" instead of "Equation" for equation citations.
\section*{Introduction}
	Statistical models can deviate from expected behavior depending on whether certain properties of the underlying data are satisfied, such as being normally distributed. The expected behavior of nearest neighbor models is further influenced by the choice of metric, such as Euclidean or Manhattan. For random normal data ($\mathcal{N}(0,1)$), for example, the variance of the pairwise distances of a Manhattan metric is proportional to the number of attributes ($p$) whereas the variance is constant for a Euclidean metric. Relief methods~\cite{urbanowicz17,urbanowicz17b,robnik2003} and nearest-neighbor projected distance regression (NDPR)~\cite{npdr} use nearest neighbors to compute attribute importance scores and often use adaptive neighborhoods that rely on the mean and variance of the distance distribution. The ability of this class of methods to identify association effects, like main effects or interaction effects, depends on parameters such as neighborhood radii or number of neighbors k~\cite{stir,mckinney13}. Thus, knowledge of the expected values for a given metric and data distribution may improve the performance of these feature selection methods by informing the selection of neighborhood parameters. 

For continuous data, the metrics most commonly used in nearest neighbor methods are $L_q$ with $q=1$ (Manhattan) or $q=2$ (Euclidean). For data from standard normal ($\mathcal{N}(0,1)$) or standard uniform ($\mathcal{U}(0,1)$) distributions, the asymptotic behavior of the $L_q$ metrics is known. However, detailed derivations of these distance distribution asymptotics are not readily available in the literature. We provide detailed derivations of generalized expressions parameterized by metric $q$, attributes $p$, and samples $m$, and we extend the derivations to $L_q$ metrics normalized by the range of the attributes using Extreme Value Theory (EVT). These range (max-min) normalized metrics are often used in Relief-based algorithms~\cite{robnik2003}. 

For genome-wide association study (GWAS) data, which is categorical, various metrics have been developed for feature selection and for computing similarity between individuals based on shared genetic variation. We build on the mathematical formalism for continuous data to derive the asymptotic properties of various categorical data metrics for GWAS. We derive asymptotic formulas for the mean and variance for three recently introduced GWAS metrics~\cite{arabnejad2018}. These metrics were developed for Relief-based feature selection to account for binary genotype differences (two levels), allelic differences (three levels), and transition/transversion differences (five levels). The mean and variance expressions we derive for these multi-level categorical data types are parameterized by the minor allele frequency and the transition/transversion ratio. 

Resting-state fMRI (rs-fMRI) is a growing application area for machine learning and feature selection~\cite{venkataraman2010,hay2017,sundermann2014,vergun2013}, which involves correlation data derived from time-series brain activity. For a given subject, a correlation or similar matrix is computed between brain regions of interest (ROIs) from their time series. Each time series represents functional activity of the ROI while the subject is not performing any task, and the ROI typically corresponds to a region with known function for emotion or cognition. Thus, the dataset consists of pairwise ROI correlations for each of the $m$ subjects. Nearest-neighbor based feature selection was applied to rs-fMRI in the private evaporative cooling method \cite{le17}, where the predictors were pairwise correlations between ROIs. The use of pairwise correlation predictors is a common practice because of convenience and differential connectivity between brain regions may be of biological importance~\cite{gotts2012}. However, one may be interested in the importance of attributes at the ROI level. Thus, in the current study we introduce a new metric to be used in NPDR~\cite{npdr} with resting state correlation matrices that provides attribute importance for ROIs. This metric is applicable to general time series derived correlation data, and we derive asymptotic estimates for the mean and variance of distance distributions for our new ts-corr based metric.

In Section~\ref{sec:notation_and_CLT}, we introduce preliminary notation and apply the Central Limit Theorem (CLT) and the Delta Method to derive asymptotics for pairwise distances. In Section~\ref{sec:moment_derivations}, we present general derivations for continuously distributed data sets with $m$ instances and $p$ attributes. We begin with the cases of standard normal ($\mathcal{N}(0,1)$) and standard uniform ($\mathcal{U}(0,1)$) data distributions, but we derive analytical expressions parameterized by $q$, $p$, and $m$. In Section~\ref{sec:extremes} we use Extreme Value Theory (EVT) to derive attribute range-normalized (max-min) versions of $L_q$ metrics. In Section~\ref{sec:gwas_distances}, we extend the derivations to categorical data with a binomial distribution for GWAS data with multiple metric types. In Section~\ref{sec:rs-fMRI_distances}, we present a new time series correlation-based distance metric, with a particular emphasis on rs-fMRI data, and we derive the corresponding asymptotic distance distribution results. Lastly, in Section~\ref{sec:correlation}, we demonstrate the effect of correlation in the attribute space on distance distributional properties. 

\section*{Limit distribution for \texorpdfstring{$L_q$}{} on null data}\label{sec:notation_and_CLT}
In the application of nearest-neighbor distance-based methods to continuous data, the distance between instances ($i,j \in \mathcal{I}, |\mathcal{I}|=m$) in the data set $X^{m \times p}$ of $m$ instances and $p$ attributes (or features) is calculated in the space of all attributes ($a \in \mathcal{A}$, $|\mathcal{A}|=p$) using a metric such as
\begin{equation}\label{eq:D}
% D^{(q)}_{ij}=\left(\sum_{a\in A}|\text{d}^{\text{type}}_{ij}(a)|^q\right)^{1/q},
D^{(q)}_{ij}=\left(\sum_{a\in \mathcal{A}}|\text{d}_{ij}(a)|^q\right)^{1/q},
\end{equation}
which is typically Manhattan ($q=1$) in Relief-based methods but may also be Euclidean ($q=2$). We use the terms ``feature" and ``attribute" interchangeably for the remainder of this work. The quantity 
% $\text{d}^{\text{type}}_{ij}(a)$, 
$\text{d}_{ij}(a)$,
known as a ``$\text{diff}$'' in Relief literature, is the projection of the distance between instances $i$ and $j$ onto the attribute $a$ dimension. The 
% ``type'' refers to the data type of the attribute
function $\text{d}_{ij}(a)$ supports any type of attributes
(e.g., numeric and categorical).
For example, the projected difference between two instances $i$ and $j$ for a continuous numeric ($\text{d}^{\text{num}}$) attribute $a$ may be
%\begin{equation}\label{eq:diff}
%\text{diff}^{(\text{num})}(a,(\ri,\rj))=\frac{|\text{value}(a,\ri)-\text{value}(a,\rj)%|}{\max(a)-\min(a)}.
%\end{equation}
\begin{equation}\label{eq:diff}
\begin{aligned}
\text{d}^{\text{num}}_{ij}(a)&=\text{diff}(a,(i,j))\\
& = {|\hat{X}_{ia}-\hat{X}_{ja}|},
\end{aligned}
\end{equation}
where $\hat{X}$ represents the standardized data matrix $X$.
We use a simplified d$_{ij}(a)$ notation in place of the $\text{diff}(a,(i,j))$ notation that is customary in Relief-based methods.
In NPDR, we omit the division by $\max(a)-\min(a)$ used by Relief to constrain scores to the interval from $-1$ to $1$, where $\max(a) = \max_{k \in \mathcal{I}}\{X_{ka}\}$ and $\min(a) = \min{k \in \mathcal{I}}\{X_{ka}\}$. The numeric d$^{\text{num}}_{ij}(a)$ projection is simply the absolute difference between row elements $i$ and $j$ of the data matrix $X^{m \times p}$ for the attribute column $a$. 

All derivations in the following sections are applicable to nearest-neighbor distance-based methods in general, which includes not only NPDR, but also Relief-based algorithms. Each of these methods uses a distance metric (Eq.~\ref{eq:D}) to compute neighbors for each instance $i \in \mathcal{I}$. Therefore, our derivations of asymptotic distance distributions are applicable to all methods that compute neighbors in order to weight features. The predictors used by NPDR, however, are the one-dimensional projected distances between two instances $i,j \in \mathcal{I}$ (Eq.~\ref{eq:diff}). Hence, all asymptotic estimates we derive for diff metrics (Eq.~\ref{eq:diff}) are particularly relevant to NPDR. Since the standard distance metric (Eq.~\ref{eq:D}) is a function of the one-dimensional projection (Eq.~\ref{eq:diff}), asymptotic estimates derived for this projection (Eq.~\ref{eq:diff}) are implicitly relevant to older nearest-neighbor distance-based methods like Relief-based algorithms. We proceed in the following section by applying the Classical Central Limit Theorem and the Delta Method to derive the limit distribution of pairwise distances on any data distribution that is induced by the standard distance metric (Eq.~\ref{eq:D}).

\subsection*{Asymptotic normality of pairwise distances}
%Discuss Central Limit Theorem argument for the distribution being Gaussian. We can always say this is an assumption, but I think we can invoke the CLT. [I would be cautious here - TTL. We will be cautious invoking the CLT - BAM.]

Suppose that $X_{ia}, X_{ja} \overset{iid}{\sim} \mathcal{F}_X\left(\mu_X,\sigma^2_X\right)$ for two fixed and distinct instances $i,j \in \mathcal{I}$ and a fixed attribute $a \in \mathcal{A}$.
$\mathcal{F}_X$ represents any data distribution with mean $\mu_X$ and variance $\sigma^2_X$.

It is clear that $|X_{ia} - X_{ja}|^q = |\text{d}_{ij}(a)|^q$ is another random variable. Let $Z^q_a \sim \mathcal{F}_{Z^q_a}\left(\mu_{z^q_a},\sigma^2_{z^q_a}\right)$ be the random variable such that
%
\begin{equation}\label{eq:diffDistr}
Z^q_a = |\text{d}_{ij}(a)|^q = |X_{ia} - X_{ja}|^q, \quad a \in \mathcal{A}.
\end{equation}

Furthermore, the collection $\{Z^q_a | a \in \mathcal{A}\}$ is a random sample of size $p$ of mutually independent random variables. Hence, the sum of $Z^q_a$ over all $a \in \mathcal{A}$ is asymptotically normal by the Classical Central Limit Theorem (CCLT). More explicitly, this implies that
%
\begin{equation}\label{eq:DqAsympt}
\left(D^{(q)}_{ij}\right)^q = \sum_{a \in \mathcal{A}} |\text{d}_{ij}(a)|^q = \sum_{a \in \mathcal{A}} |X_{ia} - X_{ja}|^q = \sum_{a \in \mathcal{A}} Z^q_a \overset{.}{\sim} \mathcal{N}\left(\mu_{z^q_a}p,\sigma^2_{z^q_a}p\right).
\end{equation}

Consider the smooth function $g(z) = z^{1/q}$ that is continuously differentiable for $z>0$. Assuming that $\mu_{z^q_a}>0$, the Delta Method \cite{allStats} can be applied to show that 

\begin{equation}\label{eq:DqDeltaMethod}
\begin{aligned}
g\left(\left(D^{(q)}_{ij}\right)^q\right) &= g\left(\displaystyle \sum_{a \in \mathcal{A}}^{p}Z^q_a\right) \\
&= \left(\sum_{a \in \mathcal{A}} |X_{ia} - X_{ja}|^q\right)^{1/q} \\
&= D^{(q)}_{ij} \overset{.}{\sim} \mathcal{N}\left(g\left(\mu_{z^q}p\right),\left[g^\prime \left(\mu_{z^q_a}p\right)\right]^2\sigma^2_{z^q_a}p\right) \\
\Rightarrow &\hphantom{=} D^{(q)}_{ij} \overset{.}{\sim} \mathcal{N}\left(\left(\mu_{z^q_a}p\right)^{1/q},\frac{\sigma^2_{z^q_a}p}{q^2\left(\mu_{z^q_a}p\right)^{2\left(1 - \frac{1}{q}\right)}}\right).
\end{aligned}
\end{equation}

Therefore, the distance between two fixed, distinct instances $i$ and $j$ given by Eq. \ref{eq:D} is asymptotically normal.
Specifically, when $q = 2$, the distribution of $D_{ij}^{(2)}$ asymptotically approaches $\mathcal{N}\left(\sqrt{\mu_{z^2_a}p}, \frac{\sigma^2_{z^2_a}}{4\mu_{z^2_a}}\right)$. 
When $p$ is small, however, we observe empirically that a closer estimate of the sample mean is 
% \begin{equation}\label{eq:DqMeanImproved}
% \text{E}\left(D^{(q)}_{ij}\right) = \left(\mu_{z^q}p - \frac{\sigma^2_{z^q}p}{q^2\left(\mu_{z^q}p\right)^{2\left(1 - \frac{1}{q}\right)}}\right)^{1/q}
% \end{equation}
%
\begin{equation}\label{eq:DqImprovedExplained}
\begin{aligned}
\text{E}\left(D^{(2)}_{ij}\right) &= \sqrt{\text{E}\left[\left(D^{(2)}_{ij}\right)^2\right] - \text{Var}\left(D^{(2)}_{ij}\right)} \\
&= \sqrt{\mu_{z^2_a}p - \frac{\sigma^2_{z^2_a}}{4\mu_{z^2_a}}}.
\end{aligned}
\end{equation}
%
We estimate rate of convergence to normality for Euclidean ($q=2$) and Manhattan ($q=1$)  metrics by comparing the distribution of pairwise distances in simulated data to a Gaussian (Fig.~\ref{fig:central_limit_convergence}). We compute the distance between all pairs of instances in simulated datasets of uniformly distributed random data. We simulate data with fixed $m=100$ instances, and, by varying the number of attributes ($p=10,100,10000$), we observe rapid convergence to Gaussian. For $p$ as low as $10$ attributes, Gaussian is a good approximation. The number of attributes in bioinformatics data is typically quite large, at least on the order of $10^3$. The Euclidean metric has stronger convergence to a Gaussian than Manhattan. This may be due to Euclidean's use of the square root, which is a common transformation of data in statistics. Normality was assessed using the Shapiro-Wilk test. 

To show asymptotic normality of distances, we did not specify whether the data distribution $\mathcal{F}_X$ was discrete or continuous. This is because asymptotic normality is a general phenomenon in high attribute dimension $p$ for any data distribution $\mathcal{F}_X$ satisfying the assumptions we have made. Therefore, the simulated distances we have shown (Fig.~\ref{fig:central_limit_convergence}) has an analogous representation for discrete data, as well as all other continuous data distributions.

For distance based learning methods, all pairwise distances are used to determine relative importances for attributes. The collection of all distances above the diagonal in an $m \times m$ distance matrix does not satisfy the independence assumption used in the previous derivations. This is because of the redundancy that is inherent to the distance matrix calculation. However, this collection is still asymptotically normal with mean and variance approximately equal to those we have previously given (Eq.~\ref{eq:DqDeltaMethod}). In the next section, we assume actual data distributions in order to define more specific general formulas for standard $L_q$ and max-min normalized $L_q$ metrics. We also derive asymptotic moments for a new discrete metric in GWAS data and a new metric for time series correlation-based data, such as, resting-state fMRI.

\begin{figure}[H]
	\centering
	%\includegraphics[width=0.98\textwidth]{central_limit_hist_uniform-data.pdf}
	\caption{{\bf Convergence to Gaussian for Manhattan and Euclidean distances for simulated standard uniform data with $m=100$ instances and $p=10, 100,$ and $10000$ attributes.} Convergence to Gaussian occurs rapidly with increasing $p$, and Gaussian is a good approximation for $p$ as low as $10$ attributes. The number of attributes in bioinformatics data is typically much larger, at least on the order of $10^3$. The Euclidean metric has stronger convergence to normal than Manhattan.  P values from Shapiro-Wilk test, where the null hypothesis is a Gaussian distribution.}
	\label{fig:central_limit_convergence}
\end{figure}

For distance based learning methods, all pairwise distances are used to determine relative importances for attributes. The collection of all distances above the diagonal in an $m \times m$ distance matrix does not satisfy the independence assumption used in the previous derivations. This is because of the redundancy that is inherent to the distance matrix calculation. However, this collection is still asymptotically normal with mean and variance approximately equal to those we have previously given (Eq.~\ref{eq:DqDeltaMethod}). In the next section, we assume actual data distributions in order to define more specific general formulas for standard $L_q$ and max-min normalized $L_q$ metrics. We also derive asymptotic moments for a new discrete metric in GWAS data and a new metric for time series correlation-based data, such as, resting-state fMRI.

\section*{\texorpdfstring{$L_q$}{} metric moments for continuous data distributions}\label{sec:moment_derivations}

In this section, we begin by deriving general formulas for asymptotic means and variances of the $L_q$ distance (Eq.~\ref{eq:D}) for standard normal and standard uniform data. With our general formulas for continuous data, we compute moments associated with Manhattan ($L_1$) and Euclidean ($L_2$) metrics. We then consider the max-min normalized version of the $L_q$ distance, where the magnitude difference (Eq.~\ref{eq:diff}) is divided by the range of each attribute $a$. Using Extreme Value Theory (EVT), we derive formulas for the moments of attribute range in standard normal and standard uniform data. Transitioning into discrete data distributions relevant to GWAS, we derive asymptotic moments for two well known metrics and one new metric. In addition, we derive distance asymptotics for time series correlation-based data, such as, resting-state fMRI.  

\subsection*{Distribution of \texorpdfstring{$|\text{d}_{ij}(a)|^q = |X_{ia} - X_{ja}|^q$}{}}

Suppose that $X_{ia}, X_{ja} \overset{iid}{\sim} \mathcal{F}_X(\mu_x,\sigma^2_x)$ and define $Z^q_a = |\text{d}_{ij}(a)|^q = |X_{ia} - X_{ja}|^q$, where $a \in \mathcal{A}$ and $|\mathcal{A}| = p$. In order to find the distribution of $Z^q_a$, we will use the following theorem given in \cite{freund2004}.

\begin{theorem}\label{thm:freund}
	Let $f(x)$ be the value of the probability density of the continuous random variable $X$ at $x$. If the function given by $y = u(x)$ is differentiable and either increasing or decreasing for all values within the range of $X$ for which $f(x) \neq 0$, then, for these values of $x$, the equation $y = u(x)$ can be uniquely solved for $x$ to give $x = w(y)$, and for the corresponding values of $y$ the probability density of $Y = u(X)$ is given by
	
	\[g(y) = f[w(y)] \cdot |w^\prime(y)| \quad \text{ provided } u^\prime(x) \neq 0\]
	
	\noindent Elsewhere, $g(y) = 0$.
\end{theorem}

We have the following cases that result from solving for $X_{ja}$ in the equation given by $Z^q_a = |X_{ia} - X_{ja}|^q$:
\begin{itemize}
	\item[(i)] Suppose that $X_{ja} = X_{ia} - \left(Z^q_a\right)^{1/q}$. Based on the iid assumption for $X_{ia}$ and $X_{ja}$, it follows from Thm. \ref{thm:freund} that the joint density function $g^{(1)}$ of $X_{ia}$ and $Z^q_a$ is given by
	%
	\begin{equation}
	\begin{aligned}
	g^{(1)}(x_{ia},z_a) &= f_X(x_{ia},x_{ja})\biggl|\frac{\partial x_{ja}}{\partial z_a}\biggr| \\
	&= f_X(x_{ia})f_X(x_{ja})\biggl|\frac{-1}{q} \left(z^q_a\right)^{\frac{1}{q}-1}\biggr| \\
	&= \frac{1}{q \left(z^q_a\right)^{1 - \frac{1}{q}}}f_X(x_{ia})f_X\left(x_{ia}-\left(z^q_a\right)^{1/q}\right), \quad z_a > 0
	\end{aligned}
	\end{equation}
	
	The density function $f^{(1)}_{Z^q_a}$ of $Z^q_a$ is then defined as
	%
	\begin{equation}
	\begin{aligned}
	f^{(1)}_{Z^q_a}(z^q_a) &= \int_{-\infty}^{\infty} g^{(1)}(x_{ia},z^q_a)\text{d}x_{ia} \\
	&= \frac{1}{q \left(z^q_a\right)^{1 - \frac{1}{q}}}\int_{-\infty}^{\infty} f_X(x_{ia})f_X\left(x_{ia}-\left(z^q_a\right)^{1/q}\right)\text{d}x_{ia}, \quad z_a > 0.
	\end{aligned}
	\end{equation}
	
	\item[(ii)] Suppose that $X_{ja} = X_{ia} + \left(Z^q_a\right)^{1/q}$. Based on the iid assumption for $X_{ia}$ and $X_{ja}$, it follows from Thm. \ref{thm:freund} that the joint density function $g^{(2)}$ of $X_{ia}$ and $Z_a$ is given by
	%
	\begin{equation}
	\begin{aligned}
	g^{(2)}(x_{ia},z_a) &= f_X(x_{ia},x_{ja})\biggl|\frac{\partial x_{ja}}{\partial z_a}\biggr| \\
	&= f_X(x_{ia})f_X(x_{ja})\biggl|\frac{1}{q} \left(z^q_a\right)^{\frac{1}{q}-1}\biggr| \\
	&= \frac{1}{q \left(z^q_a\right)^{1 - \frac{1}{q}}}f_X(x_{ia})f_X\left(x_{ia}-\left(z^q_a\right)^{1/q}\right), \quad z_a > 0.
	\end{aligned}
	\end{equation}
	
	The density function $f^{(2)}_{Z^q_a}$ of $Z^q_a$ is then defined as
	%
	\begin{equation}
	\begin{aligned}
	f^{(2)}_{Z^q_a}(z^q_a) &= \int_{-\infty}^{\infty} g^{(2)}(x_{ia},z^q_a)\text{d}x_{ia} \\
	&= \frac{1}{q \left(z^q_a\right)^{1 - \frac{1}{q}}}\int_{-\infty}^{\infty} f_X(x_{ia})f_X\left(x_{ia}+\left(z^q_a\right)^{1/q}\right)\text{d}x_{ia}, \quad z_a > 0.
	\end{aligned}
	\end{equation}
\end{itemize}

Let $F_{Z^q_a}$ denote the distribution function of the random variable $Z^q_a$. Furthermore, we define the events $E^{(1)}$ and $E^{(2)}$ as
%
\begin{equation}\label{eq:E(1)}
E^{(1)} = \bigl\{|X_{ia}-X_{ja}|^q \leq z^q_a : X_{ja} = X_{ia} - \left(Z^q_a\right)^{1/q}\bigr\}
\end{equation}
%
and
%
\begin{equation}\label{eq:E(2)}
E^{(2)} = \bigl\{|X_{ia}-X_{ja}|^q \leq z^q_a : X_{ja} = X_{ia} + \left(Z^q_a\right)^{1/q}\bigr\}.
\end{equation}

Then it follows from fundamental rules of probability that
%
\begin{equation}\label{eq:DqCDF}
\begin{aligned}
F_{Z^q_a}(z^q_a) &= \text{P}\left[Z^q_a \leq z^q_a\right] \\
&= \text{P}\left[|X_{ia} - X_{ja}|^q \leq z^q_a\right] \\
&= \text{P}\left[E^{(1)} \cup E^{(2)}\right] \\
&= \text{P}\bigl[E^{(1)}\bigr] + \text{P}\bigl[E^{(2)}\bigr] - \text{P}\bigl[E^{(1)} \cap E^{(2)}\bigr] \\
&= \text{P}\bigl[E^{(1)}\bigr] + \text{P}\bigl[E^{(2)}\bigr] \\
&= \int_{-\infty}^{z^q_a} f^{(1)}_{Z^q_a}(t) \text{d}t + \int_{-\infty}^{z^q_a} f^{(2)}_{Z^q_a}(t) \text{d}t \\
&= \int_{-\infty}^{z^q_a} \left(f^{(1)}_{Z^q_a}(t) + f^{(2)}_{Z^q_a}(t)\right) \text{d}t \\
&= \frac{1}{q \left(z^q_a\right)^{1 - \frac{1}{q}}}\int_{-\infty}^{z^q_a} \left(\int_{-\infty}^{\infty}f_X(x_{ia})\left[f_X(x_{ia} - t) + f_X(x_{ia} + t)\right] \text{d}x_{ia}\right)\text{d}t, \quad z_a > 0.
\end{aligned}
\end{equation}

It follows directly from the previous result (Eq.~\ref{eq:DqCDF}) that the density function of the random variable $Z^q_a$ is given by
%
\begin{equation}\label{eq:DqPDF}
\begin{aligned}
f_{Z^q_a}(z^q_a) &= \frac{\partial}{\partial z^q_a} F_{Z^q_a}(z^q_a) \\
&= \frac{1}{q \left(z^q_a\right)^{1 - \frac{1}{q}}}\int_{-\infty}^{\infty} f_X(x_{ia})\left[f_X\left(x_{ia} - \left(z^q_a\right)^{1/q}\right) + f_X\left(x_{ia} + \left(z^q_a\right)^{1/q}\right)\right] \text{d}x_{ia},
\end{aligned}
\end{equation}
%
where $z_a > 0$.

Using the previous result (Eq.~\ref{eq:DqPDF}), we can compute the mean and variance of the random variable $Z^q_a$ as
%
\begin{equation}\label{eq:1DDqMean}
\mu_{z^q_a} = \int_{-\infty}^{\infty} z^q_a f_{Z^q_a}(z^q_a) \text{d}z^q_a
\end{equation}
%
and 
%
\begin{equation}\label{eq:1DDqVar}
\sigma^2_{z^q_a} = \int_{-\infty}^{\infty} \left(z^q_a\right)^2 f_{Z^q_a}(z^q_a) \text{d}z^q_a - \mu^2_{z^q_a}.
\end{equation}

It follows immediately from the mean (Eq.~\ref{eq:1DDqMean}) and variance (Eq.~\ref{eq:1DDqVar}) and the Classical Central Limit Theorem (CCLT) that
%
\begin{equation}\label{eq:DqDistr}
\left(D^{(q)}_{ij}\right)^q = \sum_{a \in \mathcal{A}} Z^q_a = \sum_{a \in \mathcal{A}} |X_{ia} - X_{ja}|^q \overset{.}{\sim} \mathcal{N}\left(\mu_{z^q}p,\sigma^2_{z^q}p\right).
\end{equation}

Applying the convergence result we derived previously (Eq.~\ref{eq:DqDeltaMethod}), the distribution of $D^{(q)}_{ij}$ is given by
%
\begin{equation}\label{eq:DDistr}
D^{(q)}_{ij} \overset{.}{\sim} \mathcal{N}\left(\left(\mu_{z^q_a}p\right)^{1/q},\frac{\sigma^2_{z^q_a}p}{q^2\left(\mu_{z^q_a}p\right)^{2\left(1 - \frac{1}{q}\right)}}\right), \quad \mu_{z^q_a} > 0,
\end{equation}
%
where we have an improved estimate of the mean for $q=2$ (Eq.~\ref{eq:DqImprovedExplained}).

\subsubsection*{Standard normal data}

If $X_{ia},X_{ja} \overset{iid}{\sim} \mathcal{N}(0,1)$, then the marginal density functions with respect to $X$ for $X_{ia}$, $X_{ia} - \left(Z^q_a\right)^{1/q}$, and $X_{ia} + \left(Z^q_a\right)^{1/q}$ are defined as
%
\begin{equation}\label{eq:normalXmarg}
f_X(x_{ia}) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2_{ia}},
\end{equation}
%
\begin{equation}\label{eq:normalXMinusZmarg}
f_X\left(x_{ia} - \left(z^q_a\right)^{1/q}\right) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}\left(x_{ia} - \left(z^q_a\right)^{1/q}\right)^2}, \quad z_a > 0, \text{ and}
\end{equation}
%
\begin{equation}\label{eq:normalXPlusZmarg}
f_X\left(x_{ia} + \left(z^q_a\right)^{1/q}\right) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}\left(x_{ia} + \left(z^q_a\right)^{1/q}\right)^2}, \quad z_a > 0.
\end{equation}

Substituting these marginal densities (Eqs.~\ref{eq:normalXmarg}-\ref{eq:normalXPlusZmarg}) into the general density function for $Z^q_a$ (Eq.~\ref{eq:DqPDF}) and completing the square on $x_{ia}$ in the exponents, we have
%
\begin{equation}\label{eq:normalPDF}
\begin{aligned}
f_{Z^q_a}(z^q_a) &= \frac{1}{2 q \pi \left(z^q_a\right)^{1 - \frac{1}{q}}} e^{-\frac{1}{4}\left(z^q_a\right)^{2    /q}}\int_{-\infty}^{\infty} \biggl(e^{-\frac{1}{2}\left[\sqrt{2}x_{ia} - \frac{\sqrt{2}}{2}\left(z^q_a\right)^{1/q}\right]^2} \\
&\hspace{2in} + e^{-\frac{1}{2}\left[\sqrt{2}x_{ia} + \frac{\sqrt{2}}{2}\left(z^q_a\right)^{1/q}\right]^2}\biggr) \text{d}x_{ia} \\
&= \frac{1}{2 q \sqrt{\pi} \left(z^q_a\right)^{1 - \frac{1}{q}}} e^{-\frac{1}{4}\left(z^q_a\right)^{2/q}} \int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}} \left(e^{-\frac{1}{2}u^2} + e^{-\frac{1}{2}u^2}\right) \text{d}u \\
&= \frac{1}{2 q \sqrt{\pi} \left(z^q_a\right)^{1 - \frac{1}{q}}} e^{-\frac{1}{4}\left(z^q_a\right)^{2/q}} (1 + 1) \\
&= \frac{1}{q \sqrt{\pi}}\left(z^q_a\right)^{\frac{1}{q} - 1} e^{-\frac{1}{4}\left(z^q_a\right)^{2/q}} \\
&= \frac{\frac{2}{q}}{\left(2^q\right)^{1/q} \Gamma\left(\frac{\frac{1}{q}}{\frac{2}{q}}\right)}\left(z^q_a\right)^{\frac{1}{q} - 1} e^{-\left(\frac{z^q_a}{2^q}\right)^{2/q}}.
\end{aligned}
\end{equation}

The density function given previously (Eq.~\ref{eq:normalPDF}) is a Generalized Gamma density with parameters $b = \frac{2}{q}$, $c = 2^q$, and $d = \frac{1}{q}$. This distribution has mean and variance given by
%
\begin{equation}\label{eq:1DnormalDqMean}
\begin{aligned}
\mu_{z^q_a} &= \frac{c\Gamma\left(\frac{d+1}{b}\right)}{\Gamma\left(\frac{d}{b}\right)} \\
&= \frac{2^q \Gamma\left(\frac{q + 1}{2}\right)}{\sqrt{\pi}}
\end{aligned}
\end{equation}
%
and
%
\begin{equation}\label{eq:1DnormalDqVar}
\begin{aligned}
\sigma^2_{z^q_a} &= c^2\left[\frac{\Gamma\left(\frac{d+2}{b}\right)}{\Gamma\left(\frac{d}{b}\right)} - \left(\frac{\Gamma\left(\frac{d+1}{b}\right)}{\Gamma\left(\frac{d}{b}\right)}\right)^2\right] \\
&= 4^{q}\left[\frac{\Gamma\left(q + \frac{1}{2}\right)}{\sqrt{\pi}} - \frac{\Gamma^2\left(\frac{1}{2}q + \frac{1}{2}\right)}{\pi}\right].
\end{aligned}
\end{equation}

By linearity of the expected value and variance operators under the iid assumption, the mean (Eq.~\ref{eq:1DnormalDqMean}) and variance (Eq.~\ref{eq:1DnormalDqVar}) of the random variable $Z^q_a$ allow the $p\text{-dimensional}$ mean and variance of the $\left(D^{(q)}_{ij}\right)^q$ distribution to be computed directly as
%
\begin{equation}\label{eq:normalDqMean}
\mu_{\left(D^{(q)}_{ij}\right)^q} = \text{E}\left[\left(D^{(q)}_{ij}\right)^q\right] = \text{E}\left(\sum_{a \in \mathcal{A}} Z^q_a\right) = \sum_{a \in \mathcal{A}} \text{E}\left(Z^q_a\right) = \sum_{a \in \mathcal{A}} \frac{2^q \Gamma\left(\frac{q + 1}{2}\right)}{\sqrt{\pi}} = \frac{2^q\Gamma\left(\frac{q + 1}{2}\right)}{\sqrt{\pi}}p
\end{equation}
%
and
%
\begin{equation}\label{eq:normalVar}
\begin{split}
\sigma^2_{\left(D^{(q)}_{ij}\right)^q} = \text{Var}\left[\left(D^{(q)}_{ij}\right)^q\right] &= \text{Var}\left(\sum_{a \in \mathcal{A}} Z^q_a\right) \\
&= \sum_{a \in \mathcal{A}} \text{Var}\left(Z^q_a\right) \\
&= \sum_{a \in \mathcal{A}} 4^{q}\left[\frac{\Gamma\left(q + \frac{1}{2}\right)}{\sqrt{\pi}} - \frac{\Gamma^2\left(\frac{1}{2}q + \frac{1}{2}\right)}{\pi}\right] \\
&= 4^{q}\left[\frac{\Gamma\left(q + \frac{1}{2}\right)}{\sqrt{\pi}} - \frac{\Gamma^2\left(\frac{1}{2}q + \frac{1}{2}\right)}{\pi}\right]p.
\end{split}
\end{equation}

Therefore, the asymptotic distribution of $D^{(q)}_{ij}$ for standard normal data is
%
\begin{equation}\label{eq:normalDistr}
\mathcal{N}\left(\left(2^q\frac{\Gamma\left(\frac{q + 1}{2}\right)}{\sqrt{\pi}}p\right)^{1/q},
\frac{4^q p}{q^2 \left(\frac{2^q \Gamma\left(\frac{1}{2}q + \frac{1}{2}\right)}{\sqrt{\pi}}p\right)^{2\left(1 - \frac{1}{q}\right)}}\left[\frac{\Gamma\left(q + \frac{1}{2}\right)}{\sqrt{\pi}} - \frac{\Gamma^2\left(\frac{1}{2}q + \frac{1}{2}\right)}{\pi}\right]\right).
\end{equation}

We provide a summary figure of the moment estimates (Eq.~\ref{eq:normalDistr}) for the $L_q$ metric on standard normal data (Fig.~\ref{tab:dist_distr_general1}). The summary is organized by data type, type of statistic (mean or variance), and corresponding asymptotic formula. In the next section, we derive the $L_q$ distance distribution on standard uniform data in a similar fashion.

\subsubsection*{Standard uniform data}

If $X_{ia},X_{ja} \overset{iid}{\sim} \mathcal{U}(0,1)$, then the marginal density functions with respect to $X$ for $X_{ia}$, $X_{ia} - \left(Z^q_a\right)^{1/q}$, and $X_{ia} + \left(Z^q_a\right)^{1/q}$ are defined as
%
\begin{equation}\label{eq:uniformXmarg}
f_X(x_{ia}) = 1, \quad 0 \leq x_{ia} \leq 1
\end{equation}
%
\begin{equation}\label{eq:uniformXMinusZmarg}
f_X\left(x_{ia} - \left(z^q_a\right)^{1/q}\right) = 1, \quad 0 \leq x_{ia} - \left(z^q_a\right)^{1/q} \leq 1, \text{ and}
\end{equation}
%
\begin{equation}\label{eq:uniformXPlusZmarg}
f_X\left(x_{ia} + \left(z^q_a\right)^{1/q}\right) = 1, \quad 0 \leq x_{ia} + \left(z^q_a\right)^{1/q} \leq 1.
\end{equation}

Substituting these marginal densities (Eqs.~\ref{eq:uniformXmarg}-\ref{eq:uniformXPlusZmarg}) into the more general density function for $Z^q_a$ (Eq.~\ref{eq:DqPDF}), we have
%
\begin{equation}\label{eq:uniformDqPDF}
\begin{aligned}
f_{Z^q_a}(z^q_a) &= \frac{1}{q\left(z^q_a\right)^{1 - \frac{1}{q}}}\int_{-\infty}^{\infty}f_X(x_{ia})\left[f_X\left(x_{ia} - \left(z^q_a\right)^{1/q}\right) + f_X\left(x_{ia} + \left(z^q_a\right)^{1/q}\right)\right]\text{d}x_{ia},\\
& \hspace{4in} 0 < z_a \leq 1\\
&= \frac{1}{q\left(z^q_a\right)^{1 - \frac{1}{q}}}\int_{0}^{1}\left[f_X(x_{ia} - \left(z^q_a\right) + f_X\left(x_{ia} + \left(z^q_a\right)^{1/q}\right)\right]\text{d}x_{ia}, \quad 0 < z_a \leq 1 \\
&= \frac{1}{q\left(z^q_a\right)^{1 - \frac{1}{q}}}\int_{\left(z^q_a\right)}^{1}1\text{d}x_{ia} + \int_{0}^{1 - \left(z^q_a\right)}1\text{d}x_{ia}, \quad 0 < z_a \leq 1 \\
&= \frac{1}{q\left(z^q_a\right)^{1 - \frac{1}{q}}}\left[\left(1 - \left(z^q_a\right)\right) + \left(1 - \left(z^q_a\right)\right)\right], \quad 0 < z_a \leq 1 \\
&= \frac{1}{q} \cdot 2 \left(z^q_a\right)^{\frac{1}{q} - 1}\left[1 - \left(z^q_a\right)^{1/q}\right]^{2 - 1}, \quad 0 < z_a \leq 1.
\end{aligned}
\end{equation}

The previous density (Eq.~\ref{eq:uniformDqPDF}) is a Kumaraswamy density with parameters $b = \frac{1}{q}$ and $c = 2$ with moment generating function (MGF) given by
%
\begin{equation}\label{eq:uniformDqMGF}
\begin{aligned}
M_n &=  \frac{c\Gamma\left(1 + \frac{n}{b}\right) \Gamma(c)}{\Gamma\left(1 + c + \frac{n}{b}\right)}\\
&= \frac{2}{(nq + 2)(nq + 1)}.
\end{aligned}
\end{equation}

Using this MGF (Eq.~\ref{eq:uniformDqMGF}), the mean and variance of $Z^q_a$ are computed as
%
\begin{equation}\label{eq:1DuniformDqMean}
\mu_{z^q_a} = \frac{2}{(q + 2)(q + 1)}
\end{equation}
%
and
%
\begin{equation}\label{eq:1DuniformDqVar}
\sigma^2_{z^q_a} = \frac{1}{(q + 1)(2q + 1)} - \left(\frac{2}{(q + 2)(q + 1)}\right)^2.
\end{equation}

By linearity of the expected value and variance operators under the iid assumption, the mean (Eq.~\ref{eq:1DuniformDqMean}) and variance (Eq.~\ref{eq:1DuniformDqVar}) of the random variable $Z^q_a$ allow the $p \text{-dimensional}$ mean and variance of the $\left(D^{(q)}_{ij}\right)^q$ distribution to be computed directly as
%
\begin{equation}\label{eq:uniformDqMean}
\begin{split}
\mu_{\left(D^{(q)}_{ij}\right)^q} = \text{E}\left[\left(D^{(q)}_{ij}\right)^q\right] &= \text{E}\left(\sum_{a \in \mathcal{A}}Z^q_a\right) \\
&= \sum_{a \in \mathcal{A}} \text{E}(Z^q_a) \\
&= \sum_{a \in \mathcal{A}} \frac{2}{(q + 2)(q + 1)} \\
&= \frac{2p}{(q + 2)(q + 1)}
\end{split}
\end{equation}
%
and
%
\begin{equation}\label{eq:uniformDqVar}
\begin{split}
\sigma^2_{\left(D^{(q)}_{ij}\right)^q} = \text{Var}\left[\left(D^{(q)}_{ij}\right)^q\right] &= \text{Var}\left(\sum_{a \in \mathcal{A}} Z^q_a\right) \\
&= \sum_{a \in \mathcal{A}} \text{Var}\left(Z^q_a\right) \\
&= \sum_{a \in \mathcal{A}} \left[\frac{1}{(q + 1)(2q + 1)} - \left(\frac{2}{(q + 2)(q + 1)}\right)^2\right] \\
&= \left[\frac{1}{(q + 1)(2q + 1)} - \left(\frac{2}{(q + 2)(q + 1)}\right)^2\right]p.
\end{split}
\end{equation}

Therefore, the asymptotic distribution of $D^{(q)}_{ij}$ for standard uniform data is
%
\begin{equation}\label{eq:uniformDistr}
\begin{split}
\mathcal{N}{\text{\LARGE $\Biggl($}}& \left(\frac{2p}{(q + 2)(q + 1)}\right)^{1/q}, \\
& \frac{p}{q^2\left(\frac{2p}{(q + 2)(q + 1)}\right)^{2\left(1 - \frac{1}{q}\right)}}\left[\frac{1}{(q + 1)(2q + 1)} - \left(\frac{2}{(q + 2)(q + 1)}\right)^2\right]{\text{\LARGE $\Biggr)$}}.
\end{split}
\end{equation}

We provide a summary figure of the moment estimates (Eq.~\ref{eq:uniformDistr}) for the $L_q$ metric on standard uniform data (Fig.~\ref{tab:dist_distr_general1}). The summary is organized by data type, type of statistic (mean or variance), and corresponding asymptotic formula. In the next section, we use our general $L_q$ distance distribution derivations to provide Manhattan ($q=1$) and Euclidean ($q=2$) asymptotic moments on both standard normal and standard uniform data. These are the most commonly applied metrics in the context of nearest-neighbor feature selection, so they are of particular interest.

\subsection*{Manhattan \texorpdfstring{($L_1$)}{}}

With our general formulas for the asymptotic mean and variance (Eqs.~\ref{eq:normalDistr} and \ref{eq:uniformDistr}) for any value of $q \in \mathbb{N}$, we can simply substitute a particular value of $q$ in order to determine the asymptotic distribution of the corresponding distance $L_q$ metric. We demonstrate this with the example of the Manhattan metric ($L_1$) for standard normal and standard uniform data (Eq.~\ref{eq:D}, $q=1$).

\subsubsection*{Standard normal data}

Substituting $q=1$ into the asymptotic formula for the mean $L_q$ distance (Eq.~\ref{eq:normalDistr}), we have the following for expected $L_1$ distance between two independently sample instances $i,j \in \mathcal{I}$ in standard normal data
%
\begin{equation}\label{eq:normalManMean}
\begin{aligned}
\text{E}\left(D^{(1)}_{ij}\right) &= \left(2\frac{\Gamma\left(\frac{1 + 1}{2}\right)}{\sqrt{\pi}}p\right)^{1/1} \\
&= \frac{2p}{\sqrt{\pi}}.
\end{aligned}
\end{equation}

We see in the formula for the expected Manhattan distance (Eq.~\ref{eq:normalManMean}) that $D^{(1)}_{ij} \sim p$ in the limit, which implies that this distance is unbounded as feature dimension $p$ increases.

Substituting $q=1$ into the formula for the asymptotic variance of $D^{(1)}_{ij}$ (Eq.~\ref{eq:normalDistr}) leads to the following
%
\begin{equation}\label{eq:normalManVar}
\begin{aligned}
\text{Var}\left(D^{(1)}_{ij}\right) &= \frac{4^1p}{1^2\left(\frac{2^1\Gamma\left(\frac{1}{2}(1) + \frac{1}{2}\right)}{\sqrt{\pi}}p\right)^{2\left(1 - \frac{1}{1}\right)}}\left[\frac{\Gamma\left(1 + \frac{1}{2}\right)}{\sqrt{\pi}} - \frac{\Gamma^2\left(\frac{1}{2}(1) + \frac{1}{2}\right)}{\pi}\right] \\
&= \frac{2(\pi - 2)p}{\pi}.
\end{aligned}
\end{equation}

Similar to the mean (Eq.~\ref{eq:normalManMean}), the limiting variance of $D^{(1)}_{ij}$ (Eq.~\ref{eq:normalManVar}) grows on the order of feature dimension $p$, which implies that points become more dispersed as the dimension increases. The moment estimates given in this section (Eqs.~\ref{eq:normalManMean} and \ref{eq:normalManVar}) are summarized in a figure that is organized by metric, data type, statistic (mean or variance), and asymptotic formula (Fig.~\ref{tab:dist_distr_standardL1L2}).

\subsubsection*{Standard uniform data}

Substituting $q=1$ into the asymptotic formula of the mean (Eq.~\ref{eq:uniformDistr}), we have the following for the expected $L_1$ distance between two independently sampled instances $i,j \in \mathcal{I}$ in standard uniform data
%
\begin{equation}\label{eq:uniformManMean}
\begin{aligned}
\text{E}\left(D^{(1)}_{ij}\right) &= \left(\frac{2p}{(1+2)(1+1)}\right)^{1/1} \\
&= \frac{p}{3}.
\end{aligned}
\end{equation}

Once again, we see that the mean of $D^{(1)}_{ij}$ (Eq.~\ref{eq:uniformManMean}) grows on the order of $p$ just as in the case of standard normal data.

Substituting $q=1$ into the formula of the asymptotic variance of $D^{(1)}_{ij}$ (Eq.~\ref{eq:uniformDistr}) leads to the following
%
\begin{equation}\label{eq:uniformManVar}
\begin{aligned}
\text{Var}\left(D^{(1)}_{ij}\right) &= \frac{p}{1^2\left(\frac{2p}{(1 + 2)(1 + 1)}\right)^{2\left(1 - \frac{1}{1}\right)}}\left[\frac{1}{(1 + 1)(2(1) + 1)} - \left(\frac{2}{(1 + 2)(1 + 1)}\right)^2\right] \\
&= \frac{p}{18}.
\end{aligned}
\end{equation}

As in the case of the $L_1$ metric on standard normal data, we have a variance (Eq.~\ref{eq:uniformManVar}) that grows on the order of $p$. The distances between points in high-dimensional uniform data become more widely dispersed with this metric. The moment estimates given in this section (Eqs.~\ref{eq:uniformManMean} and \ref{eq:uniformManVar}) are summarized in a figure that is organized by metric, data type, statistic (mean or variance), and asymptotic formula (Fig.~\ref{tab:dist_distr_standardL1L2}).

\subsubsection*{Distribution of one-dimensional projection of pairwise distance onto an attribute}\label{sec:continuous_diff}

In nearest-neighbor distance-based feature selection like NPDR and Relief-based algorithms, the one-dimensional projection of the pairwise distance onto an attribute (Eq.~\ref{eq:diff}) is particularly fundamental to feature quality for association with an outcome. For instance, this distance projection is the predictor used to determine beta coefficients in NPDR. In particular, understanding distributional properties of the projected distances is necessary for defining pseudo P values for NPDR. In this section, we summarize the exact distribution of the one-dimensional projected distance onto an attribute $a \in \mathcal{A}$. These results apply to continuous data, such as gene expression. 

In previous sections, we derived the exact density function (Eq.~\ref{eq:DqPDF}) and moments (Eqs.~\ref{eq:1DDqMean} and \ref{eq:1DDqVar}) for the distribution of $Z^q_a=|X_{ia}-X_{ja}|^q$. We then derived the exact density (Eq.~\ref{eq:normalPDF}) and moments (Eqs.~\ref{eq:1DnormalDqMean} and \ref{eq:1DnormalDqVar}) for standard normal data. Analogously, we formulated the exact density (Eq.~\ref{eq:uniformDqPDF}) and moments (Eqs.~\ref{eq:1DuniformDqMean} and \ref{eq:1DuniformDqVar}) for standard uniform data. From these exact densities and moments, we simply substitute $q=1$ to define the distribution of the one-dimensional projected distance onto an attribute $a \in \mathcal{A}$.

Assuming data is standard normal, we substitute $q=1$ into the density function of $Z^q_a$ (Eq.~\ref{eq:normalPDF}) to arrive at the following density function
%
\begin{equation}\label{eq:projected_distance_density_normal1}
\begin{aligned}
f_{Z^1_a}\left(z^1_a\right) &= \frac{\frac{2}{1}}{\left(2^1\right)^{1/1}\Gamma\left(\frac{\frac{1}{1}}{\frac{2}{1}}\right)}\left(z^1_a\right)^{1/1 - 1} e^{-\left(\frac{z^1_a}{2^1}\right)^{2/1}}, \quad z_a > 0\\
&= \frac{1}{\sqrt{\pi}} z_a e^{-\frac{1}{4}z^2_a}, \quad z_a > 0.
\end{aligned}
\end{equation}

The mean corresponding to this Generalized Gamma density is computed by substituting $q=1$ into the formula for the mean of $Z^q_a$ (Eq.~\ref{eq:1DnormalDqMean}). This result is given by
%
\begin{equation}\label{eq:1DnormalD1Mean}
\begin{aligned}
\mu_{Z^1_a} &= \frac{2^1 \Gamma\left(\frac{1 + 1}{2}\right)}{\sqrt{\pi}} \\
&= \frac{2}{\sqrt{\pi}}.
\end{aligned}
\end{equation}

Substituting $q=1$ into Eq.~\ref{eq:1DnormalDqVar} for the variance, we have the following
%
\begin{equation}\label{eq:1DnormalD1Var}
\begin{aligned}
\sigma^2_{Z^1_a} &= 4^1\left[\frac{\Gamma\left(1 + \frac{1}{2}\right)}{\sqrt{\pi}} - \frac{\Gamma^2\left(\frac{1}{2}\cdot 1 + \frac{1}{2}\right)}{\pi}\right] \\
&= \frac{2(\pi-2)}{\pi}.
\end{aligned}
\end{equation}

These last few results (Eqs.~\ref{eq:projected_distance_density_normal1}-\ref{eq:1DnormalD1Var}) provide us with the distribution for NPDR predictors when the data is from the standard normal distribution. 

If we have standard uniform data, we substitute $q=1$ into the density function of $Z^q_a$ (Eq.~\ref{eq:uniformDqPDF}) to obtain the following density function
%
\begin{equation}\label{eq:projected_distance_density_uniform1}
\begin{aligned}
f_{Z^1_a} &= \frac{1}{1}\cdot 2\left(z^1_a\right)^{1/1 - 1}\left[1 - \left(z^1_a\right)^{1/1}\right]^{2-1}, \quad 0 < z_a \leq 1 \\
&= 2z_a(1 - z_a), \quad 0 < z_a \leq 1.
\end{aligned}
\end{equation}

The mean corresponding to this Kumaraswamy density is computed by substituting $q=1$ into the formula for the mean of $Z^q_a$ (Eq.~\ref{eq:1DuniformDqMean}). After substitution, we have the following result
%
\begin{equation}\label{eq:1DuniformD1Mean}
\begin{aligned}
\mu_{Z^1_a} &= \frac{2}{(1 + 2)(1 + 1)} \\
&= \frac{1}{3}.
\end{aligned}
\end{equation}

Substituting $q=1$ into the formula for the variance of $Z^q_a$ (Eq.~\ref{eq:1DuniformDqVar}), we have the following
%
\begin{equation}\label{eq:1DuniformD1Var}
\begin{aligned}
\sigma^2_{Z^1_a} &= \frac{1}{(1 + 1)(2\cdot 1 + 1)} - \left(\frac{2}{(1 + 2)(1 + 1)}\right)^2 \\
&= \frac{1}{18}.
\end{aligned}
\end{equation}

In the event that the data distribution is standard uniform, the density function (Eq.~\ref{eq:projected_distance_density_uniform1}), the mean (Eq.~\ref{eq:1DuniformD1Mean}, and the variance (Eq.~\ref{eq:1DuniformD1Var}) sufficiently define the distribution for NPDR predictors. The means (Eqs.~\ref{eq:1DnormalD1Mean} and \ref{eq:1DuniformD1Mean}) and variances (Eqs.\ref{eq:1DnormalD1Var} and \ref{eq:1DuniformD1Var}) come from the exact distribution of pairwise distances with respect to a single attribute $a \in \mathcal{A}$. This is the distribution of the so-called ``projection" of the pairwise distance onto a single attribute to which we have been referring, which is a direct implication from our more general derivations. In a similar manner, one can substitute any value of $q \geq 2$ into the general densities of $Z^q_a$ for standard normal (Eq.~\ref{eq:normalPDF}) and standard uniform (Eq.~\ref{eq:uniformDqPDF}) to derive the associated density of $Z^q_a = |X_{ia} - X_{ja}|^q$ for the given data type.

\subsection*{Euclidean \texorpdfstring{($L_2$)}{}}

Moment estimates for the Euclidean metric are obtained by substituting $q=2$ into the asymptotic moment formulas for standard normal data (Eq.~\ref{eq:normalDistr}) and standard uniform data (Eq.~\ref{eq:uniformDistr}). As in the case of the Manhattan metric in the previous sections, we initially proceed by deriving Euclidean distance moments in standard normal data.

\subsubsection*{Standard normal data}

Substituting $q=2$ into the asymptotic formula of the mean (Eq. \ref{eq:normalDistr}), we have the following for expected $L_2$ distance between two independently sampled instances $i,j \in \mathcal{I}$ in standard normal data
%
\begin{equation}\label{eq:normalEucMean}
\begin{aligned}
\text{E}\left(D^{(2)}_{ij}\right) &= \left(2\frac{\Gamma\left(\frac{2 + 1}{2}\right)}{\sqrt{\pi}}p\right)^{1/2} \\
&= \sqrt{2p}.
\end{aligned}
\end{equation}

In the case of $L_2$ on standard normal data, we see that the mean of $D^{(2)}_{ij}$ (Eq.~\ref{eq:normalEucMean}) grows on the order of $\sqrt{p}$. Hence, the Euclidean distance does not increase as quickly as the Manhattan distance on standard normal data.

Substituting $q=2$ into the formula for the asymptotic variance of $D^{(2)}_{ij}$ (Eq.~\ref{eq:normalDistr}) leads to the following
%
\begin{equation}\label{eq:normalEucVar}
\begin{aligned}
\text{Var}\left(D^{(2)}_{ij}\right) &= \frac{4^2p}{2^2\left(\frac{2^2\Gamma\left(\frac{1}{2}(2) + \frac{1}{2}\right)}{\sqrt{\pi}}p\right)^{2\left(1 - \frac{1}{2}\right)}}\left[\frac{\Gamma\left(2 + \frac{1}{2}\right)}{\sqrt{\pi}} - \frac{\Gamma^2\left(\frac{1}{2}(2) + \frac{1}{2}\right)}{\pi}\right] \\
&= 1.
\end{aligned}
\end{equation}

Surprisingly, the asymptotic variance (Eq.~\ref{eq:normalEucVar}) is just 1. Regardless of data dimensions $m$ and $p$, the variance of Euclidean distances on standard normal data tends to 1. Therefore, most instances are contained within a ball of radius 1 about the mean in high feature dimension $p$. This means that the Euclidean distance distribution on standard normal data is simply a horizontal shift to the right of the standard normal distribution.

For the case in which the number of attributes $p$ is small, we have an improved estimate of the mean (Eq.~\ref{eq:DqImprovedExplained}). The lower dimensional estimate of the mean is given by
%
\begin{equation}\label{eq:normalEucMeanImproved}
\begin{aligned}
\text{E}\left(D^{(2)}_{ij}\right) &= \left(2\frac{\Gamma\left(\frac{2 + 1}{2}\right)}{\sqrt{\pi}}p - 1\right)^{1/2} \\
&= \sqrt{2p - 1}.
\end{aligned}
\end{equation}

For high dimensional data sets like gene expression \cite{brazma2000,wang2018}, which typically contain thousands of genes (or features), it is clear that the magnitude of $p$ will be sufficient to use the standard asymptotic estimate (Eq.~\ref{eq:normalEucMean}) since $\sqrt{2p} \approx \sqrt{2p - 1}$ in that case. The moment estimates given in this section (Eqs.~\ref{eq:normalEucMeanImproved} and \ref{eq:normalEucVar}) are summarized in a figure that is organized by metric, data type, statistic (mean or variance), and asymptotic formula (Fig.~\ref{tab:dist_distr_standardL1L2}).

\subsubsection*{Standard uniform data}

Substituting $q=2$ into the asymptotic formula of the mean (Eq.~\ref{eq:uniformDistr}), we have the following for expected $L_2$ distance between two independently sampled instances $i,j \in \mathcal{I}$ in standard uniform data
%
\begin{equation}\label{eq:uniformEucMean}
\begin{aligned}
\text{E}\left(D^{(2)}_{ij}\right) &= \left(\frac{2p}{(2+2)(2+1)}\right)^{1/2} \\
&= \sqrt{\frac{p}{6}}.
\end{aligned}
\end{equation}

As in the case of standard normal data, the expected value of $D^{(2)}_{ij}$ (Eq.~\ref{eq:uniformEucMean}) grows on the order of $\sqrt{p}$. 

Substituting $q=2$ into the formula for the asymptotic variance of $D^{(2)}_{ij}$ (Eq.~\ref{eq:uniformDistr}) leads to the following
%
\begin{equation}\label{eq:uniformEucVar}
\begin{aligned}
\text{Var}\left(D^{(2)}_{ij}\right) &= \frac{p}{2^2\left(\frac{2p}{(2 + 2)(2 + 1)}\right)^{2\left(1 - \frac{1}{2}\right)}}\left[\frac{1}{(2 + 1)(2(2) + 1)} - \left(\frac{2}{(2 + 2)(2 + 1)}\right)^2\right] \\
&= \frac{7}{120}.
\end{aligned}
\end{equation}

Once again, the variance of Euclidean distance surprisingly approaches a constant.

For the case in which the number of attributes $p$ is small, we have an improved estimate of the mean (Eq.~\ref{eq:DqImprovedExplained}). The lower dimensional estimate of the mean is given by
%
\begin{equation}\label{eq:uniformEucMeanImproved}
\begin{aligned}
\text{E}\left(D^{(2)}_{ij}\right) &= \left(\frac{2p}{(2+2)(2+1)} - \frac{7}{120}\right)^{1/2} \\
&= \sqrt{\frac{p}{6} - \frac{7}{120}}.
\end{aligned}
\end{equation}

We summarize the moment estimates given in this section for standard $L_q$ metrics (Eqs.~\ref{eq:uniformEucMeanImproved} and \ref{eq:uniformEucVar}) organized by metric, data type, statistic (mean or variance), and asymptotic formula (Fig.~\ref{tab:dist_distr_standardL1L2}). In the next section, we extend these results for the standard $L_q$ metric to derive asymptotics for the attribute range-normalized (max-min) $L_q$ metric used frequently in Relief-based algorithms \cite{urbanowicz17,robnik2003} for scoring attributes. These derivations use extreme value theory to handle the maximum and minimum attributes for standard normal and standard uniform data. 

\subsection*{Distribution of max-min normalized \texorpdfstring{$L_q$}{} metric}\label{sec:extremes}

For Relief-based methods \cite{robnik2003,urbanowicz17}, the standard numeric diff metric is given by
%
\begin{equation}\label{eq:normDiff}
\text{d}^{\text{num}}_{ij}(a) = \text{diff}(a,(i,j)) = \frac{|X_{ia} - X_{ja}|}{\text{max}(a) - \text{min}(a)},
\end{equation}
%
where $\text{max}(a) = \displaystyle \max_{k \in \mathcal{I}}\{X_{ka}\}$, $\text{min}(a) = \displaystyle \min_{k \in \mathcal{I}}\{X_{ka}\}$, and $\mathcal{I} = \{1,2,\dots,m\}$. 

The pairwise distance using this max-min normalized diff metric is then computed as
%
\begin{equation}\label{eq:D*}
\begin{aligned}
D^{(q*)}_{ij} &= \left(\sum_{a\in \mathcal{A}}|\text{d}_{ij}(a)|^q\right)^{1/q} \\
&= \left(\sum_{a\in \mathcal{A}}\left(\frac{|X_{ia} - X_{ja}|}{\text{max}(a) - \text{min}(a)}\right)^q\right)^{1/q}.
\end{aligned}
\end{equation}

In order to determine moments of asymptotic max-min normalized distance (Eq.~\ref{eq:normDiff}) distributions, we will first derive the asymptotic extreme value distributions of the attribute maximum and minimum. Although the exact distribution of the maximum or minimum requires an assumption about the data distribution, the Fisher-Tippett-Gnedenko Theorem is an important result that allows one to generally categorize the extreme value distribution for a collection of independent and identically distributed random variables into one of three distributional families. This theorem does not, however, tell us the exact distribution of the maximum that we require in order to determine asymptotic results for the max-min normalized distance (Eq.~\ref{eq:D*}). We mention this theorem simply to provide some background on convergence of extreme values. Before stating the theorem, we first need the following definition
%
\begin{definition}
	A distribution $\mathcal{F}_X$ is said to be \textbf{degenerate} if its density function $f_X$ is the Dirac delta $\delta(x - c_0)$ centered at a constant $c_0 \in \mathbb{R}$, with corresponding distribution function $F_X$ defined as
	
	\[F_X(x)=\begin{cases}
	1, & x \geq c_0, \\
	0, & x < c_0.
	\end{cases}
	\]
\end{definition}
%
\begin{theorem}[Fisher-Tippett-Gnedenko]\label{thm:EVT}
	Let $X_{1a},X_{2a},\dots,X_{ma} \overset{iid}{\sim} \mathcal{F}_X\left(\mu_x,\sigma^2_x\right)$ and let $X^\text{max}_a = \displaystyle \max_{k \in \mathcal{I}}\{X_{ka}\}$. If there exists two non-random sequences $b_m>0$ and $c_m$ such that
	
	\[\lim_{m \to \infty} \text{P}\left(\frac{X^\text{max}_a - c_m}{b_m} \leq x\right) = G_X(x),\]
	
	\noindent where $G_X$ is a non-degenerate distribution function, then the limiting distribution $\mathcal{G}_X$ is in the Gumbel, Fr\'{e}chet, or Wiebull family.
\end{theorem}

The three distribution families given in Theorem~\ref{thm:EVT} are actually special cases of the Generalized Extreme Value Distribution. In the context of extreme values, Theorem~\ref{thm:EVT} is analogous to the Central Limit Theorem for the distribution of sample mean.  Although we will not explicitly invoke this theorem, it does tell us something very important about the asymptotic behavior of sample extremes under certain necessary conditions. For illustration of this general phenomenon of sample extremes, we derive the distribution of the maximum for standard normal data to show that the limiting distribution is in the Gumbel family, which is a well known result. In the case of standard uniform data, we will derive the distribution of the maximum and minimum directly. Regardless of data type, the distribution of the sample maximum can be derived as follows
%
\begin{equation}\label{eq:exact_max}
\begin{aligned}
\text{P}[X^\text{max}_a \leq x] &= \text{P}\left[\max_{k \in \mathcal{I}}\{X_{ka}\} \leq x\right] \\
&= \text{P}[X_{1a} \leq x, X_{2a} \leq x, \dots, X_{ma} \leq x] \\
&= \prod_{k = 1}^{m} \text{P}[X_{ka} \leq x] \\
&= \prod_{k=1} F_X(x) \\
&= [F_X(x)]^m.
\end{aligned}
\end{equation}

Using more precise notation, the distribution function of the sample maximum in standard normal data is
%
\begin{equation}\label{eq:exact_max_distr_fn}
F_\text{max}(x) = [F_X(x)]^m,
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived and $F_X$ is the distribution function corresponding to the data sample. This means that the distribution of the sample maximum relies only on the distribution function of the data from which extremes are drawn $F_X$ and the size of the sample $m$.

Differentiating the distribution function (Eq.~\ref{eq:exact_max_distr_fn}) gives us the following density function for the distribution of the maximum
%
\begin{equation}\label{eq:exact_max_dens_fn}
\begin{aligned}
f_\text{max}(x) &= \frac{\text{d}}{\text{d}x} F_\text{max}(x) \\
&= \frac{\text{d}}{\text{d}x} [F_X(x)]^m \\
&= m [F_X(x)]^{m-1} f_X(x),
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived, $F_X$ is the distribution function corresponding to the data sample, and $f_X$ is the density function corresponding to the data sample. Similar to the distribution function for the sample maximum (Eq.~\ref{eq:exact_max_distr_fn}), the density function (Eq~\ref{eq:exact_max_dens_fn}) relies only on the distribution and density function of the data from which extremes are derived.

The distribution of the sample minimum, $X^\text{min}_a$, can be derived as follows
%
\begin{equation}\label{eq:exact_min}
\begin{aligned}
\text{P}[X^\text{min}_a \leq x] &= 1 - \text{P}[X^\text{min}_a \geq x] \\
&= 1 - \text{P}\left[\min_{k \in \mathcal{I}}\{X_{ka}\} \geq x\right] \\
&= 1 - \text{P}[X_{1a} \geq x, X_{2a} \geq x, \dots, X_{ma} \geq x] \\
&= 1 - \prod_{k=1}^{m}\text{P}[X_{ka} \geq x] \\
&= 1 - \left[\text{P}[X_{1a} \geq x]\right]^m \\
&= 1 - \left[1 - \text{P}[X_{1a} \leq x]\right]^m \\
&= 1 - \left[1 - F_X(x)\right]^m,
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived and $F_X$ is the distribution function corresponding to the data sample. Therefore, the distribution of sample minimum also relies only on the distribution function of the data from which extremes are derived.

With more precise notation, we have the following expression for the distribution function of the minimum
%
\begin{equation}\label{eq:exact_min_distr_fn}
F_\text{min}(x) = 1 - [1 - F_X(x)]^m.
\end{equation}
%
where $m$ is the size of the sample from which the minimum is derived and $F_X$ is the distribution function corresponding to the data sample.

Differentiating the distribution function (Eq.~\ref{eq:exact_min_distr_fn}) gives us the following density function for the distribution of sample minimum
%
\begin{equation}\label{eq:exact_min_dens_fn}
\begin{aligned}
f_\text{min}(x) &= \frac{\text{d}}{\text{d}x} F_\text{min}(x) \\
&= \frac{\text{d}}{\text{d}x} \left(1 - [1 - F_X(x)]^m\right) \\
&= m\left[1 - F_X(x)\right]^{m-1}f_X(x),
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which the minimum is derived, $F_X$ is the distribution function corresponding to the data sample, and $f_X$ is the density function corresponding to the data sample. As in the case of the density function for sample maximum (Eq.~\ref{eq:exact_max_dens_fn}), the density function for sample minimum relies only on the distribution $F_X$ and density $f_X$ functions of the data from which extremes are derived and the sample size $m$.

Given the densities of the distribution of sample maximum and minimum, we can easily compute the raw moments and variance. The first moment about the origin of the distribution of sample maximum is given by the following
%
\begin{equation}\label{eq:mu_max}
\begin{aligned}
\mu^{(1)}_\text{max}(m) = \text{E}(X^\text{max}_a) &= \int_{-\infty}^{\infty}x f_\text{max}(x)\text{d}x \\
&= \int_{-\infty}^{\infty}x \left(m [F_X(x)]^{m-1} f_X(x)\right)\text{d}x \\
&= m \int_{-\infty}^{\infty}x f_X(x) [F_X(x)]^{m-1}\text{d}x,
\end{aligned}
\end{equation}
%
where $m$ is the sample size, $F_X$ is the distribution function, and $f_X$ is the density function of the data from which the maximum is derived.

The second raw moment of the distribution of sample maximum is derived similarly as follows
%
\begin{equation}\label{eq:mu2_max}
\begin{aligned}
\mu^{(2)}_\text{max}(m) = \text{E}[(X^\text{max}_a)^2] &= \int_{-\infty}^{\infty}x^2 f_\text{max}(x)\text{d}x \\
&= \int_{-\infty}^{\infty}x^2 \left(m [F_X(x)]^{m-1} f_X(x)\right)\text{d}x \\
&= m \int_{-\infty}^{\infty}x^2 f_X(x) [F_X(x)]^{m-1}\text{d}x
\end{aligned}
\end{equation}
%
where $m$ is the sample size, $F_X$ is the distribution function, and $f_X$ is the density function of the data from which the maximum is derived.

Using the first (Eq.~\ref{eq:mu_max}) and second (Eq.~\ref{eq:mu2_max}) raw moments of the distribution of sample maximum, the variance is given by
\begin{equation}\label{eq:sig_max}
\sigma^2_\text{max}(m) = \mu^{(2)}_\text{max}(m) - \left[\mu^{(1)}_\text{max}(m)\right]^2,
\end{equation}
%
where $m$ is the sample size of the data from which the maximum is derived and $\mu^{(1)}_\text{max}(m)$ and $\mu^{(2)}_\text{max}$ are the first and second raw moments, respectively, of the distribution of sample maximum.

Moving on to the distribution of sample minimum, the first raw moment is given by the following
%
\begin{equation}\label{eq:mu_min}
\begin{aligned}
\mu^{(1)}_\text{min}(m) = \text{E}(X^\text{min}_a) &= \int_{-\infty}^{\infty}x f_\text{min}(x)\text{d}x \\
&= \int_{-\infty}^{\infty}x \left(m [1 - F_X(x)]^{m-1} f_X(x)\right)\text{d}x \\
&= m \int_{-\infty}^{\infty}x f_X(x) [1 - F_X(x)]^{m-1}\text{d}x,
\end{aligned}
\end{equation}
%
where $m$ is the sample size, $F_X$ is the distribution function, and $f_X$ is the density function of the data from which the minimum is derived.

Similarly, the second raw moment of the distribution of sample minimum is given by the following
%
\begin{equation}\label{eq:mu2_min}
\begin{aligned}
\mu^{(2)}_\text{min}(m) = \text{E}[(X^\text{min}_a)^2] &= \int_{-\infty}^{\infty}x^2 f_\text{min}(x)\text{d}x \\
&= \int_{-\infty}^{\infty}x^2 \left(m [1 - F_X(x)]^{m-1} f_X(x)\right)\text{d}x \\
&= m \int_{-\infty}^{\infty}x^2 f_X(x) [1 - F_X(x)]^{m-1}\text{d}x,
\end{aligned}
\end{equation}
%
where $m$ is the sample size, $F_X$ is the distribution function, and $f_X$ is the density function of the data from which the minimum is derived.

Using the first (Eq.~\ref{eq:mu_min}) and second (Eq.~\ref{eq:mu2_min}) raw moments of the distribution of sample minimum, the variance is given by
%
\begin{equation}\label{eq:sig_min}
\sigma^2_\text{min}(m) = \mu^{(2)}_\text{min}(m) - \left[\mu^{(1)}_\text{min}(m)\right]^2,
\end{equation}
%
where $m$ is the sample size of the data from which the maximum is derived and $\mu^{(1)}_\text{min}(m)$ and $\mu^{(2)}_\text{min}$ are the first and second raw moments, respectively, of the distribution of sample maximum.

Using the expected attribute maximum (Eq.~\ref{eq:mu_max}) and minimum (Eq.~\ref{eq:mu_min}) for sample size $m$, the following expected attribute range results from linearity of the expectation operator
%
\begin{equation}\label{eq:exp_rng}
\begin{aligned}
\text{E}(X^\text{max}_a - X^\text{min}_a) &= \text{E}(X^\text{max}_a) - \text{E}(X^\text{min}_a) \\
&= \mu^{(1)}_\text{max}(m) - \mu^{(1)}_\text{min}(m).
\end{aligned}
\end{equation}
%
where $\mu^{(1)}_\text{max}(m)$ is the expected sample maximum (Eq.~\ref{eq:mu_max}) and $\mu^{(1)}_\text{min}(m)$ is the expected sample minimum.

For a data distribution whose density is an even function, the expected attribute range (Eq.~\ref{eq:exp_rng}) can be simplified to the following expression
%
\begin{equation}\label{eq:exp_rng_symm}
\text{E}(X^\text{max}_a - X^\text{min}_a) = 2 \mu^{(1)}_\text{max}(m),
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived. Hence, the expected attribute range is simply twice the expected attribute maximum (Eq.~\ref{eq:mu_max}). This result naturally applies to standard normal data, which is symmetric about its mean at 0 and without any skewness.  

For large samples ($m >> 1$) from an exponential type distribution that has infinite support and all moments, the covariance between the sample maximum and minimum is approximately zero \cite{gumbel1947}. In this case, the variance of the attribute range of a sample of size $m$ is given by the following
%
\begin{equation}\label{eq:var_rng}
\begin{aligned}
\text{Var}(X^\text{max}_a - X^\text{min}_a) &\approx \text{Var}(X^\text{max}_a) + \text{Var}(X^\text{min}_a) \\
&= \sigma^2_\text{max}(m) + \sigma^2_\text{min}(m).
\end{aligned}
\end{equation}

Under the assumption of zero skewness, infinite support and even density function, sufficiently large sample size $m$, and distribution of an exponential type for all moments, the variance of attribute range (Eq.~\ref{eq:var_rng}) simplifies to the following
%
\begin{equation}\label{eq:var_rng_symm}
\begin{aligned}
\text{Var}(X^\text{max}_a - X^\text{min}_a) &= 2 \text{Var}(X^\text{max}_a) \\
&= 2 \sigma^2_\text{max}.
\end{aligned}
\end{equation} 

Let $\mu_{D^{(q)}_{ij}}$ and $\sigma^2_{D^{(q)}_{ij}}$ (Eq.~\ref{eq:DDistr}) denote the mean and variance of the standard $L_q$ distance metric (Eq.~\ref{eq:D}). Then the expected value of the max-min normalized distance (Eq.~\ref{eq:D*}) distribution is given by the following
%
\begin{equation}\label{eq:max-min_D_mean}
\begin{aligned}
\mu_{D^{(q*)}_{ij}} &= \text{E}\left[\left(\sum_{a \in \mathcal{A}}\left(\frac{|X_{ia} - X_{ja}|}{X^\text{max}_a - X^\text{min}_a}\right)^q\right)^{1/q}\right] \\
&\approx \frac{1}{\text{E}(X^\text{max}_a - X^\text{min}_a)}\text{E}\left[\left(\sum_{a \in \mathcal{A}}|X_{ia} - X_{ja}|^q\right)^{1/q}\right] \\
&= \frac{\mu_{D^{(q)}_{ij}}}{\text{E}(X^\text{max}_a) - \text{E}(X^\text{min}_a)} \\
&= \frac{\mu_{D^{(q)}_{ij}}}{\mu^{(1)}_\text{max}(m) - \mu^{(1)}_\text{min}(m)},
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived, $\mu^{(1)}_\text{max}(m)$ is the expected value of the sample maximum (Eq.~\ref{eq:mu_max}), and $\mu^{(1)}_\text{min}$ is the expected value of the sample minimum. 

The variance of the max-min normalized distance (Eq.~\ref{eq:D*}) distribution is given by the following
%
\begin{equation}\label{eq:max-min_D_var}
\begin{aligned}
\sigma^2_{D^{(q*)}_{ij}} &= \text{Var}\left[\left(\sum_{a \in \mathcal{A}}\left(\frac{|X_{ia} - X_{ja}|}{X^\text{max}_a - X^\text{min}_a}\right)^q\right)^{1/q}\right] \\
&= \text{E}\left[\left(\sum_{a \in \mathcal{A}}\left(\frac{|X_{ia} - X_{ja}|}{X^\text{max}_a - X^\text{min}_a}\right)^q\right)^{2/q}\right] - \left(\text{E}\left[\left(\sum_{a \in \mathcal{A}}\left(\frac{|X_{ia} - X_{ja}|}{X^\text{max}_a - X^\text{min}_a}\right)^q\right)^{1/q}\right]\right)^2 \\
&\approx \frac{\text{E}\left[\left(\displaystyle \sum_{a \in \mathcal{A}}|X_{ia} - X_{ja}|^q\right)^{2/q}\right]}{\text{E}[(X^\text{max}_a - X^\text{min}_a)^2]} - \frac{\left(\text{E}\left[\left(\displaystyle \sum_{a \in \mathcal{A}}|X_{ia} - X_{ja}|^q\right)^{1/q}\right]\right)^2}{\text{E}[(X^\text{max}_a - X^\text{min}_a)^2]} \\
&= \frac{\sigma^2_{D^{(q)}_{ij}} + \mu^2_{D^{(q)}_{ij}}}{\text{E}[(X^\text{max}_a - X^\text{min}_a)^2]} - \frac{\mu^2_{D^{(q)}_{ij}}}{\text{E}[(X^\text{max}_a - X^\text{min}_a)^2]} \\
&= \frac{\sigma^2_{D^{(q)}_{ij}}}{\text{E}[(X^\text{max}_a - X^\text{min}_a)^2]} \\
&= \frac{\sigma^2_{D^{(q)}_{ij}}}{\text{E}[(X^\text{max}_a)^2] - 2\text{E}(X^\text{max}_a)\text{E}(X^\text{min}_a) + \text{E}(X^\text{min}_a)} \\
&= \frac{\sigma^2_{D^{(q)}_{ij}}}{\mu^{(2)}_\text{max}(m) - 2\mu^{(1)}_\text{max}(m)\mu^{(1)}_\text{min}(m) + \mu^{(2)}_\text{min}(m)},
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived, $\mu^{(1)}_\text{max}(m)$ is the expected value of the sample maximum (Eq.~\ref{eq:mu_max}), and $\mu^{(1)}_\text{min}$ is the expected value of the sample minimum.  

With the mean (Eq.~\ref{eq:max-min_D_mean}) and variance (Eq.~\ref{eq:max-min_D_var}) of the max-min normalized distance (Eq.~\ref{eq:D*}), we have the following generalized estimate for the asymptotic distribution of the max-min normalized distance distribution
%
\begin{equation}\label{eq:max-min-DDistr-general}
D^{(q*)}_{ij} \overset{.}{\sim} \mathcal{N}\left(\frac{\mu_{D^{(q)}_{ij}}}{\mu^{(1)}_\text{max}(m) - \mu^{(1)}_\text{min}(m)}, \frac{\sigma^2_{D^{(q)}_{ij}}}{\mu^{(2)}_\text{max}(m) - 2 \mu^{(1)}_\text{max}(m) \mu^{(1)}_\text{min}(m) + \mu^{(2)}_\text{min}(m)}\right),
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived, $\mu^{(1)}_\text{max}(m)$ is the expected value of the sample maximum (Eq.~\ref{eq:mu_max}), and $\mu^{(1)}_\text{min}$ is the expected value of the sample minimum.

For data with zero skewness and support that is symmetric about 0, the expected sample maximum is the additive inverse of the expected sample minimum. This allows us to express the expected max-min normalized pairwise distance (Eq.~\ref{eq:max-min_D_mean}) exclusively in terms of the expected sample maximum. This result is given by the following
%
\begin{equation}\label{eq:max-min_D_mean_symm}
\mu_{D^{(q*)}_{ij}} \approx \frac{\mu_{D^{(q)}_{ij}}}{2\mu^{(1)}_\text{max}(m)},
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived and $\mu^{(1)}_\text{max}(m)$ is the expected value of the sample maximum (Eq.~\ref{eq:mu_max}).

A similar substitution gives us the following expression for the variance of the max-min normalized distance distribution
%
\begin{equation}\label{eq:max-min_D_var_symm}
\begin{aligned}
\sigma^2_{D^{(q*)}_{ij}} &\approx \frac{\sigma^2_{D^{(q)}_{ij}}}{2\mu^{(2)}_\text{max}(m) + 2\left[\mu^{(1)}_\text{max}(m)\right]^2} \\
&= \frac{\sigma^2_{D^{(q)}_{ij}}}{2\left(\sigma^2_\text{max}(m) + \left[\mu^{(1)}_\text{max}(m)\right]^2\right)},
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived, $\mu^{(1)}_\text{max}(m)$ is the expected value of the sample maximum (Eq.~\ref{eq:mu_max}), and $\sigma^2_\text{max}(m)$ is the variance of the sample maximum (Eq.~\ref{eq:sig_max}).

Therefore, the asymptotic distribution of the max-min normalized distance distribution (Eq.~\ref{eq:max-min-DDistr-general}) becomes
%
\begin{equation}\label{eq:max-min_DDistr}
D^{(q*)}_{ij} \overset{.}{\sim} \mathcal{N}\left(\frac{\mu_{D^{(q)}_{ij}}}{2\mu^{(1)}_\text{max}(m)}, \frac{\sigma^2_{D^{(q)}_{ij}}}{2\left(\sigma^2_\text{max}(m) + \left[\mu^{(1)}_\text{max}(m)\right]^2\right)}\right),
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived, $\mu^{(1)}_\text{max}(m)$ is the expected value of the sample maximum (Eq.~\ref{eq:mu_max}), and $\sigma^2_\text{max}(m)$ is the variance of the sample maximum (Eq.~\ref{eq:sig_max}). 

We have now derived asymptotic estimates of the moments of the max-min normalized $L_q$ distance metric (Eq.~\ref{eq:D*}) for any continuous data distribution. In the next two sections, we examine the max-min normalized $L_q$ distance on standard normal and standard uniform data. As in previous sections in which we analyzed the standard $L_q$ metric (Eq.~\ref{eq:D}), we will use the more general results for the max-min $L_q$ metric to derive asymptotic estimates for normalized Manhattan ($q=1$) and Euclidean ($q=2$).

\subsubsection*{Standard normal data}

The standard normal distribution has zero skewness, even density function, infinite support, and all moments. This implies that the corresponding mean and variance of the distribution of sample range can be expressed exclusively in terms of the sample maximum. Given the nature of the density function of the sample maximum for sample size $m$, the integration required to determine the moments (Eqs.~\ref{eq:mu_max} and \ref{eq:mu2_max}) is not possible. These moments can either be approximated numerically or we can use extreme value theory to determine the form of the asymptotic distribution of the sample maximum. Using the latter method, we will show that the asymptotic distribution of the sample maximum for standard normal data is in the Gumbel family. Let $c_m = -\Phi^{-1}\left(\frac{1}{m}\right)$ and $b_m = \frac{1}{c_m}$, where $\Phi$ is the standard normal cumulative distribution function. Using Taylor's Theorem, we have the following expansion
%
\begin{equation}\label{eq:log_expand}
\begin{aligned}
\text{log}\Phi(-c_m - b_m x) &= \text{log}\Phi(-c_m) - b_m x \frac{\phi(-c_m)}{\Phi(-c_m)} + \mathcal{O}(b^2_m x^2) \\
&= \text{log}\left(\frac{1}{m}\right) - x \frac{\phi(-c_m)}{c_m \Phi(-c_m)} + \mathcal{O}(b^2_m x^2),
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived.

In order to simplify the right-hand side of this expansion (Eq.~\ref{eq:log_expand}), we will use the well known Mills Ratio Bounds \cite{chatterjee2014} given by the following
%
\begin{equation}\label{eq:mills}
1 \leq \frac{\phi(x)}{x \Phi(-x)} \leq 1 + \frac{1}{x^2} \quad , x > 0,
\end{equation}
%
where $\Phi$ and $\phi$ once again represent the cumulative distribution function and density function, respecively, of the standard normal distribution.

The inequalities given above (Eq.~\ref{eq:mills}) show that 
%
\[\frac{\phi(x)}{x \Phi(-x)} \rightarrow 1 \text{ as } x \rightarrow \infty.
\] 

This further implies that 
%
\[
\frac{\phi(c_m)}{c_m \Phi(-c_m)} \rightarrow 1 \text{ as } m \rightarrow \infty
\] 
%
since 
%
\[
c_m = -\Phi^{-1}\left(\frac{1}{m}\right) \rightarrow \infty \text{ as } m \rightarrow \infty.
\] 

This gives us the following approximation of the right-hand side of the expansion (Eq.~\ref{eq:log_expand}) given previously
%
\begin{equation}\label{eq:approx_log_expand}
\begin{aligned}
\text{log}\Phi(-c_m - b_m x) &\approx \text{log}\left(\frac{1}{m}\right) - x + \mathcal{O}(b^2_m x^2) \\
\Rightarrow \Phi(-c_m - b_m x) &\approx \frac{1}{m}e^{-x + \mathcal{O}(b^2_m x^2)} \\
\Rightarrow \Phi(c_m + b_m x) &\approx 1 - \frac{1}{m}e^{-x + \mathcal{O}(b^2_m x^2)},
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived.

Using the approximation of expansion given previously (Eq.~\ref{eq:approx_log_expand}), we now derive the limit distribution for the sample maximum in standard normal data as
%
\begin{equation}\label{eq:prob_normal_max}
\begin{aligned}
\text{P}\left(\frac{X^\text{max}_a - c_m}{b_m} \leq x\right) &= \text{P}(X^\text{max}_a \leq c_m + b_m x) \\
&= \Phi^m(c_m + b_m x) \\
&\approx \left(1 - \frac{1}{m}e^{-x + \mathcal{O}(b^2_m x^2)}\right)^m \\
&= \left(1 - \frac{1}{m}e^{-x + \mathcal{O}\left(\frac{1}{c^2_m} x^2\right)}\right)^m \\
&\approx \left(1 - \frac{1}{m}e^{-x}\right)^m \\
\Rightarrow \lim_{m \to \infty} \text{P}\left(\frac{X^\text{max}_a - c_m}{b_m} \leq x\right) &= \lim_{m \to \infty} \left(1 - \frac{1}{m}e^{-x}\right)^m \\
&= e^{-e^{-x}},
\end{aligned}
\end{equation}
%
which is the cumulative distribution function of the standard Gumbel distribution. The mean of this distribution is given by the following
%
\begin{equation}\label{eq:mu_max_normal}
\text{E}(X^\text{max}_a) = \mu^{(1)}_\text{max} = -\Phi^{-1} \left(\frac{1}{m}\right) - \frac{\gamma}{\Phi^{-1}\left(\frac{1}{m}\right)},
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived and $\gamma$ is the well known Euler-Mascheroni constant. This constant has many equivalent definitions, one of which is given by
%
\[
\gamma = \lim_{m \to \infty} \left(-\text{log}(m) + \sum^{m}_{k=1}\frac{1}{k}\right).
\]

Perhaps a more convenient definition of the Euler-Mascheroni constant is simply
%
\[
\gamma = - \Gamma^\prime (1) = \frac{\text{d}}{\text{d}t} \left(\int^{\infty}_{0} z^{t-1} e^{-z} \text{d}z\right) \biggr|_{t=1},
\]
%
which is just the additive inverse of the first derivative of the gamma function evaluated at 1.

The median of the distribution of the maximum for standard normal data is given by
%
\begin{equation}\label{eq:med_max_normal}
\overset{\sim}{\mu}_\text{max} = \frac{\text{log}(\text{log}(2))}{\Phi^{-1}\left(\frac{1}{m}\right)} - \Phi^{-1}\left(\frac{1}{m}\right),
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived.

Finally, the variance of the asymptotic distribution of the sample maximum is given by
%
\begin{equation}\label{eq:var_max_normal}
\text{Var}(X^\text{max}_a) = \frac{\pi^2}{6}\left(\frac{1}{-\Phi^{-1}\left(\frac{1}{m}\right)}\right)^2,
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived.

For typical sample sizes $m$ in high-dimensional spaces, the variance estimate (Eq.~\ref{eq:var_max_normal}) exceeds the variance of the sample maximum significantly. Using the fact that
%
\[
-\Phi^{-1}\left(\frac{1}{m}\right) \overset{.}{\sim} \sqrt{2 \text{log}(m)} \text{\cite{cramer1999}}
\] 
%
and 
%
\[
\frac{1}{2 \text{log}(m)} \leq \left(\frac{1}{-\Phi^{-1}\left(\frac{1}{m}\right)}\right)^2, \quad m \geq 2,
\] 
%
we can get a more accurate approximation of the variance with the following
%
\begin{equation}\label{eq:var_max_normal_improved}
\begin{aligned}
\sigma^2_\text{max}(m) = \text{Var}(X^\text{max}_a) &\approx \frac{\pi^2}{6}\left(\frac{1}{\sqrt{2\text{log}(m)}}\right)^2 \\
&= \frac{\pi^2}{12\text{log}(m)}.
\end{aligned}
\end{equation}

Therefore, the mean of the range of $m$ iid standard normal random variables is given by
%
\begin{equation}\label{eq:mu_rng_normal}
\text{E}(X^\text{max}_a - X^\text{min}_a) = 2\mu^{(1)}_\text{max}(m) = 2\left[-\Phi^{-1} \left(\frac{1}{m}\right) - \frac{\gamma}{\Phi^{-1}\left(\frac{1}{m}\right)}\right],
\end{equation}
%
where $\gamma$ is the Euler-Mascheroni constant.

It is well known that the sample extremes from the standard normal distribution are approximately uncorrelated for large sample size $m$ \cite{gumbel1947}. This implies that we can approximate the variance of the range of $m$ iid standard normal random variables with the following result
%
\begin{equation}\label{eq:var_rng_normal}
\begin{aligned}
\text{Var}(X^\text{max}_a - X^\text{min}_a) &\approx \text{Var}(X^\text{max}_a) + \text{Var}(X^\text{min}_a) \\
&= \sigma^2_\text{max}(m) + \sigma^2_\text{min}(m) \\
&= 2\sigma^2_\text{max}(m) \\
&\approx 2\left(\frac{\pi^2}{12\text{log}(m)}\right) \\
&= \frac{\pi^2}{6\text{log}(m)}.
\end{aligned}
\end{equation}

For the purpose of approximating the mean and variance of the max-min normalized distance distribution, we observe empirically that the formula for the median of the distribution of the attribute maximum (Eq.~\ref{eq:med_max_normal}) yields more accurate results. More precisely, the approximation of the expected maximum (Eq.~\ref{eq:mu_max_normal}) overestimates the sample maximum slightly. The formula for the median of the sample maximum (Eq.~\ref{eq:med_max_normal}) provides a more accurate estimate of this sample extreme. Therefore, the following estimate for the mean of the attribute range will be used instead
%
\begin{equation}\label{eq:mu_rng_normal_improved}
\text{E}(X^\text{max}_a - X^\text{min}_a) = 2\mu^{(1)}_\text{max}(m) \approx 2\left[\frac{\text{log}(\text{log}(2))}{\Phi^{-1}\left(\frac{1}{m}\right)} - \Phi^{-1}\left(\frac{1}{m}\right)\right],
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived.

We have already determined the mean and variance (Eq.~\ref{eq:normalDistr}) for the $L_q$ metric (Eq.~\ref{eq:D}) on standard normal data. Using the expected value of the sample maximum (Eq.~\ref{eq:mu_rng_normal_improved}), the variance of the sample maximum (Eq.~\ref{eq:var_rng_normal}), and the general formulas for the mean and variance of the max-min normalized distance distribution (Eq.~\ref{eq:max-min_DDistr}), this leads us to the following asymptotic estimate for the distribution of the max-min normalized distances for standard normal data
%
\begin{equation}\label{eq:max-min_DDistr_normal}
D^{(q*)}_{ij} \overset{.}{\sim} \mathcal{N}\left(\frac{\mu_{D^{(q)}_{ij}}}{2\mu^{(1)}_\text{max}(m)}, \frac{6 \text{log}(m) \sigma^2_{D^{(q)}_{ij}}}{\pi^2 + 24 \left[\mu^{(1)}_\text{max}(m)\right]^2 \text{log}(m)}\right).
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived, $\mu^{(1)}_\text{max}$ is the median of the sample maximum (Eq.~\ref{eq:med_max_normal}), $\mu_{D^{(q)}_{ij}}$ is the expected $L_q$ pairwise distance (Eq.~\ref{eq:normalDqMean}), and $\sigma^2_{D^{(q)}_{ij}}$ is the variance of the $L_q$ pairwise distance (Eq.~\ref{eq:normalVar}). The moments of the max-min normalized $L_q$ distance metric in standard normal data (Eq.~\ref{eq:max-min_DDistr_normal}) are summarized in a figure that is organized by metric, data type, statistic (mean or variance), and asymptotic formula (Fig.~\ref{tab:dist_distr_general1}).

\subsubsection*{Standard uniform data}

Standard uniform data does not have an even density function. Due to the simplicity of the density function, however, we can derive the distribution of the maximum and minimum of a sample of size $m$ explicitly. Using the general forms of the distribution functions of the maximum (Eq.~\ref{eq:exact_max_distr_fn}) and minimum (Eq.~\ref{eq:exact_min_distr_fn}), we have the following distribution functions for standard uniform data
%
\begin{equation}\label{eq:uniform_max_distr}
F_\text{max}(x) = x^m
\end{equation}
%
and
%
\begin{equation}\label{eq:uniform_min_distr}
F_\text{min}(x) = 1 - (1 - x)^m,
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived.

Using the general forms of the density functions of the maximum (Eq.~\ref{eq:exact_max_dens_fn}) and minimum (Eq.~\ref{eq:exact_min_dens_fn}), we have the following density functions for standard uniform data
%
\begin{equation}\label{eq:uniform_max_dens}
f_\text{max}(x) = m x^{m-1}
\end{equation}
%
and
%
\begin{equation}\label{eq:uniform_min_dens}
f_\text{min}(x) = m(1 - x)^{m-1},
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived.

Then the expected maximum and minimum are computed through straightforward integration as follows
%
\begin{equation}\label{eq:mu_max_uniform}
\begin{aligned}
\text{E}(X^\text{max}_a) = \mu^{(1)}_\text{max}(m) &= \int_{0}^{1} x f_\text{max}(x) \text{d}x \\
&= \int_{0}^{1} x [m x^{m-1}] \text{d}x \\
&= \frac{m}{m+1}
\end{aligned}
\end{equation}
%
and
%
\begin{equation}\label{eq:mu_min_uniform}
\begin{aligned}
\text{E}(X^\text{min}_a) = \mu^{(1)}_\text{min}(m) &= \int_{0}^{1} x f_\text{min}(x) \text{d}x \\
&= \int_{0}^{1} x [m (1 - x)^{m-1}] \text{d}x \\
&= \frac{1}{m+1},
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived.

We can compute the second moment about the origin of the sample range as follows
%
\begin{equation}\label{eq:mu2_rng_uniform}
\begin{aligned}
\text{E}[(X^\text{max}_a - X^\text{min}_a)^2] &= \text{E}[(X^\text{max}_a)^2 - 2 X^\text{max}_a X^\text{min}_a + (X^\text{min}_a)^2] \\
&= \text{E}[(X^\text{max}_a)^2] - 2 \text{E}(X^\text{max}_a) \text{E}(X^\text{min}_a) + \text{E}[(X^\text{min}_a)^2] \\
&= \mu^{(2)}_\text{max}(m) - 2 \mu^{(1)}_\text{max}(m) \mu^{(1)}_\text{min}(m) + \mu^{(2)}_\text{min}(m) \\
&= \int_{0}^{1} x^2 [m x^{m-1}] \text{d}x - 2 \left(\frac{m}{m+1}\right) \left(\frac{1}{m+1}\right) \\
& \hspace{3.12cm} + \int_{0}^{1} x^2 [m (1 - x)^{m-1}] \text{d}x \\
&= \frac{m}{m+2} - \frac{2m}{(m+1)^2} + \frac{2}{(m+1)(m+2)} \\
&= \frac{m^3 - m + 2}{(m+2)(m+1)^2},
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived.

Using the general asymptotic distribution of max-min normalized distances for any data type (Eq.~\ref{eq:max-min-DDistr-general}) and the mean and variance (Eq.~\ref{eq:uniformDistr}) of the standard $L_q$ distance metric (Eq.~\ref{eq:D}), we have the following asymptotic estimate for the max-min normalized distance distribution for standard uniform data
%
\begin{equation}\label{eq:max-min_DDistr_uniform}
D^{(q*)}_{ij} \overset{.}{\sim} \mathcal{N}\left(\frac{(m+1)\mu_{D^{(q)}_{ij}}}{m-1}, \frac{(m+2)(m+1)^2 \sigma^2_{D^{(q)}_{ij}}}{m^3 - m + 2}\right),
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived, $\mu_{D^{(q)}_{ij}}$ is the expected value (Eq.~\ref{eq:uniformDqMean}) of the $L_q$ metric (Eq.~\ref{eq:D}) in standard uniform data, and $\sigma^2_{D^{(q)}_{ij}}$ is the variance (Eq.~\ref{eq:uniformDqVar}) of the $L_q$ metric (Eq.~\ref{eq:D}) in standard uniform data. The moments of the max-min normalized $L_q$ distance metric in standard uniform data (Eq.~\ref{eq:max-min_DDistr_normal}) are summarized in a figure that is organized by metric, data type, statistic (mean or variance), and asymptotic formula (Fig.~\ref{tab:dist_distr_general1}).

\subsection*{Range-Normalized Manhattan \texorpdfstring{($q=1$)}{}}

Using the general asymptotic results for mean and variance of max-min normalized distances in standard normal and standard uniform data (Eqs.~\ref{eq:max-min_DDistr_normal} and \ref{eq:max-min_DDistr_uniform}) for any value of $q \in \mathbb{N}$, we can substitute a particular value of $q$ in order to determine a more specified distribution for the normalized distance ($D^{(q*)}$, Eq.~\ref{eq:D*}). The following results are for the max-min normalized Manhattan ($q = 1$), $D^{(1*)}$, metric for both standard normal and standard uniform data.

\subsubsection*{Standard normal data}

Substituting $q=1$ into the asymptotic formula for the expected max-min normalized distance (Eq.~\ref{eq:max-min_DDistr_normal}), we derive the expected normalized Manhattan distance in standard normal data as follows
%
\begin{equation}\label{eq:max-min_mean_normal_manhattan}
\begin{aligned}
\text{E}\left(D^{(1*)}_{ij}\right) &= \frac{\mu_{D^{(1)}_{ij}}}{2\mu^{(1)}_\text{max}(m)} \\
&= \frac{p}{\sqrt{\pi}\mu^{(1)}_\text{max}(m)},
\end{aligned}
\end{equation}
%
where $\mu^{(1)}_\text{max}(m)$ is the expected attribute maximum (Eq.~\ref{eq:med_max_normal}), $m$ is the size of the sample from which the maximum is derived, and $p$ is the total number of attributes.

Similarly, the variance of $D^{(1*)}_{ij}$ is given by
%
\begin{equation}\label{eq:max-min_var_normal_manhattan}
\begin{aligned}
\text{Var}\left(D^{(1*)}_{ij}\right) &= \frac{6\text{log}(m)\sigma^2_{D^{(1)}_{ij}}}{\pi^2 + 24\left[\mu^{(1)}_\text{max}\right]^2\text{log}(m)} \\
&= \frac{12p(\pi-2)\text{log}(m)}{\pi\left(\pi^2 + 24\left[\mu^{(1)}_\text{max}\right]^2\text{log}(m)\right)},
\end{aligned}
\end{equation}
%
where $\mu^{(1)}_\text{max}(m)$ is the expected attribute maximum (Eq.~\ref{eq:med_max_normal}), $m$ is the size of the sample from which the maximum is derived, and $p$ is the total number of attributes. Similar to the variance of the standard Manhattan distance, the variance of the max-min normalized Manhattan distance is on the order of $p$ for fixed instance dimension $m$. For fixed $p$, the variance (Eq.~\ref{eq:max-min_var_normal_manhattan}) vanishes as $m$ grows without bound. If we fix $m$, the same variance increases monotonically with increasing $p$. The moments derived in this section (Eqs.~\ref{eq:max-min_mean_normal_manhattan} and \ref{eq:max-min_var_normal_manhattan}) are summarized in a figure that is organized by metric, data type, statistic (mean or variance), and asymptotic formula (Fig.~\ref{tab:dist_distr_normalizedL1L2}).

\subsubsection*{Standard uniform data}

Substituting $q=1$ into the asymptotic formula for the expected max-min pairwise distance (Eq.~\ref{eq:max-min_DDistr_uniform}), we derive the expected normalized Manhattan distance in standard uniform data as
%
\begin{equation}\label{eq:max-min_mean_uniform_manhattan}
\begin{aligned}
\text{E}\left(D^{(1*)}_{ij}\right) &= \frac{(m+1)\mu_{D^{(1)}_{ij}}}{m-1} \\
&= \frac{(m+1)p}{3(m-1)},
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived and $p$ is the total number attributes.

Similarly, the variance of $D^{(1*)}_{ij}$ is given by
%
\begin{equation}\label{eq:max-min_var_uniform_manhattan}
\begin{aligned}
\text{Var}\left(D^{(1*)}_{ij}\right) &= \frac{(m+2)(m+1)^2\sigma^2_{D^{(1)}_{ij}}}{m^3-m+2} \\
&= \frac{(m+2)(m+1)^2p}{18(m^3 - m + 2)},
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived and $p$ is the total number of attributes. Interestingly, the variance of the max-min normalized Manhattan distance in standard uniform data approaches $p/18$ as $m$ increases without bound for a fixed number of attributes $p$. This is the same asymptotic value to which the variance of the standard Manhattan distance (Eq.~\ref{eq:uniformManMean}) converges. Therefore, large sample sizes make the variance of the normalized Manhattan distance approach the variance of the standard Manhattan distance in standard uniform data. The moments derived in this section (Eqs.~\ref{eq:max-min_mean_uniform_manhattan} and \ref{eq:max-min_var_uniform_manhattan}) are summarized in a figure that is organized by metric, data type, statistic (mean or variance), and asymptotic formula (Fig.~\ref{tab:dist_distr_normalizedL1L2}).

\subsection*{Range-Normalized Euclidean \texorpdfstring{($q=2$)}{}}

Analogous to the previous section, we use the asymptotic moment estimates for the max-min normalized metric ($D^{(q*)}$, Eq.~\ref{eq:D*}) for standard normal (Eq.~\ref{eq:max-min_DDistr_normal}) and standard uniform (Eq.~\ref{eq:max-min_DDistr_uniform}) data but specific to a range-normalized Euclidean metric ($q=2$).

\subsubsection*{Standard normal data}

Substituting $q=2$ into the asymptotic formula for the expected max-min normalized pairwise distance (Eq.~\ref{eq:max-min_DDistr_normal}), we derive the expected normalized Euclidean distance in standard normal data as
%
\begin{equation}\label{eq:max-min_mean_normal_euclidean}
\begin{aligned}
\text{E}\left(D^{(2*)}_{ij}\right) &= \frac{\mu^{D^{(2)}_{ij}}}{2\mu^{(1)}_\text{max}(m)} \\
&= \frac{\sqrt{2p - 1}}{2\mu^{(1)}_\text{max}(m)},
\end{aligned}
\end{equation}
%
where $\mu^{(1)}_\text{max}(m)$ is the expected attribute maximum (Eq.~\ref{eq:med_max_normal}), $m$ is the size of the sample from which the maximum is derived, and $p$ is the total number of attributes.

Similarly, the variance of $D^{(2*)}_{ij}$ is given by
%
\begin{equation}\label{eq:max-min_var_normal_euclidean}
\begin{aligned}
\text{Var}\left(D^{(2*)}_{ij}\right) &= \frac{6\text{log}(m)\sigma^2_{D^{(2)}_{ij}}}{\pi^2 + 24\left[\mu^{(1)}_\text{max}(m)\right]^2\text{log}(m)} \\
&= \frac{6\text{log}(m)}{\pi^2 + 24\left[\mu^{(1)}_\text{max}(m)\right]^2\text{log}(m)},
\end{aligned}
\end{equation}
%
where $\mu^{(1)}_\text{max}(m)$ is the expected attribute maximum (Eq.~\ref{eq:med_max_normal}) and $m$ is the size of the sample from which the maximum is derived. It is interesting to note that the variance (Eq.~\ref{eq:max-min_var_normal_euclidean}) vanishes as the sample size $m$ increases without bound, which means that all distances will be tightly clustered about the mean (Eq.~\ref{eq:max-min_mean_normal_euclidean}). This is different than the variance of the standard $L_2$ metric (Eq.~\ref{eq:normalEucVar}), which is asymptotically equal to 1. This could imply that any two pairwise distances computed with the max-min normalized Euclidean metric in a large sample space $m$ may be indistinguishable, which is another curse of dimensionality. The moments derived in this section (Eqs.~\ref{eq:max-min_mean_normal_euclidean} and \ref{eq:max-min_var_normal_euclidean}) are summarized in a figure that is organized by metric, data type, statistic (mean or variance), and asymptotic formula (Fig.~\ref{tab:dist_distr_normalizedL1L2}).

\subsubsection*{Standard uniform data}

Substituting $q=2$ into the asymptotic formula for the expected max-min normalized pairwise distance (Eq.~\ref{eq:max-min_DDistr_uniform}), we derive the expected normalized Euclidean distance in standard uniform data as
%
\begin{equation}\label{eq:max-min_mean_uniform_euclidean}
\begin{aligned}
\text{E}\left(D^{(2*)}_{ij}\right) &= \frac{(m+1)\mu_{D^{(2)}_{ij}}}{m-1} \\
&= \sqrt{\frac{p}{6} - \frac{7}{120}}\left(\frac{m+1}{m-1}\right).
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived and $p$ is the total number of attributes.

Similarly, the variance of $D^{(2*)}_{ij}$ is given by
%
\begin{equation}\label{eq:max-min_var_uniform_euclidean}
\begin{aligned}
\text{Var}\left(D^{(2*)}_{ij}\right) &= \frac{(m+2)(m+1)^2\sigma^2_{D^{(2)}_{ij}}}{m^3 - m + 2} \\
&= \frac{7(m+2)(m+1)^2}{120(m^3 - m + 2)}.
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived. Similar to the variance of max-min normalized Manhattan distances in standard uniform data (Eq.~\ref{eq:max-min_var_uniform_manhattan}), the variance of normalized Euclidean distances approaches the variance of the standard Euclidean distances in uniform data (Eq.~\ref{eq:uniformEucVar}) as $m$ increases without bound. That is, the variance of the max-min normalized Euclidean distance (Eq.~\ref{eq:max-min_var_uniform_euclidean}) approaches $7/120$ as $m$ grows larger. The moments derived in this section (Eqs.~\ref{eq:max-min_mean_uniform_euclidean} and \ref{eq:max-min_var_uniform_euclidean}) are summarized in a figure that is organized by metric, data type, statistic (mean or variance), and asymptotic formula (Fig.~\ref{tab:dist_distr_normalizedL1L2}).

We summarize moment estimates in figures (Fig.~\ref{tab:dist_distr_general1}-\ref{tab:dist_distr_normalizedL1L2}) that contain all of our asymptotic results for both standard and max-min normalized $L_q$ metrics in each data type we have considered. This includes our most general results for any combination of sample size $m$, number of attributes $p$, type of metric $L_q$, and data type (Fig.~\ref{tab:dist_distr_general1}). From these more general derivations, we show the results of the standard $L_1$ and $L_2$ metrics for any combination of sample size $m$, number of attributes $p$, and data type (Fig.~\ref{tab:dist_distr_standardL1L2}). Our last set of summarized results show asymptotics for the max-min normalized $L_1$ and $L_2$ metrics for any combination of sample size $m$, number of attributes $p$, and data type (Fig.~\ref{tab:dist_distr_normalizedL1L2}). For both standard and max-min normalized $L_2$ metrics (Fig.~\ref{tab:dist_distr_standardL1L2} and \ref{tab:dist_distr_normalizedL1L2}), the low-dimensional improved estimates of sample means (Eqs.~\ref{eq:normalEucMeanImproved} and \ref{eq:uniformEucMeanImproved}) are used because they perform well at both low and high attribute dimension $p$.

In the next section, we make a transition into discrete GWAS data. We will discuss some commonly known metrics and then a relatively new metric, which will lead us into novel asymptotic results for this data type.

\begin{figure}[H]
	\centering
	%\includegraphics[width=1\textwidth]{updated_distributions_table(6-1-2020).pdf}
	\caption{{\bf Summary of distance distribution derivations for standard normal ($\mathcal{N}(0,1)$) and standard uniform ($\mathcal{U}(0,1)$) data.} Asymptotic estimates are given for both standard (Eq.~\ref{eq:D}) and max-min normalized (Eq.~\ref{eq:D*}) q-metrics. These estimates are relevant for all $q \in \mathbb{N}$ and $p \gg 1$ for which the normality assumption of distances holds.}
	\label{tab:dist_distr_general1}
\end{figure}

\begin{figure}[H]
	\centering
	%\includegraphics[width=0.73\textwidth]{updated_typical_metrics_table(6-15-2020).pdf}
	\caption{{\bf Asymptotic estimates of means and variances for the standard $L_1$ and $L_2$ ($q=1$ and $q=2$ in Fig.~\ref{tab:dist_distr_general1}) distance distributions.} Estimates for both standard normal ($\mathcal{N}(0,1)$) and standard uniform ($\mathcal{U}(0,1)$) data are given.}
	\label{tab:dist_distr_standardL1L2}
\end{figure}

\begin{figure}[H]
	\centering
	%\includegraphics[width=0.9\textwidth]{updated_typical_metrics_table(normalized_6-15-2020).pdf}
	\caption{{\bf Asymptotic estimates of means and variances for the max-min normalized $L_1$ and $L_2$ distance distributions commonly used in Relief-based algorithms.} Estimates for both standard normal ($\mathcal{N}(0,1)$) and standard uniform ($\mathcal{U}(0,1)$) data are given. The cumulative distribution function of the standard normal distribution is represented by $\Phi$. Furthermore, $\mu^{(1)}_\text{max}(m)$ (Eq.~\ref{eq:med_max_normal}) is the asymptotic median of the sample maximum from $m$ standard normal random samples.}
	\label{tab:dist_distr_normalizedL1L2}
\end{figure}

\section*{GWAS distance distributions}\label{sec:gwas_distances}

In genome-wide association studies (GWAS), data is encoded by the minor allele at a particular locus $a$, which is just the second most common allele (adenine-A, thymine-T, cytosine-C, or guanine-G) associated with a given attribute $a$ in the data set. Attributes in GWAS data are single nucleotide polymorphisms (SNPs), or mutations involving the substitution, deletion, or insertion of one nucleotide at some point in the DNA sequence of an organism. These are common mutations that can affect how an individual reacts to certain pathogens or the susceptibility for certain diseases. Feature selection in GWAS is typically concerned with finding interacting SNPs that are associated with disease susceptibility \cite{li2014}.

Consider a GWAS data set, which has the following encoding based on minor allele frequency
%
\begin{equation}\label{eq:gwas_data}
X_{ia} = \begin{cases}
0 & \text{ if there are no minor alleles at locus } a,  \\
1 & \text{ if there is 1 minor allele at locus } a, \\
2 & \text{ if there are 2 minor alleles at locus } a.
\end{cases}
\end{equation}

A minor allele at a particular locus $a$ is the least frequent of the two alleles at that particular locus $a$. For random GWAS data sets, we can think $X_{ia}$ as the number of successes in two Bernoulli trials. That is, $X_{ia} \sim \mathcal{B}(2,f_a)$ where $f_a$ is the probability of success. The success probability $f_a$ is the probability of a minor allele occurring at $a$. Furthermore, the minor allele probabilities are assumed to be independent and identically distributed according to $\mathcal{U}(l,u)$, where $l$ and $u$ are the lower and upper bounds, respectively, of the sampling distribution's support. Two commonly known types of distance metrics for GWAS data are the Genotype Mismatch (GM) and Allele Mismatch (AM) metrics. The GM and AM metrics are defined by
%
\begin{equation}\label{eq:diff_GM}
\text{d}^\text{GM}_{ij}(a) = \begin{cases} 
0 & \text{ if } X_{ia} \neq X_{ja}, \\
1 & \text{ otherwise}
\end{cases}
\end{equation}
%
and
%
\begin{equation}\label{eq:diff_AM}
\text{d}^\text{AM}_{ij}(a) = \frac{1}{2}\bigl|X_{ia} - X_{ja}\bigr|.
\end{equation}

More informative metrics may include differences at the nucleotide level for each allele by considering differences in the rates of transition and transversion mutations (Fig.~\ref{fig:TiTv_diagram}). One such discrete metric that accounts for transitions (Ti) and transversions (Tv) was introduced in \cite{arabnejad2018} and can be written as
%
\begin{equation}\label{eq:diff_TiTv}
\text{d}^\text{TiTv}_{ij}(a) = \begin{cases}
0 & \text{ if } X_{ia} = X_{ja} \text{ and Ti/Tv}, \\
1/4 & \text{ if } |X_{ia} - X_{ja}|=1 \text{ and Ti}, \\
1/2 & \text{ if } |X_{ia} - X_{ja}|=1 \text{ and Tv}, \\
3/4 & \text{ if } |X_{ia} - X_{ja}|=2 \text{ and Ti}, \\
1 & \text{ if } |X_{ia} - X_{ja}|=2 \text{ and Tv}.
\end{cases}
\end{equation}

With these GWAS distance metrics, we then compute the pairwise distance between two instances $i,j \in \mathcal{I}$ with
%
\begin{equation}\label{eq:D_GM}
D^\text{GM}_{ij}(a) = \sum_{a \in \mathcal{A}} \text{d}^\text{GM}_{ij}(a),
\end{equation}
%
\begin{equation}\label{eq:D_AM}
D^\text{AM}_{ij}(a) = \sum_{a \in \mathcal{A}} \text{d}^\text{AM}_{ij}(a), \text{ or}
\end{equation}
%
\begin{equation}\label{eq:D_TiTv}
D^\text{TiTv}_{ij}(a) = \sum_{a \in \mathcal{A}} \text{d}^\text{TiTv}_{ij}(a).
\end{equation}

Assuming that all data entries $X_{ia}$ are independent and identically distributed, we have already shown that the distribution of pairwise distances is asymptotically normal regardless of data distribution and value of $q$. Therefore, the distance distributions induced by each of the GWAS metrics (Eqs.~\ref{eq:diff_GM}-\ref{eq:diff_TiTv}) are asymptotically normal. With this Gaussian limiting behavior, we will proceed by deriving the mean and variance for each distance distribution induced by these three GWAS metrics. 

\subsection*{GM distance distribution}

The simplest distance metric in nearest-neighbor feature selection in GWAS data is the genotype-mismatch (GM) distance metric (Eq.~\ref{eq:D_GM}). The GM attribute diff (Eq.~\ref{eq:diff_GM}) indicates only whether two genotypes are the same or not. There are many ways two genotypes could differ, but this metric does not record this information. We will now derive the moments for the GM distance (Eq.~\ref{eq:D_GM}), which are sufficient for defining its corresponding asymptotic distribution.

The expected value of the GM attribute diff metric (Eq.~\ref{eq:diff_GM}) is given by the following
%
\begin{equation}\label{eq:mean_diff_GM}
\begin{aligned}
\text{E}\left[\text{d}^\text{GM}_{ij}(a)\right] &= \sum_{k=0}^{1} k \cdot \text{P}\left[\text{d}^\text{GM}_{ij}(a) = k\right] \\
&= 0 \cdot \text{P}\left[\text{d}^\text{GM}_{ij}(a) = 0\right] + 1 \cdot \text{P}\left[\text{d}^\text{GM}_{ij}(a) = 1\right] \\
&= \text{P}\left[\text{d}^\text{GM}_{ij}(a) = 1\right] \\
&= 2\text{P}[X_{ia} = 0, X_{ja} = 1] + 2\text{P}[X_{ia} = 1, X_{ja} = 2] + 2\text{P}[X_{ia} = 0, X_{ja} = 2] \\
&= 4(1 - f_a)^3f_a + 4(1 - f_a)f^3_a + 2(1 - f_a)^2f^2_a \\
&= 2\left[2(1 - f_a)^3f_a + 2(1 - f_a)f^3_a + (1 - f_a)^2f^2_a\right] \\
&= 2F^\text{GM}(a),
\end{aligned}
\end{equation}
%
where $F^\text{GM}(a) = 2(1 - f_a)^3f_a + 2(1 - f_a)f^3_a + (1 - f_a)^2f^2_a$ and $f_a$ is the probability of a minor allele occurring at locus $a$.

Then the expected pairwise GM distance between instances $i,j \in \mathcal{I}$ is given by
%
\begin{equation}\label{eq:mu_DDistr_GM}
\begin{aligned}
\text{E}\left(D^\text{GM}_{ij}\right) &= \text{E}\left(\sum_{a \in \mathcal{A}} \text{d}^\text{GM}_{ij}(a)\right) \\
&= \sum_{a \in \mathcal{A}} \text{E}\left[\text{d}^\text{GM}_{ij}(a)\right] \\
&= 2 \sum_{a \in \mathcal{A}} F^\text{GM}(a),
\end{aligned}
\end{equation}
%
where $F^\text{GM}(a) = 2(1 - f_a)^3f_a + 2(1 - f_a)f^3_a + (1 - f_a)^2f^2_a$ and $f_a$ is the probability of a minor allele occurring at locus $a$. We see that the expected GM pairwise distance (Eq.~\ref{eq:mu_DDistr_GM}) relies only on the minor allele probabilities $f_a$ for all $a \in \mathcal{A}$. In real data, we can easily determine these probabilities by dividing the total number of minor alleles at locus $a$ by the twice the number of instances $m$. To be more explicit, this is just
%
\[
f_a = \frac{1}{2m}\sum_{i \in \mathcal{I}} X_{ia},  \quad \text{ for all } a \in \mathcal{A},
\]
%
where $m$ is the number of instances (or sample size). This is because each instance has two alleles, the minor and major alleles, at each locus. Therefore, the total number of alleles at locus $a$ is $2m$.

The second moment about the origin for the GM distance is computed as follows
%
\begin{equation}\label{eq:mu2_DDistr_GM}
\begin{aligned}
\text{E}\left[\left(D^\text{GM}_{ij}\right)^2\right] &= \text{E}\left[\left(\sum_{a \in \mathcal{A}} \text{d}^\text{GM}_{ij}(a)\right)^2\right] \\
&= \text{E}\left[\sum_{a \in \mathcal{A}} \left(\text{d}^\text{GM}_{ij}(a)\right)^2\right] + 2 \text{E}\left[\sum_{r \in \mathcal{A}} \sum_{s \leq r - 1} \text{d}^\text{GM}_{ij}(r) \cdot \text{d}^\text{GM}_{ij}(s)\right] \\
&= \sum_{a \in \mathcal{A}} \left(\sum_{k = 0}^{1} k^2 \cdot \text{P}\left[\text{d}^\text{GM}_{ij}(a) = k\right]\right) \\
&+ 2\sum_{a \in \mathcal{A}} \sum_{s \leq r - 1} \left(\sum_{k = 0}^{1} k \cdot \text{P}\left[\text{d}^\text{GM}_{ij}(r) = k\right]\right) \cdot \left(\sum_{k = 0}^{1} k \cdot \text{P}\left[\text{d}^\text{GM}_{ij}(s) = k\right]\right) \\
&= 2\sum_{a \in \mathcal{A}} F^\text{GM}(a) + 8 \sum_{r \in \mathcal{A}} \sum_{s \leq r - 1} \prod_{\lambda \in \{r,s\}} F^\text{GM}(\lambda),
\end{aligned}
\end{equation}
%
where $F^\text{GM}(a) = 2(1 - f_a)^3f_a + 2(1 - f_a)f^3_a + (1 - f_a)^2f^2_a$ and $f_a$ is the probability of a minor allele occurring at locus $a$.

Using the first (Eq.~\ref{eq:mu_DDistr_GM}) and second (Eq.~\ref{eq:mu2_DDistr_GM}) raw moments of the GM distance, the variance is given by
%
\begin{equation}\label{eq:var_DDistr_GM}
\begin{aligned}
\text{Var}\left(D^\text{GM}_{ij}\right) &= \text{E}\left[\left(D^\text{GM}_{ij}\right)^2\right] - \left[\text{E}\left(D^\text{GM}_{ij}\right)\right]^2 \\
&= 2\sum_{a \in \mathcal{A}} F^\text{GM}(a) + 8\sum_{r \in \mathcal{A}} \sum_{s \leq r - 1} \prod_{\lambda \in \{r,s\}} F^\text{GM}(\lambda) - 4\left(\sum_{a \in \mathcal{A}}F^\text{GM}(a)\right)^2 \\
&= 2\sum_{a \in \mathcal{A}} F^\text{GM}(a) - 4\sum_{a \in \mathcal{A}}\left[F^\text{GM}(a)\right]^2 \\
&= 2\sum_{a \in \mathcal{A}} F^\text{GM}(a)[1 - 2F^\text{GM}(a)],
\end{aligned}
\end{equation}
%
where $F^\text{GM}(a) = 2(1 - f_a)^3f_a + 2(1 - f_a)f^3_a + (1 - f_a)^2f^2_a$ and $f_a$ is the probability of a minor allele occurring at locus $a$. Hence, the variance of the asymptotic GM distance distribution also just depends on the minor allele probabilities $f_a$ for all $a \in \mathcal{A}$. This implies that the limiting GM distance distribution is fully determined by the minor allele probabilities, which are known in real data.

With the mean and variance estimates (Eqs.~\ref{eq:mu_DDistr_GM} and \ref{eq:var_DDistr_GM}), the asymptotic GM distance distribution is given by the following
%
\begin{equation}\label{eq:DDistr_GM}
D^\text{GM}_{ij} \overset{.}{\sim} \mathcal{N}\left(2\sum_{a \in \mathcal{A}} F^\text{GM}(a), 2\sum_{a \in \mathcal{A}} F^\text{GM}(a)[1 - 2F^\text{GM}(a)]\right),
\end{equation}
%
where $F^\text{GM}(a) = 2(1 - f_a)^3f_a + 2(1 - f_a)f^3_a + (1 - f_a)^2f^2_a$ and $f_a$ is the probability of a minor allele occurring at locus $a$. This GM distribution holds for random independent GWAS data with minor allele probabilities $f_a$ and binomial samples $X_{ia} \sim \mathcal{B}(2,f_a)$ for all $a \in \mathcal{A}$. Next we consider the distance distribution for an AM metric, which incorporates differences at the allele level and contains more information than genotype differences.

\subsection*{AM distance distribution}

As we have mentioned previously, the AM attribute diff metric (Eq.~\ref{eq:diff_AM}) is slightly more dynamic than the GM metric because the AM metric accounts for differences between the alleles of two genotypes. In this section, we derive moments of the AM distance metric (Eq.~\ref{eq:D_AM}) that adequately define its corresponding asymptotic distribution.

The expected value of the AM attribute diff metric (Eq.~\ref{eq:diff_AM}) is given by the following
%
\begin{equation}\label{eq:mean_diff_AM}
\begin{aligned}
\text{E}\left[\text{d}^\text{AM}_{ij}(a)\right] &= \sum_{k \in \mathcal{D}} k \cdot \text{P}\left[\text{d}^\text{AM}_{ij}(a) = k\right] \\
&= 0 \cdot \text{P}\left[\text{d}^\text{AM}_{ij}(a) = 0\right] + \frac{1}{2} \cdot \text{P}\left[\text{d}^\text{AM}_{ij}(a) = \frac{1}{2}\right] + 1 \cdot \text{P}\left[\text{d}^\text{AM}_{ij}(a) = 1\right] \\
&= \frac{1}{2}\left(2 \text{P}\left[X_{ia} = 0, X_{ja} = 1\right] + 2 \text{P}\left[X_{ia} = 1, X_{ja} = 2\right]\right) \\
&+ 2 \text{P}\left[X_{ia} = 0, X_{ja} = 2\right] \\
&= \text{P}\left[X_{ia} = 0, X_{ja} = 1\right] + \text{P}\left[X_{ia} = 1, X_{ja} = 2\right] + 2 \text{P}\left[X_{ia} = 0, X_{ja} = 2\right] \\
&= 2(1 - f_a)^3f_a + 2(1 - f_a)f^3_a + 2(1 - f_a)^2 f^2_a \\
&= 2\left[(1 - f_a)^3f_a + (1 - f_a)f^3_a + (1 - f_a)^2 f^2_a\right] \\
&= 2F^\text{AM}(a),
\end{aligned}
\end{equation}
%
where $F^\text{AM}(a) = (1 - f_a)^3f_a + (1 - f_a)f^3_a + (1 - f_a)^2 f^2_a$, $\mathcal{D} = \{0,1/2,1\}$, and $f_a$ is the probability of a minor allele occurring at locus $a$.

Using the expected AM attribute diff (Eq.~\ref{eq:mean_diff_AM}), the expected pairwise AM distance (Eq.~\ref{eq:D_AM}) between instances $i,j \in \mathcal{I}$ is given by
%
\begin{equation}\label{eq:mu_DDistr_AM}
\begin{aligned}
\text{E}\left(D^\text{AM}_{ij}\right) &= \text{E}\left(\sum_{a \in \mathcal{A}} \text{d}^\text{AM}_{ij}(a)\right) \\
&= \sum_{a \in \mathcal{A}} \text{E}\left[\text{d}^\text{AM}_{ij}(a)\right] \\
&= 2 \sum_{a \in \mathcal{A}} F^\text{AM}(a).
\end{aligned}
\end{equation}
%
where $F^\text{AM}(a) = (1 - f_a)^3f_a + (1 - f_a)f^3_a + (1 - f_a)^2 f^2_a$ and $f_a$ is the probability of a minor allele occurring at locus $a$. Similar to GM distances, the expected AM distance (Eq.~\ref{eq:mu_DDistr_AM}) depends only on the minor allele probabilities $f_a$ for all $a \in \mathcal{A}$. This is to be expected because, although the AM metric is more informative, it still only accounts for simple differences between nucleotides of two instances $i,j \in \mathcal{I}$ at some locus $a$.

The second moment about the origin for the AM distance is computed as follows
%
\begin{equation}\label{eq:mu2_DDistr_AM}
\begin{aligned}
\text{E}\left[\left(D^\text{AM}_{ij}\right)^2\right] &= \text{E}\left[\left(\sum_{a \in \mathcal{A}} \text{d}^\text{AM}_{ij}(a)\right)^2\right] \\
&= \text{E}\left[\sum_{a \in \mathcal{A}} \left(\text{d}^\text{AM}_{ij}(a)\right)^2\right] + 2 \text{E}\left[\sum_{r \in \mathcal{A}} \sum_{s \leq r - 1} \text{d}^\text{AM}_{ij}(r) \cdot \text{d}^\text{AM}_{ij}(s)\right] \\
&= \sum_{a \in \mathcal{A}} \left(\sum_{k \in \mathcal{D}} k^2 \cdot \text{P}\left[\text{d}^\text{AM}_{ij}(a) = k\right]\right) \\
&+ 2\sum_{a \in \mathcal{A}} \sum_{s \leq r - 1} \left(\sum_{k \in \mathcal{D}} k \cdot \text{P}\left[\text{d}^\text{AM}_{ij}(r) = k\right]\right) \cdot \left(\sum_{k \in \mathcal{D}} k \cdot \text{P}\left[\text{d}^\text{AM}_{ij}(s) = k\right]\right) \\
&= \sum_{a \in \mathcal{A}} G^\text{AM}(a) + 8 \sum_{r \in \mathcal{A}} \sum_{s \leq r - 1} \prod_{\lambda \in \{r,s\}} F^\text{AM}(\lambda),
\end{aligned}
\end{equation}
%
where $G^\text{AM}(a) = (1 - f_a)^3 f_a + f^3_a (1 - f_a) + 2 (1 - f_a)^2 f^2_a$, $F^\text{AM}(\lambda) = (1 - f_\lambda)^3 f_\lambda + f^3_\lambda (1 - f_\lambda) + (1 - f_\lambda)^2 f^2_\lambda$, and $f_a$ is the probability of a minor allele occurring at locus $a$.

Using the first (Eq.~\ref{eq:mu_DDistr_AM}) and second (Eq.~\ref{eq:mu2_DDistr_AM}) raw moments of the asymptotic AM distance distribution, the variance is given by
%
\begin{equation}\label{eq:var_DDistr_AM}
\begin{aligned}
\text{Var}\left(D^\text{AM}_{ij}\right) &= \text{E}\left[\left(D^\text{AM}_{ij}\right)^2\right] - \left[\text{E}\left(D^\text{AM}_{ij}\right)\right]^2 \\
&= \sum_{a \in \mathcal{A}} G^\text{AM}(a) + 8\sum_{r \in \mathcal{A}} \sum_{s \leq r - 1} \prod_{\lambda \in \{r,s\}} F^\text{AM}(\lambda) - 4\left(\sum_{a \in \mathcal{A}}F^\text{AM}(a)\right)^2 \\
&= \sum_{a \in \mathcal{A}} G^\text{AM}(a) - 4\sum_{a \in \mathcal{A}}\left[F^\text{AM}(a)\right]^2 \\
&= \sum_{a \in \mathcal{A}} \left(G^\text{AM}(a)- 4\left[F^\text{AM}(a)\right]^2\right),
\end{aligned}
\end{equation}
%
where $G^\text{AM}(a) = (1 - f_a)^3 f_a + f^3_a (1 - f_a) + 2 (1 - f_a)^2 f^2_a$, $F^\text{AM}(a) = (1 - f_a)^3 f_a + f^3_a (1 - f_a) + (1 - f_a)^2 f^2_a$, and $f_a$ is the probability of a minor allele occurring at locus $a$. Similar to the mean (Eq.~\ref{eq:mu_DDistr_AM}), the variance just depends on minor allele probabilities $f_a$ for all $a \in \mathcal{A}$.

With the mean (Eq.~\ref{eq:mu_DDistr_AM}) and variance (Eq.~\ref{eq:var_DDistr_AM}) estimates of AM distances, the asymptotic AM distance distribution is given by the following
%
\begin{equation}\label{eq:DDistr_AM}
D^\text{AM}_{ij} \overset{.}{\sim} \mathcal{N}\left(2\sum_{a \in \mathcal{A}} F^\text{AM}(a), \sum_{a \in \mathcal{A}} \left(G^\text{AM}(a)- 4\left[F^\text{AM}(a)\right]^2\right)\right),
\end{equation}
%
where $G^\text{AM}(a) = (1 - f_a)^3 f_a + f^3_a (1 - f_a) + 2 (1 - f_a)^2 f^2_a$, $F(a) = (1 - f_a)^3 f_a + f^3_a (1 - f_a) + (1 - f_a)^2 f^2_a$, and $f_a$ is the probability of a minor allele occurring at locus $a$.

This concludes our analysis of the AM metric in GWAS data when the independence assumption holds for minor allele probabilities $f_a$ and binomial samples $\mathcal{B}(2,f_a)$ for all $a \in \mathcal{A}$. In the next section, we derive more complex asymptotic results for the TiTv distance metric (Eq.~\ref{eq:D_TiTv}).

\subsection*{TiTv distance distribution}\label{sec:TiTv_distances}

The TiTv metric allows for one to account for both genotype mismatch, allele mismatch, transition, and transversion. However, this added dimension of information requires knowledge of the nucleotide makeup at a particular locus. A sufficient condition to compute the TiTv metric between instances $i,j \in \mathcal{I}$ is that we know whether the nucleotides associated with a particular locus $a$ are both purines (PuPu), purine and pyrimidine (PuPy), or both pyrimidines (PyPy). We illustrate all possibilities for transitions and transversions in a diagram (Fig.~\ref{fig:TiTv_diagram}). Purines (A and G) and pyrimidines (C and T) are shown at the top and bottom, respectively. Transitions occur in the cases of PuPu and PyPy, while transversion occurs only with PuPy encoding.

\bigskip

\begin{minipage}[c]{0.62\textwidth}\hspace{-0.5cm}
	%\framebox{\includegraphics[width=0.95\textwidth]{TiTv_diagram2.pdf}}
\end{minipage}\hspace{-0.3cm}
\begin{minipage}[c]{0.35\textwidth}
	\captionsetup{type=figure}\captionof{figure}{{\bf Purines (A and G) and pyrimidines (C and T) are shown.} Transitions occur when a mutation involves purine-to-purine or pyrimidine-to-pyrimidine insertion. Transversions occur when a purine-to-pyrimidine or pyrimidine-to-purine insertion happens, which is a more extreme case. There are visibly more possibilities for transversions to occur than there are transitions, but there are about twice as many transitions in real data.}\label{fig:TiTv_diagram}
\end{minipage}

\bigskip

This additional encoding is always given in a particular GWAS data set, which leads us to consider the probabilities of PuPu, PuPy, and PyPy. These will be necessary to determine asymptotics for the TiTv distance metric. Let $\gamma_0$, $\gamma_1$, and $\gamma_2$ denote the probabilities of PuPu, PuPy, and PyPy, respectively, for the $p$ loci of data matrix $X$. In real data, there are approximately twice as many transitions as there are transversions. That is, the probability of a transition $\text{P}(\text{Ti})$ is approximately twice the probability of transversion $\text{P}(\text{Tv})$. It is likely that any particular data set will not satisfy this criterion exactly. In this general case, we have $\text{P}(\text{Ti})$ being equal to some multiple $\eta$ times $\text{P}(\text{Tv})$. In order to enforce this general constraint in simulated data, we define the following set of equalities
%
\begin{alignat}{2}\label{eq:TiTv_constraints1}
\gamma_0 + \gamma_1 + \gamma_2 &= 1, \\ \label{eq:TiTv_constraints2}
\text{P}(\text{Ti}) - \eta \text{P}(\text{Tv}) &= 0.
\end{alignat}

The sum-to-one constraint (Eq.~\ref{eq:TiTv_constraints1}) is natural in this context because there are only three possible genotype encodings at a particular locus, which are PuPu, PuPy, and PyPy. Solving the Ti/Tv ratio constraint (Eq.~\ref{eq:TiTv_constraints2}) for $\eta$ gives
%
\[
\eta = \frac{\text{P}(\text{Ti})}{\text{P}(\text{Tv})},
\]
%
which is easily computed in a real data set by dividing the fraction of Ti out of the total $p$ loci by the fraction of Tv out of the total $p$ loci. We will use the simplified notation $\eta=\text{Ti}/\text{Tv}$ to represent this factor for the remainder of this work.

Using this PuPu, PuPy, and PyPy encoding, the probability of a transversion occurring at any fixed locus $a$ is given by the following
%
\begin{equation}\label{eq:prob_Tv}
\begin{aligned}
\text{P}(\text{Tv}) &= \gamma_1.
\end{aligned}
\end{equation}

Using the sum-to-one constraint (Eqs.~\ref{eq:TiTv_constraints1}) and the probability of transversion (Eq.~\ref{eq:TiTv_constraints2}), the probability of a transition occuring at locus $a$ is computed as follows
%
\begin{equation}\label{eq:prob_Ti}
\begin{aligned}
\text{P}(\text{Ti}) &= \gamma_0 + \gamma_2.
\end{aligned}
\end{equation}

Also using the sum-to-one constraint (Eq.~\ref{eq:TiTv_constraints1}) and the Ti/Tv ratio constraint (Eq.~\ref{eq:TiTv_constraints2}), it is clear that we have $\text{P}(\text{Tv}) = \frac{1}{\eta + 1}$ and $\text{P}(\text{Ti}) = \frac{\eta}{\eta + 1}$. Without loss of generality, we then sample 
%
\begin{equation}\label{eq:gamma0}
\gamma_0 \sim \mathcal{U}\left(\varepsilon,\frac{\eta}{\eta + 1} - \varepsilon\right),
\end{equation}
%
where $\varepsilon$ is some small positive real number.

Then it immediately follows that we have 
%
\begin{equation}\label{eq:gamma2}
\gamma_2 = \frac{\eta}{\eta + 1} - \gamma_0.
\end{equation}

However, we can derive the mean and variance of the distance distribution induced by the TiTv metric without specifying any relationship between $\gamma_0$, $\gamma_1$, and $\gamma_2$. We proceed by computing $\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = k\right]$ for each $k \in \mathcal{D} = \bigl\{0,\frac{1}{4},\frac{1}{2},\frac{3}{4},1\bigr\}$. Let $y$ represent a random sample of size $p$ from $\{0,1,2\}$, where 
%
\begin{equation}\label{eq:yvec}
y_a = \begin{cases}
0 & \text{if locus } a \text{ is PuPu}, \\
1 & \text{if locus } a \text{ is PuPy}, \\
2 & \text{if locus } a \text{ is PyPy}.
\end{cases}
\end{equation}

We derive $\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = 0\right]$ as follows
%
\begin{equation}\label{eq:prob_TiTv_0}
\begin{aligned}
\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = 0\right] &= \text{P}\left[y_a = 0, X_{ia} = X_{ja}\right] \\
&+ \text{P}\left[y_a = 1, X_{ia} = X_{ja}\right] \\
&+ \text{P}\left[y_a = 2, X_{ia} = X_{ja}\right] \\
&= \gamma_0 \left[(1 - f_a)^2 + 4 f_a (1 - f_a) + f^2_a\right] \\
&+ \gamma_1 \left[(1 - f_a)^2 + 4 f_a (1 - f_a) + f^2_a\right] \\
&+ \gamma_2 \left[(1 - f_a)^2 + 4 f_a (1 - f_a) + f^2_a\right] \\
&= (\gamma_0 + \gamma_1 + \gamma_2)\left[(1 - f_a)^2 + 4 f_a (1 - f_a) + f^2_a\right] \\
&= (1 - f_a)^2 + 4 f_a (1 - f_a) + f^2_a,
\end{aligned}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$.

We derive $\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = \frac{1}{4}\right]$ as follows
%
\begin{equation}\label{eq:prob_TiTv_0.25}
\begin{aligned}
\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = \frac{1}{4}\right] &= 2 \text{P}\left[y_a = 0, X_{ia} = 0, X_{ja} = 1\right] \\
&+ 2 \text{P}\left[y_a = 0, X_{ia} = 1, X_{ja} = 2\right] \\
&+ 2 \text{P}\left[y_a = 2, X_{ia} = 0, X_{ja} = 1\right] \\
&+ 2 \text{P}\left[y_a = 2, X_{ia} = 1, X_{ja} = 2\right] \\
&= 4 \gamma_0 (1 - f_a)^3 f_a + 4 \gamma_0 f^3_a (1 - f_a) + 4 \gamma_2 (1 - f_a)^3 f_a \\
&+ 4 \gamma_2 f^3_a (1 - f_a) \\
&= 4 \gamma_0 \left[(1 - f_a)^3 f_a + f^3_a (1 - f_a)\right] \\
&+ 4 \gamma_2 \left[(1 - f_a)^3 f_a + f^3_a (1 - f_a)\right] \\
&= 4(\gamma_0 + \gamma_2)\left[(1 - f_a)^3 f_a + f^3_a (1 - f_a)\right],
\end{aligned}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$, $\gamma_0$ is the probability of PuPu occurring at any locus $a$,  and $\gamma_2$ is the probability of PyPy occurring at any locus $a$.

We derive $\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = \frac{1}{2}\right]$ as follows
%
\begin{equation}\label{eq:prob_TiTv_0.5}
\begin{aligned}
\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = \frac{1}{2}\right] &= 2 \text{P}\left[y_a = 1, X_{ia} = 0, X_{ja} = 1\right] \\
&+ 2 \text{P}\left[y_a = 1, X_{ia} = 1, X_{ja} = 2\right] \\
&= 4 \gamma_1 (1 - f_a)^3 f_a + 4 \gamma_1 f^3_a (1 - f_a) \\
&= 4 \gamma_1 \left[(1 - f_a)^3 f_a + f^3_a (1 - f_a)\right],
\end{aligned}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$ and $\gamma_1$ is the probability of PuPy occurring at any locus $a$.

We derive $\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = \frac{3}{4}\right]$ as follows
%
\begin{equation}\label{eq:prob_TiTv_0.75}
\begin{aligned}
\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = \frac{3}{4}\right] &= 2 \text{P}\left[y_a = 0, X_{ia} = 0, X_{ja} = 2\right] \\
&+ 2 \text{P}\left[y_a = 2, X_{ia} = 0, X_{ja} = 2\right] \\
&= 2 \gamma_0 (1 - f_a)^2 f^2_a + 2 \gamma_2 (1 - f_a)^2 f^2_a \\
&= 2(\gamma_0 + \gamma_2)(1 - f_a)^2 f^2_a,
\end{aligned}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$, $\gamma_0$ is the probability of PuPu occurring at any locus $a$, and $\gamma_2$ is the probability of PyPy occurring at any locus $a$.

We derive $\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = 1\right]$ as follows
%
\begin{equation}\label{eq:prob_TiTv_1}
\begin{aligned}
\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = 1\right] &= 2 \text{P}\left[y_a = 1, X_{ia} = 0, X_{ja} = 2\right] \\
&= 2 \gamma_1 (1 - f_a)^2 f^2_a,
\end{aligned}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$ and $\gamma_1$ is the probability of PuPy occurring at any locus $a$.

Using the TiTv diff probabilities (Eqs.~\ref{eq:prob_TiTv_0}-\ref{eq:prob_TiTv_1}), we compute the expected TiTv distance between instances $i,j \in \mathcal{I}$ as follows
%
\begin{equation}\label{eq:mu_DDistr_TiTv}
\begin{aligned}
\text{E}\left(D^\text{TiTv}_{ij}\right) &= \sum_{a \in \mathcal{A}} \left(\sum_{k \in \mathcal{D}} k \cdot \text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = k\right]\right) \\
&= (\gamma_0 + \gamma_2 + 2\gamma_1) \sum_{a \in \mathcal{A}} \left[(1 - f_a)^3 f_a + f^3_a (1 - f_a)\right] \\
&+ \left[\frac{3}{2}(\gamma_0 + \gamma_2) + 2\gamma_1\right] \sum_{a \in \mathcal{A}} (1 - f_a)^2 f^2_a \\
&= (\gamma_0 + \gamma_2 + 2\gamma_1) \sum_{a \in \mathcal{A}} F^\text{TiTv}(a) + \left[\frac{3}{2}(\gamma_0 + \gamma_2) + 2\gamma_1\right] \sum_{a \in \mathcal{A}} G^\text{TiTv}(a),
\end{aligned}
\end{equation}
%
where $F^\text{TiTv}(a) = (1 - f_a)^3 f_a + f^3_a (1 - f_a)$, $G^\text{TiTv}(a) = (1 - f_a)^2 f^2_a$, $f_a$ is the probability of a minor allele occurring at locus $a$, $\gamma_0$ is the probability of PuPu occurring at any locus $a$, $\gamma_1$ is the probability of PuPy occurring at any locus $a$, and $\gamma_2$ is the probability of PyPy occurring at any locus $a$. In contrast to the expected GM and AM distances (Eqs.~\ref{eq:mu_DDistr_GM} and \ref{eq:mu_DDistr_AM}), the expected TiTv distance (Eq.~\ref{eq:mu_DDistr_TiTv}) depends on minor allele probabilities $f_a$ for all $a \in \mathcal{A}$ and the genotype encoding probabilities $\gamma_0, \gamma_1, \text{ and } \gamma_2$. 

The second moment about the origin for the TiTv distance is computed as follows
%
\begin{equation}\label{eq:mu2_DDistr_TiTv}
\begin{split}
\begin{aligned}
\text{E}\left[\left(D^\text{TiTv}_{ij}\right)^2\right] &= \text{E}\left[\left(\sum_{a \in \mathcal{A}} \text{d}^\text{TiTv}_{ij}(a)\right)^2\right] \\
&= \text{E}\left[\sum_{a \in \mathcal{A}} \left(\text{d}^\text{TiTv}_{ij}(a)\right)^2\right] + 2 \text{E}\left[\sum_{r \in \mathcal{A}} \sum_{s \leq r - 1} \text{d}^\text{TiTv}_{ij}(r) \cdot \text{d}^\text{TiTv}_{ij}(s)\right] \\
&= \sum_{a \in \mathcal{A}} \left(\sum_{k \in \mathcal{D}} k^2 \cdot \text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = k\right]\right) \\
&+ 2\sum_{a \in \mathcal{A}} \sum_{s \leq r - 1} \left(\sum_{k \in \mathcal{D}} k \cdot \text{P}\left[\text{d}^\text{TiTv}_{ij}(r) = k\right]\right) \cdot \left(\sum_{k \in \mathcal{D}} k \cdot \text{P}\left[\text{d}^\text{TiTv}_{ij}(s) = k\right]\right) \\
&= \left[\frac{1}{4}(\gamma_0 + \gamma_2) + \gamma_1\right] \sum_{a \in \mathcal{A}} F^\text{TiTv}(a) + \left[\frac{9}{8}(\gamma_0 + \gamma_2) + 2\gamma_1\right] \sum_{a \in \mathcal{A}} G^\text{TiTv}(a) \\
\end{aligned} \\
+ 2 \sum_{r \in \mathcal{A}} \sum_{s \leq r - 1} \prod_{\lambda \in \{r,s\}} \left([\gamma_0 + \gamma_2 + 2\gamma_1] F^\text{TiTv}(\lambda) + \left[\frac{3}{2}(\gamma_0 + \gamma_2) + 2\gamma_1\right] G^\text{TiTv}(\lambda)\right),
\end{split}
\end{equation}
%
where $F^\text{TiTv}(a) = (1 - f_a)^3 f_a + f^3_a (1 - f_a)$, $G^\text{TiTv}(a) = (1 - f_a)^2 f^2_a$, $f_a$ is the probability of a minor allele occurring at locus $a$, $\gamma_0$ is the probability of PuPu occurring at any locus $a$, $\gamma_1$ is the probability of PuPy occurring at any locus $a$, and $\gamma_2$ is the probability of PyPy occurring at any locus $a$.

Using the first (Eq.~\ref{eq:mu_DDistr_TiTv}) and second (Eq.~\ref{eq:mu2_DDistr_TiTv}) raw moments of the TiTv distance, the variance is given by
%
\begin{equation}\label{eq:var_DDistr_TiTv}
\begin{split}
\begin{aligned}
\text{Var}\left(D^\text{TiTv}_{ij}\right) &= \text{E}\left[\left(D^\text{TiTv}_{ij}\right)^2\right] - \left[\text{E}\left(D^\text{TiTv}_{ij}\right)\right]^2 \\
&=\left[\frac{1}{4}(\gamma_0 + \gamma_2) + \gamma_1\right] \sum_{a \in \mathcal{A}} F^\text{TiTv}(a) + \left[\frac{9}{8}(\gamma_0 + \gamma_2) + 2\gamma_1\right] \sum_{a \in \mathcal{A}} G^\text{TiTv}(a)
\end{aligned} \\
+ 2 \sum_{r \in \mathcal{A}} \sum_{s \leq r - 1} \prod_{\lambda \in \{r,s\}} \left([\gamma_0 + \gamma_2 + 2\gamma_1] F^\text{TiTv}(\lambda) + \left[\frac{3}{2}(\gamma_0 + \gamma_2) + 2\gamma_1\right] G^\text{TiTv}(\lambda)\right) \\
\begin{aligned}
&- \left([\gamma_0 + \gamma_2 + 2\gamma_1] \sum_{a \in \mathcal{A}} F^\text{TiTv}(a) + \left[\frac{3}{2}(\gamma_0 + \gamma_2) + 2\gamma_1\right] \sum_{a \in \mathcal{A}} G^\text{TiTv}(a)\right)^2 \\
&=\left[\frac{1}{4}(\gamma_0 + \gamma_2) + \gamma_1\right] \sum_{a \in \mathcal{A}} F^\text{TiTv}(a) + \left[\frac{9}{8}(\gamma_0 + \gamma_2) + 2\gamma_1\right] \sum_{a \in \mathcal{A}} G^\text{TiTv}(a) \\
&- \sum_{a \in \mathcal{A}} \left([\gamma_0 + \gamma_2 + 2\gamma_1] F^\text{TiTv}(a) + \left[\frac{3}{2}(\gamma_0 + \gamma_2) + 2\gamma_1\right] G^\text{TiTv}(a)\right)^2,
\end{aligned}
\end{split}
\end{equation}
%
where $F^\text{TiTv}(a) = (1 - f_a)^3 f_a + f^3_a (1 - f_a)$, $G^\text{TiTv}(a) = (1 - f_a)^2 f^2_a$, $f_a$ is the probability of a minor allele occurring at locus $a$, $\gamma_0$ is the probability of PuPu occurring at any locus $a$, $\gamma_1$ is the probability of PuPy occurring at any locus $a$, and $\gamma_2$ is the probability of PyPy occurring at any locus $a$.

With the mean (Eq.~\ref{eq:mu_DDistr_TiTv}) and variance (Eq.~\ref{eq:var_DDistr_TiTv}) estimates, the asymptotic TiTv distance distribution is given by the following
%
\begin{equation}\label{eq:DDistr_TiTv}
\begin{aligned}
D^\text{TiTv}_{ij} \overset{.}{\sim} \mathcal{N}\Biggl(& (\gamma_0 + \gamma_2 + 2\gamma_1) \sum_{a \in \mathcal{A}} F^\text{TiTv}(a) + \left[\frac{3}{2}(\gamma_0 + \gamma_2) + 2\gamma_1\right] \sum_{a \in \mathcal{A}} G^\text{TiTv}(a), \\
&\left[\frac{1}{4}(\gamma_0 + \gamma_2) + \gamma_1\right] \sum_{a \in \mathcal{A}} F^\text{TiTv}(a) + \left[\frac{9}{8}(\gamma_0 + \gamma_2) + 2\gamma_1\right] \sum_{a \in \mathcal{A}} G^\text{TiTv}(a) \\
&- \sum_{a \in \mathcal{A}} \left([\gamma_0 + \gamma_2 + 2\gamma_1] F^\text{TiTv}(a) + \left[\frac{3}{2}(\gamma_0 + \gamma_2) + 2\gamma_1\right] G^\text{TiTv}(a)\right)^2\Biggr),
\end{aligned}
\end{equation}
%
where $F^\text{TiTv}(a) = (1 - f_a)^3 f_a + f^3_a (1 - f_a)$, $G^\text{TiTv}(a) = (1 - f_a)^2 f^2_a$, $f_a$ is the probability of a minor allele occurring at locus $a$, $\gamma_0$ is the probability of PuPu occurring at any locus $a$, $\gamma_1$ is the probability of PuPy occurring at any locus $a$, and $\gamma_2$ is the probability of PyPy occurring at any locus $a$.

Given upper and lower bounds $l$ and $u$, respectively, of the success probability sampling interval, the average success probability (or average MAF) is computed as follows
%
\begin{equation}\label{eq:avg_maf}
\bar{f}_a = \frac{1}{2}(l + u).
\end{equation}

The maximum TiTv distance occurs at $\bar{f}_a=0.5$ for any fixed Ti/Tv ratio $\eta$ (Eq.~\ref{eq:TiTv_constraints2}), which is the inflection point about which the minor allele changes at locus $a$ (Fig.~\ref{fig:TiTv-vs-maf}). If few minor alleles are present ($\bar{f}_a \to 0$), the predicted TiTv distance approaches 0. The same is true after the minor allele switches ($\bar{f}_a \to 1$). To explore how TiTv distance changes with increased minor allele frequency, we fixed the Ti/Tv ratio $\eta$ and generated simulated TiTv distances for $\bar{f}_a = 0.055, 0.150, 0.250, \text{ and } 0.350$ (Fig.~\ref{fig:TiTv_ridge}A). For fixed $\eta$, TiTv distance increases significantly with increased $\bar{f}_a$. We similarly fixed the average minor allele frequency $\bar{f}_a$ and generated simulated TiTv distances for $\eta = \text{Ti/Tv} = 0.5, 1, 1.5, \text{ and } 2$ (Fig.~\ref{fig:TiTv_ridge}C). The TiTv distance decreases slightly with increased $\eta = \text{Ti/Tv}$. As $\eta \to 0^+$, the data is approaching all Tv and no Ti, which means the TiTv distance is larger by definition. On the other hand, the TiTv distance decreases as $\eta \to 2^-$ because the data is approaching approximately twice as many Ti as there are Tv, which is typical for GWAS data in humans.

\vspace{0.22cm}

% made with simulate_TiTv_distances.R and LaTeX
\begin{minipage}[c]{0.65\textwidth}\hspace{-0.6cm}
	%\includegraphics[width=0.98\textwidth]{TiTv_distance-vs-maf.pdf}
\end{minipage}\hspace{-0.8cm}
\begin{minipage}[c]{0.35\textwidth}
	\captionsetup{type=figure}\captionof{figure}{{\bf Predicted average TiTv distance as a function of average minor allele frequency $\bar{f}_a$ (see Eq.~\ref{eq:avg_maf}).} Success probabilities $f_a$ are drawn from a sliding window interval from 0.01 to 0.9 in increments of about 0.009 and $m=p=100$. For $\eta=0.1$, where $\eta$ is the Ti/Tv ratio given by Eq.~\ref{eq:TiTv_constraints1}, Tv is ten times more likely than Ti and results in larger distance. Increasing to $\eta=1$, Tv and Ti are equally likely and the distance is lower.  In line with real data for $\eta=2$, Tv is half as likely as Ti so the distances are relatively small.}\label{fig:TiTv-vs-maf}
\end{minipage}

We also compared theoretical and sample moments as a function of $\eta = \text{Ti/Tv}$ and $\bar{f}_a$ for the TiTv distance metric (Fig.~\ref{fig:TiTv_ridge}B and D). We fixed $\bar{f}_a$ and computed the theoretical and simulated moments as a function of $\eta$ (Fig.~\ref{fig:TiTv_ridge}B). Theoretical average TiTv distance, given by Eq.~\ref{eq:mu_DDistr_TiTv}, and simulated TiTv average distance are approximately equal as $\eta$ increases. Theoretical standard deviation, given by Eq.~\ref{eq:var_DDistr_TiTv}, and simulated TiTv standard deviation differ slightly. We also fixed $\eta$ and computed theoretical and sample moments as a function of $\bar{f}_a$ (Fig.~\ref{fig:TiTv_ridge}D). In this case, there is approximate agreement with simulated and theoretical moments as $\bar{f}_a$ increases.

\begin{figure}[H]
	\centering
	%\framebox{\includegraphics[width=0.98\textwidth]{re_fig_5_BDedit.pdf}}
	\caption{{\bf Density curves and moments of TiTv distance as a function of average MAF $\bar{f}_a$, given by Eq.~\ref{eq:avg_maf}, and Ti/Tv ratio $\eta$, given by Eq.~\ref{eq:TiTv_constraints2}.} We fix $m=p=100$ for all simulated TiTv distances. (\textbf{A}) For fixed $\bar{f}_a=0.055$, TiTv distance density is plotted as a function of increasing $\eta$. TiTv distance decreases as $\eta$ increases. For $\eta=\text{Ti/Tv}=0.5$, there are twice as many transversions as there are transitions. On the other hand, $\eta=\text{Ti/Tv}=2$ indicates that there are half as many transversions as transitions. Since transversions encode a larger magnitude distance than transitions, this behavior is expected. (\textbf{B}) Simulated and predicted mean $\pm$ SD are shown as a function of increasing Ti/Tv ratio $\eta$. Distance decreases as Ti/Tv increases. Theoretical and simulated moments are approximately the same. (\textbf{C}) For fixed $\eta=2$, TiTv distance density is plotted as a function of increasing $\bar{f}_a$. TiTv distance increases as $\bar{f}_a$ approaches maximum of 0.5, which means that there is about the same frequency of minor alleles as major alleles. (\textbf{D}) Simulated and predicted mean $\pm$ SD as a function of increasing average MAF $\bar{f}_a$. Distance increases as the number of minor alleles increases. Theoretical and simulated moments are approximately the same.}\label{fig:TiTv_ridge}
\end{figure}

We summarize our moment estimates for GWAS distance metrics (Eqs.~\ref{eq:D_GM}-\ref{eq:D_TiTv}) (Fig.~\ref{tab:dist_distr_gwas}) organized by metric, statistic (mean or variance), and asymptotic formula. Next we consider the important case of distributions of GWAS distances projected onto a single attribute (Eqs.~\ref{eq:diff_GM}-\ref{eq:diff_TiTv}).


%\begin{figure}[H]
%	\centering
%	\framebox{\includegraphics[width=0.98\textwidth]{TiTv_distance_ridges_maf_eta.pdf}}
%	\caption{Density curves of TiTv distance as a function of average MAF $\bar{f}_a$, given by Eq.~\ref{eq:avg_maf}, and Ti/Tv ratio $\eta$, given by Eq.~\ref{eq:TiTv_constraints2}. (\textbf{A}) For fixed $\eta=2$, TiTv distance density is plotted as a function of increasing $\bar{f}_a = 0.055, 0.150, 0.250, \text{ and } 0.350$. TiTv distance increases as $\bar{f}_a$ approaches a maximum of 0.5, which means that there is about the same frequency of minor alleles as primary alleles at locus $a$. (\textbf{B}) For fixed $\bar{f}_a=0.055$, TiTv distance density is plotted as a function of increasing $\eta = 0.5, 1, 1.5, \text{ and } 2$. TiTv distance decreases as $\eta$, the Ti/Tv ratio, increases. For $\eta=\text{Ti/Tv}=0.5$, there are twice as many transversions as there are transitions. On the other hand, $\eta=\text{Ti/Tv}=2$ indicates that there are half as many transversions as there are transitions. Since transversions encode a larger magnitude distance than transitions in Eq.~\ref{eq:diff_TiTv}, this behavior is expected.}\label{fig:TiTv_hist}
%\end{figure}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.98\textwidth]{TiTv_distance_meanSD_vs_MAF_eta.pdf}
%	\caption{TiTv distance predicted and simulated moments as a function of Ti/Tv ratio $\eta$ and average MAF $\bar{f}_a$ given by Eqs.~\ref{eq:TiTv_constraints2} and \ref{eq:avg_maf}, respectively. (\textbf{A}) Simulated and predicted mean $\pm$ SD are shown as a function of increasing Ti/Tv ratio $\eta$. Distance decreases as Tv becomes more frequent than Ti. Theoretical standard deviation is slightly larger than simulated, but the means are approximately the same. (\textbf{B}) Simulated and predicted mean $\pm$ SD are shown as a function of increasing average MAF $\bar{f}_a$. Distance increases as the number of minor alleles increases at each locus $a$. Theoretical and simulated moments are approximately the same.}\label{fig:TiTv_meanSD}
%\end{figure}

\begin{figure}[H]
	\centering
	%\includegraphics[width=0.98\textwidth]{updated_distributions_table-gwas(6-16-2020).pdf}
	\caption{{\bf Asymptotic estimates of means and variances of genotype mismatch (GM) (Eq.~\ref{eq:D_GM}), allele mismatch (AM) (Eq.~\ref{eq:D_AM}), and transition-transversion (TiTv) (Eq.~\ref{eq:D_TiTv}) distance metrics in GWAS data ($p \gg 1$).} GWAS data $X_{ia} \sim \mathcal{B}(2,f_a)$, where $f_a$ for all $a \in \mathcal{A}$ are the probabilities of a minor allele occurring at locus $a$. For the TiTv distance metric, we have the additional encoding that uses $\gamma_0=\text{P}(\text{PuPu})$, $\gamma_1=\text{P}(\text{PuPy})$, and $\gamma_2=\text{P}(\text{PyPy})$.}
	\label{tab:dist_distr_gwas}
\end{figure}

\subsection*{Distribution of one-dimensional projection of GWAS distance onto a SNP}\label{sec:GWAS_diff}

We previously derived the exact distribution of the one-dimensional projected distance onto an attribute in continuous data (Section~\ref{sec:continuous_diff}), which is used as the predictor in NPDR to calculate relative attribute importance in the form of standardized beta coefficients. GWAS data and the metrics we have considered are discrete. Therefore, we derive the density function for each diff metric (Eqs.~\ref{eq:diff_GM}-\ref{eq:diff_TiTv}), which also serves as the probability distribution for each metric, respectively.

The support of the GM metric (Eq.~\ref{eq:diff_GM}) is simply $\{0,1\}$, so we derive the probability, $\text{P}\left[\text{d}^\text{GM}_{ij}(a) = k\right]$, of this diff taking on each of these two possible values. First, the probability that the GM diff is equal to zero is given by
%
\begin{equation}\label{eq:probGM0}
\begin{aligned}
f_\text{GM}(0;f_a) = \text{P}\left[\text{d}^\text{GM}_{ij}(a) = 0\right] &= \text{P}\left(X_{ia}=0,X_{ja}=0\right) + \text{P}\left(X_{ia}=1,X_{ja}=1\right) \\
& \hspace{3.64cm} + \text{P}\left(X_{ia}=2,X_{ja}=2\right) \\
&= \left(1 - f_a\right)^4 + 4 f^2_a \left(1 - f_a\right)^2 + f^4_a,
\end{aligned}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$.

Similarly, the probability that the GM diff is equal to 1 is derived as follows
%
\begin{equation}\label{eq:probGM1}
\begin{aligned}
f_\text{GM}(1;f_a) = \text{P}\left[\text{d}^\text{GM}_{ij}(a) = 1\right] &= 2\text{P}\left(X_{ia}=0,X_{ja}=1\right) + 2\text{P}\left(X_{ia}=1,X_{ia}=2\right) \\
& \hspace{3.81cm} + 2\text{P}\left(X_{ia}=0,X_{ja}=2\right) \\
&= 4\left(1 - f_a\right)^3 f_a + 4 f^3_a \left(1 - f_a\right) + 2 f^2_a \left(1 - f_a\right)^2,
\end{aligned}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$.

This leads us to the probability distribution of the GM diff metric, which is the distribution of the one-dimensional GM distance projected onto a single SNP. This distribution is given by
%
\begin{equation}\label{eq:GMdiffPDF}
f_\text{GM}(d;f_a) = \begin{cases}
\left(1 - f_a\right)^4 + 4 f^2_a \left(1 - f_a\right)^2 + f^4_a & d=0, \\
4\left(1 - f_a\right)^3 f_a + 4 f^3_a \left(1 - f_a\right) + 2 f^2_a \left(1 - f_a\right)^2 & d=1,
\end{cases}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$. 

The mean and variance of this GM diff distribution can easily be derived using this newly determined density function (Eq.~\ref{eq:GMdiffPDF}). The average GM diff is given by the following
%
\begin{equation}\label{eq:GMdiffMean}
\text{E}\left[\text{d}^\text{GM}_{ij}(a)\right] = 2 F^\text{GM}(a),
\end{equation}
%
where $F^\text{GM} = 2 \left(1 - f_a\right)^3 f_a + 2 f^3_a \left(1 - f_a\right) + f^2_a \left(1 - f_a\right)^2$ and $f_a$ is the probability of a minor allele occurring at locus $a$.

The variance of the GM diff metric is given by
%
\begin{equation}\label{eq:GMdiffVar}
\text{Var}\left[\text{d}^\text{GM}_{ij}(a)\right] = 2 F^\text{GM}(a)\left[1 - 2 F^\text{GM}(a)\right],
\end{equation}
%
where $F^\text{GM} = 2 \left(1 - f_a\right)^3 f_a + 2 f^3_a \left(1 - f_a\right) + f^2_a \left(1 - f_a\right)^2$ and $f_a$ is the probability of a minor allele occurring at locus $a$.

The support of the AM metric (Eq.~\ref{eq:diff_AM}) is $\{0,1/2,1\}$. Beginning with the probability of the AM diff being equal to 0, we have the following probability
%
\begin{equation}\label{eq:probAM0}
\begin{aligned}
f_\text{AM}(0;f_a) = \text{P}\left[\text{d}^\text{AM}_{ij}(a) = 0\right] &= 
\text{P}\left(X_{ia}=0,X_{ja}=0\right) + \text{P}\left(X_{ia}=1,X_{ja}=1\right) \\
& \hspace{3.64cm} + \text{P}\left(X_{ia}=2,X_{ja}=2\right) \\
&= \left(1 - f_a\right)^4 + 4 f^2_a \left(1 - f_a\right)^2 + f^4_a,
\end{aligned}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$.

The probability of the AM diff metric being equal to 1/2 is computed similarly as follows
%
\begin{equation}\label{eq:probAM0.5}
\begin{aligned}
f_\text{AM}\left(1/2;f_a\right) = \text{P}\left[\text{d}^\text{AM}_{ij}(a) = 1/2\right] &= 2\text{P}\left(X_{ia}=0,X_{ja}=1\right) + 2\text{P}\left(X_{ia}=1,X_{ia}=2\right) \\
&= 4 \left(1 - f_a\right)^3 f_a + 4 f^3_a \left(1 - f_a\right),
\end{aligned}
\end{equation}
%
where $f_a$ the probability of a minor allele occurring at locus $a$.

Finally, the probability of the AM diff metric being equal to 1 is given by the following
%
\begin{equation}\label{eq:probAM1}
\begin{aligned}
f_\text{AM}(1;f_a) = \text{P}\left[\text{d}^\text{AM}_{ij}(a) = 1\right] &= 2 \text{P}\left(X_{ia}=0,X_{ja}=2\right) \\
&= 2 f^2_a \left(1 - f_a\right)^2,
\end{aligned}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$.

As in the case of the GM diff metric, we now have the probability distribution of the AM diff metric. This also serves as the distribution of the one-dimensional AM distance projected onto a single SNP, and is given by the following
%
\begin{equation}\label{eq:AMdiffPDF}
f_\text{AM}(d;f_a) = \begin{cases}
\left(1 - f_a\right)^4 + 4 f^2_a \left(1 - f_a\right)^2 + f^4_a & d=0, \\
4 \left(1 - f_a\right)^3 f_a + 4 f^3_a \left(1 - f_a\right) & d=1/2, \\
2 f^2_a \left(1 - f_a\right)^2 & d=1,
\end{cases}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$.

The mean and variance of this AM diff distribution is derived using the corresponding density function (Eq.~\ref{eq:AMdiffPDF}). The average AM diff is given by
%
\begin{equation}\label{eq:AMdiffMean}
\text{E}\left[\text{d}^\text{AM}_{ij}(a)\right] = 2 F^\text{AM}(a),
\end{equation}
%
where $F^\text{AM}(a) = \left(1 - f_a\right)^3 f_a + f^3_a \left(1 - f_a\right) + f^2_a \left(1 - f_a\right)^2$ and $f_a$ is the probability of a minor allele occurring at locus $a$.

The variance of the AM diff metric is given by
%
\begin{equation}\label{eq:AMdiffVar}
\text{Var}\left[\text{d}^\text{AM}_{ij}(a)\right] = G^\text{AM}(a) - 4 \left[F^\text{AM}(a)\right]^2,
\end{equation}
%
where $G^\text{AM}(a) = \left(1 - f_a\right)^3 f_a + f^3_a \left(1 - f_a\right) + 2\left(1 - f_a\right)^2 f^2_a$, $F^\text{AM}(a) = \left(1 - f_a\right)^3 f_a + f^3_a \left(1 - f_a\right) + f^2_a \left(1 - f_a\right)$, $f_a$ is the probability of a minor allele occurring at locus $a$.

For the TiTv diff metric (Eq.~\ref{eq:diff_TiTv}), the support is $\{0,1/4,1/2,3/4,1\}$. We have already derived the probability that the TiTv diff assumes each of the values of its support (Eqs.~\ref{eq:prob_TiTv_0}-\ref{eq:prob_TiTv_1}). Therefore, we have the following distribution of the TiTv diff metric
%
\begin{equation}\label{eq:TiTvdiffPDF}
f_\text{TiTv}(d;f_a,\gamma_0,\gamma_1,\gamma_2,\eta) = \begin{cases}
\left(1 - f_a\right)^4 + 4 f^2_a \left(1 - f_a\right)^2 + f^4_a & d=0, \\
4(\gamma_0 + \gamma_2)\left[\left(1 - f_a\right)^3 f_a + f^3_a \left(1 - f_a\right)\right] & d=1/4, \\
4 \gamma_1 \left[\left(1 - f_a\right)^3 f_a + f^3_a \left(1 - f_a\right)\right] & d=1/2, \\
2(\gamma_0 + \gamma_2)\left(1 - f_a\right)^2 f^2_a & d=3/4, \\
2\gamma_1\left(1 - f_a\right)^2 f^2_a & d=1,
\end{cases}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$, $\gamma_0$ is the probability of PuPu at locus $a$, $\gamma_1$ is the probability of PuPy at locus $a$, $\gamma_2$ is the probability of PyPy at locus $a$, and $\eta$ is the Ti/Tv ratio (Eq.~\ref{eq:TiTv_constraints2}).

The mean and variance of this TiTv diff distribution is derived using the corresponding density function (Eq.~\ref{eq:TiTvdiffPDF}). The average TiTv diff is given by
%
\begin{equation}\label{eq:TiTvdiffMean}
\begin{aligned}
\text{E}\left[\text{d}^\text{TiTv}_{ij}(a)\right] &= (\gamma_0 + \gamma_2 + 2\gamma_1)F^\text{TiTv}(a) + \left[\frac{3}{2}(\gamma_0 + \gamma_2) + 2\gamma_1\right] G^\text{TiTv}(a),
\end{aligned}
\end{equation}
%
where $F^\text{TiTv}(a) = \left(1 - f_a\right)^3 f_a + f^3_a \left(1 - f_a\right)$, $G^\text{TiTv}(a) = f^2_a \left(1 - f_a\right)^2$, $f_a$ is the probability of a minor allele occurring at locus $a$, $\gamma_0$ is the probability of PuPu at locus $a$, $\gamma_1$ is the probability of PuPy at locus $a$, and $\gamma_2$ is the probability of PyPy at locus $a$.

The variance of the TiTv diff metric is given by
%
\begin{equation}\label{eq:TiTvdiffVar}
\begin{aligned}
\text{Var}\left[\text{d}^\text{TiTv}_{ij}(a)\right] &= \left[\frac{1}{4}(\gamma_0 + \gamma_2) + \gamma_1\right] F^\text{TiTv}(a) + \left[\frac{9}{8}(\gamma_0 + \gamma_2) + 2\gamma_1\right] G^\text{TiTv}(a) \\
&- \left((\gamma_0 + \gamma_2 + 2\gamma_1)F^\text{TiTv}(a) + \left[\frac{3}{2}(\gamma_0 + \gamma_2) + 2\gamma_1\right] G^\text{TiTv}(a)\right)^2,
\end{aligned}
\end{equation}
%
where $F^\text{TiTv}(a) = \left(1 - f_a\right)^3 f_a + f^3_a \left(1 - f_a\right)$, $G^\text{TiTv}(a) = f^2_a \left(1 - f_a\right)^2$, $f_a$ is the probability of a minor allele occurring at locus $a$, $\gamma_0$ is the probability of PuPu at locus $a$, $\gamma_1$ is the probability of PuPy at locus $a$, and $\gamma_2$ is the probability of PyPy at locus $a$.

These novel distribution results for the projection of pairwise GWAS distances onto a single genetic variant, as well as results for the full space of $p$ variants, can inform NPDR and other nearest-neighbor distance-based feature selection algorithms. Next we introduce our new diff metric and distribution results for time series derived correlation-based data, with a particular application to resting-state fMRI.

%\subsection{Resting-State fMRI Distance Distribution}
\section*{Time series correlation-based distance distribution}\label{sec:rs-fMRI_distances}

For time series correlation-based data, we consider the case where there are $m$ correlation matrices $A^{(p \times p)}$ (one matrix for each subject). In particular, we have in mind the application of resting-state fMRI (rs-fMRI) data. The derivations that follow, however, are relevant to all correlation-based data with the assumptions we note. The attributes in rs-fMRI are commonly Regions of Interest (ROIs), which are collections of spatially proximal voxels \cite{lee2013}. Correlation in their time-series activity arise between different ROIs at the voxel level or for a given brain atlas \cite{dickie2017}. Because the attributes of interest ($a$) are the ROIs themselves, we propose the following attribute projection (diff)
%
\begin{equation}\label{eq:diff_rs-fMRI}
\text{d}^\text{ROI}_{ij}(a) = \sum_{k \neq a}\bigl|A^{(i)}_{ka} - A^{(j)}_{ka}\bigr|,
\end{equation}
%
where $A^{(i)}_{ak}$ and $A^{(j)}_{ak}$ are the correlations between ROI $a$ and ROI $k$ for instances $i,j \in \mathcal{I}$, respectively. With this rs-fMRI diff, we define the pairwise distance between two instances $i,j \in \mathcal{I}$ as follows
%
\begin{equation}\label{eq:D_rs-fMRI}
D^\text{fMRI}_{ij} = \sum_{a \in \mathcal{A}} \text{d}^\text{ROI}_{ij}(a),
\end{equation}
%
which is based on Manhattan ($q=1$). This metric may be expanded to general $q$, but we only consider $q=1$.

In order for comparisons between different correlations to be possible, we first perform a Fisher r-to-z transform on the correlations. This transformation makes the data approximately normally distributed with stabilized variance across different samples. After this transformation, we then load all of the transformed correlations into a $p(p-1) \times m$ matrix $X$ (Fig.~\ref{fig:rs-fMRI_matrix}). Each column of $X$ represents a single instance (or subject) in rs-fMRI data. Contrary to a typical $p \times m$ data set, each row does not represent a single attribute. Rather, each attribute (or ROI) is represented by $p - 1$ consecutive rows. The first $p - 1$ rows represent $\text{ROI}_1$, the next $p - 1$ rows represent $\text{ROI}_2$, and so on until the last $p - 1$ rows that represent $\text{ROI}_p$. For a given column of $X$, we exclude pairwise correlations between an ROI and itself. Therefore, the matrix does not contain $\hat{A}^{(i)}_{aa}$ for any $i \in \mathcal{I}$ or $a \in \mathcal{A}$. Furthermore, symmetry of correlation matrices means that each column contains exactly two of each element of the upper triangle of an instance's transformed correlation matrix. For example, $\hat{A}^{(i)}_{ka} = \hat{A}^{(i)}_{ak}$ for $k \neq a$ and both will be contained in a given column of $X$ for each $a \in \mathcal{A}$. Based on our rs-fMRI diff (Eq.~\ref{eq:diff_rs-fMRI}), the organization of $X$ makes computation of each value of the diff very simple. In order to compute each value of the rs-fMRI diff, we just need to know the starting and ending row indices for a given ROI. Starting indices are given by
%
\[
\text{start}_k = (k - 1)(p - 1) + 1, \quad \text{ for } k = 1,2,\dots,p
\]
%
and ending indices are given by
%
\[
\text{end}_k = k(p - 1), \quad \text{ for } k = 1,2,\dots,p.
\]

These indices allow us to extract just the rows necessary to compute the rs-fMRI diff for a fixed ROI. 

\bigskip

\begin{minipage}[c]{0.7\textwidth}\hspace{-0.6cm}
	%\includegraphics[width=0.95\textwidth]{fig5_rs_fmri_all_instance_matrix.pdf}
\end{minipage}\hspace{-0.8cm}
\begin{minipage}[c]{0.3\textwidth}
	\captionsetup{type=figure}\captionof{figure}{{\bf Organization based on brain regions of interest (ROIs) of resting-state fMRI correlation dataset consisting of transformed correlation matrices for $m$ subjects.} Each column corresponds to an instance (or subject) $I_j$ and each subset of rows corresponds to the correlations for an ROI attribute ($p$ sets). The notation $\hat{A}^{(j)}_{ak}$ represents the r-to-z transformed correlation between attributes (ROIs) $a$ and $k \neq a$ for instance $j$.}\label{fig:rs-fMRI_matrix}
\end{minipage}

\bigskip

%
%\begin{equation}\label{eq:rs-fMRI_matrix}
%X = \left[
%\begin{array}{c c c c c}
%\hat{A}^{(1)}_{12} & \hat{A}^{(2)}_{12} & \hat{A}^{(3)}_{12} & \dots & \hat{A}^{(m)}_{12} \\
%\hat{A}^{(1)}_{13} & \hat{A}^{(2)}_{13} & \hat{A}^{(3)}_{13} & \dots & \hat{A}^{(m)}_{13} \\
%\hat{A}^{(1)}_{14} & \hat{A}^{(2)}_{14} & \hat{A}^{(3)}_{14} & \dots & \hat{A}^{(m)}_{14} \\
%\vdots & \vdots & \vdots & \ddots & \vdots \\
%\hat{A}^{(1)}_{1p} & \hat{A}^{(2)}_{1p} & \hat{A}^{(3)}_{1p} & \dots & \hat{A}^{(m)}_{1p} \\
%\hat{A}^{(1)}_{21} & \hat{A}^{(2)}_{21} & \hat{A}^{(3)}_{21} & \dots & \hat{A}^{(m)}_{21} \\
%\hat{A}^{(1)}_{23} & \hat{A}^{(2)}_{23} & \hat{A}^{(3)}_{23} & \dots & \hat{A}^{(m)}_{23} \\
%\hat{A}^{(1)}_{24} & \hat{A}^{(2)}_{24} & \hat{A}^{(3)}_{24} & \dots & \hat{A}^{(m)}_{24} \\
%\vdots & \vdots & \vdots & \ddots & \vdots \\
%\hat{A}^{(1)}_{2p} & \hat{A}^{(2)}_{2p} & \hat{A}^{(3)}_{2p} & \dots & \hat{A}^{(m)}_{2p} \\
%\vdots & \vdots & \vdots & \ddots & \vdots \\
%\hat{A}^{(1)}_{p1} & \hat{A}^{(2)}_{p1} & \hat{A}^{(3)}_{p1} & \dots & \hat{A}^{(m)}_{p1} \\
%\hat{A}^{(1)}_{p2} & \hat{A}^{(2)}_{p2} & \hat{A}^{(3)}_{p2} & \dots & \hat{A}^{(m)}_{p2} \\
%\hat{A}^{(1)}_{p3} & \hat{A}^{(2)}_{p3} & \hat{A}^{(3)}_{p3} & \dots & \hat{A}^{(m)}_{p3} \\
%\vdots & \vdots & \vdots & \ddots & \vdots \\
%\hat{A}^{(1)}_{p,(p-1)} & \hat{A}^{(2)}_{p,(p-1)} & \hat{A}^{(3)}_{p,(p-1)} & \dots & \hat{A}^{(m)}_{p,(p-1)}
%\end{array}
%\right],
%\end{equation}

We further transform the data matrix $X$ by standardizing so that each of the $m$ columns has zero mean and unit variance. Therefore, the data in matrix $X$ are approximately standard normal. Recall that the mean (Eq.~\ref{eq:normalManMean}) and variance (Eq.~\ref{eq:normalManVar}) of the Manhattan ($L_1$) distance distribution for standard normal data are $\frac{2p}{\sqrt{\pi}}$ and $\frac{2(\pi - 2)p}{\pi}$, respectively. This allows us to easily derive the expected pairwise distance between instances $i,j \in \mathcal{I}$ in rs-fMRI data as follows
%
\begin{equation}\label{eq:mu_DDistr_rs-fMRI}
\begin{aligned}
\text{E}(D^\text{fMRI}_{ij}) &= \text{E}\left(\sum_{a \in \mathcal{A}} \text{d}^\text{ROI}_{ij}(a)\right) \\
&= \text{E}\left(\sum_{a \in \mathcal{A}} \sum_{k \neq a} \left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&= \sum_{a \in \mathcal{A}} \sum_{k \neq a} \text{E}\left(\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&= \sum_{a \in \mathcal{A}} \sum_{k \neq a} \frac{2}{\sqrt{\pi}} \\
&= \frac{2p(p-1)}{\sqrt{\pi}}.
\end{aligned}
\end{equation}

The expected pairwise rs-fMRI distance (Eq.~\ref{eq:mu_DDistr_rs-fMRI}) grows on the order of $p(p-1)$, which is the total number of transformed pairwise correlations in each column of $X$ (Fig.~\ref{fig:rs-fMRI_matrix}). This is similar to the case of a typical $m \times p$ data matrix in which the data is standard normal and Manhattan distances are computed between instances. 

We first derive the variance of the rs-fMRI distance by making an independence assumption with respect to the magnitude differences $|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}|$ for all $k \neq a \in \mathcal{A}$. We observe empirically that this assumption gives a reasonable estimate of the actual variance of rs-fMRI distances in simulated data, but there is a consistent discrepancy between predicted and simulated variances. We begin our derivation of the variance of rs-fMRI distances by assuming that cross-covariances between the diffs of different pairs of ROIs are negligible. This allows us to determine the relationship between the predicted variance under the independence assumption and the simulated variance. We proceed by applying the variance operator linearly as follows
%
\begin{equation}\label{eq:var_DDistr_rs-fMRI_standard}
\begin{aligned}
\text{Var}(D^\text{fMRI}_{ij}) &= \text{Var}\left(\sum_{a \in \mathcal{A}} \text{d}^\text{ROI}_{ij}(a)\right) \\
&= \text{Var}\left(\sum_{a \in \mathcal{A}} \sum_{k \neq a} \left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&= \sum_{a \in \mathcal{A}} \sum_{k \neq a} \text{Var}\left(\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&= \sum_{a \in \mathcal{A}} \sum_{k \neq a} \frac{2(\pi - 2)}{\pi} \\
&= \frac{2(\pi - 2)(p-1)p}{\pi}.
\end{aligned}
\end{equation}

Similar to the case of an $m \times p$ data matrix containing standard normal data, we have an rs-fMRI distance variance that grows on the order of $p(p - 1)$, which is the total number of pairwise associations in a column of data matrix $X$ (Fig.~\ref{fig:rs-fMRI_matrix}). Therefore, the expected rs-fMRI distance (Eq.~\ref{eq:mu_DDistr_rs-fMRI}) and the variance of the rs-fMRI distance (Eq.~\ref{eq:var_DDistr_rs-fMRI_standard}) increase on the same order.

The independence assumption used to derive the variance of our rs-fMRI distance metric (Eq.~\ref{eq:var_DDistr_rs-fMRI_standard}) is not satisfied because a single value of the diff (Eq.~\ref{eq:diff_rs-fMRI}) includes the same fixed ROI, $a$, for each term in the sum for all $k \neq a$. Therefore, the linear application of the variance operator we have previously employed does not account for the additional cross-covariance that exists. However, we have seen empirically that the theoretical variance of the distance we computed for the rs-fMRI distance metric (Eq.~\ref{eq:var_DDistr_rs-fMRI_standard}) still reasonably approximates the sample variance, there is a slight discrepancy between our theoretical rs-fMRI distance metric variance (Eq.~\ref{eq:var_DDistr_rs-fMRI_standard}) and the sample variance. More precisely, the formula we have given for the variance (Eq.~\ref{eq:var_DDistr_rs-fMRI_standard}) consistently underestimates the sample variance of the rs-fMRI distance. To adjust for this discrepancy, we determine a corrected formula by assuming that there is dependence between the terms of the rs-fMRI diff and estimate the cross-covariance between rs-fMRI diffs of different pairs of ROIs. 

We begin the derivation of our corrected formula by writing the variance as a two-part sum, where the first term in the sum involves the variance of the magnitude difference $|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}|$ and then second term involves the cross-covariance of the rs-fMRI diff for distinct pairwise ROI-ROI associations. This formulation is implied in our previous derivation of the variance, but our independence assumption allowed us to assume that all terms in the second part of the two-part sum were zero. Our formulation of the variance is given by the following
%
\begin{equation}\label{eq:var_DDistr_rs-fMRI}
\begin{aligned}
\text{Var}(D^\text{fMRI}_{ij}) &= \text{Var}\left(\sum_{a \in \mathcal{A}} \sum_{k \neq a} \left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&= \sum_{a = 1}^{p-1} \text{Var}\left(\sum_{k=a+1}^{p} 2\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&+ 2\sum_{a = 1}^{p-1} \sum_{r=a+1}^{p-1} \text{Cov}\left(\sum_{k=a+1}^{p} 2\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|, \sum_{s=r+1}^{p} 2\left|\hat{A}^{(i)}_{rs} - \hat{A}^{(j)}_{rs}\right|\right) \\
&= \sum_{a=1}^{p-1} \sum_{k=a+1}^{p} \text{Var}\left(2\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&+ 2\sum_{a = 1}^{p-1} \sum_{r=a+1}^{p-1} \text{Cov}\left(\sum_{k=a+1}^{p} 2\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|, \sum_{s=r+1}^{p} 2\left|\hat{A}^{(i)}_{rs} - \hat{A}^{(j)}_{rs}\right|\right) \\
&= \sum_{a = 1}^{p-1} \sum_{k=a+1}^{p-1}\frac{4(\pi-2)}{\pi} \\
&+ 2\sum_{a = 1}^{p-1} \sum_{r=a+1}^{p-1} \text{Cov}\left(\sum_{k=a+1}^{p} 2\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|, \sum_{s=r+1}^{p} 2\left|\hat{A}^{(i)}_{rs} - \hat{A}^{(j)}_{rs}\right|\right) \\
&= \frac{2p(\pi-2)(p-1)}{\pi} \\
&+ 2\sum_{a = 1}^{p-1} \sum_{r=a+1}^{p-1} \text{Cov}\left(\sum_{k=a+1}^{p} 2\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|, \sum_{s=r+1}^{p} 2\left|\hat{A}^{(i)}_{rs} - \hat{A}^{(j)}_{rs}\right|\right).
\end{aligned}
\end{equation}

In order to have a formula in terms of the number of ROIs $p$ only, we estimate the double sum on the right-hand side of the equation of rs-fMRI distance variance (Eq.~\ref{eq:var_DDistr_rs-fMRI}). Through simulation, it can be seen that the difference between sample variance $S^2_{D_{ij}}$ and $\frac{2p(\pi-2)(p-1)}{\pi}$ has a quadratic relationship with $p$. More explicitly, we have the following relationship
%
\begin{equation}\label{eq:estimate_cov}
S^2_{D^\text{fMRI}_{ij}} - \frac{2p(\pi-2)(p-1)}{\pi} = \beta_1 p^2 + \beta_0 p.
\end{equation}
%
where $\beta_0$ and $\beta_1$ are the coefficients we must estimate in order to approximate the cross-covariance term in the right-hand side of the rs-fMRI distance variance equation (Eq.~\ref{eq:var_DDistr_rs-fMRI}).

The coefficient estimates found through least squares fitting are $\beta_1 = - \beta_0 \approx 0.08$. These estimates allow us to arrive at a functional form for the double sum in the right-hand side of the rs-fMRI distance variance equation (Eq.~\ref{eq:var_DDistr_rs-fMRI}) that is proportional to $\frac{2p(\pi-2)(p-1)}{\pi}$. That is, we have the following formula for approximating the double sum
%
\begin{equation}\label{eq:estimate_cov_form}
2\sum_{a = 1}^{p-1} \sum_{r=a+1}^{p-1} \text{Cov}\left(\sum_{k=a+1}^{p} 2\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|, \sum_{s=r+1}^{p} 2\left|\hat{A}^{(i)}_{rs} - \hat{A}^{(j)}_{rs}\right|\right) \approx \frac{p(\pi - 2)(p - 1)}{4\pi}.
\end{equation}

Therefore, the variance of the rs-fMRI distances is approximated well by the following
%
\begin{equation}\label{eq:var_DDistr_rs-fMRI2}
\text{Var}(D^\text{fMRI}_{ij}) \approx \frac{9p(\pi - 2)(p-1)}{4\pi}.
\end{equation}

With the mean (Eq.~\ref{eq:mu_DDistr_rs-fMRI}) and variance (Eq.~\ref{eq:var_DDistr_rs-fMRI2}) estimates, we have the following asymptotic distribution for rs-fMRI distances
%
\begin{equation}\label{eq:DDistr_rs-fMRI}
D^\text{fMRI}_{ij} \overset{.}{\sim} \mathcal{N}\left(\frac{2p(p-1)}{\sqrt{\pi}}, \frac{9p(\pi - 2)(p-1)}{4\pi}\right).
\end{equation}

\subsection*{Max-min normalized time series correlation-based distance distribution}

Previously (Section~\ref{sec:extremes}) we determined the asymptotic distribution of the sample maximum of size $m$ from a standard normal distribution. We can naturally extend these results to our transformed rs-fMRI data because $X$ (Fig.~\ref{fig:rs-fMRI_matrix}) is approximately standard normal. We proceed with the definition of the max-min normalized rs-fMRI pairwise distance.

Consider the max-min normalized rs-fMRI distance given by the following equation
%
\begin{equation}\label{eq:max-min_diff_rs-fMRI}
D^\text{fMRI*}_{ij} = \sum_{a \in \mathcal{A}} \sum_{k \neq a} \frac{\left|A^{(i)}_{ak} - A^{(j)}_{ak}\right|}{\max(a) - \min(a)}.
\end{equation}

Assuming that the data $X$ has been r-to-z transformed and standardized, we can easily compute the expected attribute range and variance of the attribute range. The expected maximum of a given attribute in data matrix $X$ is estimated by the following
%
\begin{equation}\label{eq:mean_max_rs-fMRI}
\text{E}\left(X^\text{max}_a - X^\text{min}_a\right) = 2\mu^{(1)}_\text{max}(m,p) = 2 \left[\frac{\text{log}(\text{log}(2))}{\Phi^{-1}\left(\frac{1}{m(p-1)}\right)} - \Phi^{-1}\left(\frac{1}{m(p-1)}\right)\right].
\end{equation}

The variance can be esimated with the following
%
\begin{equation}\label{eq:var_max_rs-fMRI}
\text{Var}\left(X^\text{max}_a - X^\text{min}_a\right) = \frac{\pi^2}{6\text{log}[m(p-1)]}.
\end{equation}

Let $\mu_{D^\text{fMRI}_{ij}}$ and $\sigma^2_{D^\text{fMRI}_{ij}}$ denote the mean and variance of the rs-fMRI distance distribution given by Eqs.~\ref{eq:mu_DDistr_rs-fMRI} and \ref{eq:var_DDistr_rs-fMRI2}. Using the formulas for the mean and variance of the max-min normalized distance distribution given in Eq. \ref{eq:max-min_DDistr_normal}, we have the following asymptotic distribution for the max-min normalized rs-fMRI distances
%
\begin{equation}\label{eq:max-min_DDistr_normal_rs-fMRI}
D^\text{fMRI*}_{ij} \overset{.}{\sim} \mathcal{N}\left(\frac{\mu_{D^\text{fMRI}_{ij}}}{2\mu^{(1)}_\text{max}(m,p)}, \frac{6\sigma^2_{D^\text{fMRI}_{ij}}\text{log}[m(p-1)]}{\pi^2 + 24\left[\mu^{(1)}_\text{max}(m,p)\right]^2\text{log}[m(p-1)]}\right).
\end{equation}

\subsection*{One-dimensional projection of rs-fMRI distance onto a single ROI}\label{sec:rs-fMRI_diff}

Just as in previous sections (Sections.~\ref{sec:continuous_diff} and \ref{sec:GWAS_diff}), we now derive the distribution of our rs-fMRI diff metric (Eq.~\ref{eq:diff_rs-fMRI}). Unlike what we have seen in previous sections, we do not derive the exact distribution for this diff metric. We have determined empirically that the rs-fMRI diff is approximately normal. Although the rs-fMRI diff is a sum of $p-1$ magnitude differences, the Classical Central Limit Theorem does not apply because of the dependencies that exist between the terms of the sum. Examination of histograms and quantile-quantile plots of simulated values of the rs-fMRI diff easily indicate that the normality assumption is safe. Therefore, we derive the mean and variance of the approximately normal distribution of the rs-fMRI diff. As we have seen previously, this normality assumption is reasonable even for small values of $p$. 

The mean of the rs-fMRI diff is derived by fixing a single ROI $a$ and considering all pairwise associations with other ROIs $k \neq \mathcal{A}$. This is done as follows
%
\begin{equation}\label{eq:rs-fMRI_diffMean}
\begin{aligned}
\text{E}\left[\text{d}^\text{ROI}_{ij}(a)\right] &= \text{E}\left(\sum_{k \neq a} \left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&= \sum_{k \neq a} \text{E}\left(\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&= \sum_{k \neq a} \frac{2}{\sqrt{\pi}} \\
&= \frac{2(p-1)}{\sqrt{\pi}},
\end{aligned}
\end{equation}
% 
where $a$ is a single fixed ROI.

Considering the variance of the rs-fMRI diff metric, we have two estimates. The first estimate uses the variance operator in a linear fashion, while the second will simply be a direct implication of the corrected formula of the variance of rs-fMRI pairwise distances (Eq.~\ref{eq:var_DDistr_rs-fMRI2}). Our first estimate is derived as follows
%
\begin{equation}\label{eq:rs-fMRI_diffVar1}
\begin{aligned}
\text{Var}\left[\text{d}^\text{ROI}_{ij}(a)\right] &= \text{Var}\left(\sum_{k \neq a} \left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&= \sum_{k \neq a} \text{Var}\left(\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&= \sum_{k \neq a} \frac{2(\pi - 2)}{\pi} \\
&= \frac{2(\pi - 2)(p-1)}{\pi},
\end{aligned}
\end{equation}
%
where $a$ is a single fixed ROI.

Using the corrected rs-fMRI distance variance formula (Eq.~\ref{eq:var_DDistr_rs-fMRI2}), our second estimate of the rs-fMRI diff variance is given directly by the following
%
\begin{equation}\label{eq:rs-fMRI_diffVar2}
\text{Var}\left[\text{d}^\text{ROI}_{ij}(a)\right] = \frac{9(\pi - 2)(p - 1)}{4\pi},
\end{equation}
%
where $a$ is a single fixed ROI.

Empirically, the first estimate (Eq.~\ref{eq:rs-fMRI_diffVar1}) of the variance of our rs-fMRI diff is closer to the sample variance than the second estimate (Eq.~\ref{eq:rs-fMRI_diffVar2}). This is due to fact that we are considering only a fixed ROI $a \in \mathcal{A}$, so the cross-covariance between the magnitude differences $|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}|$ for different pairs of ROIs ($a$ and $k \neq a$) is negligible here. When considering all ROIs $a \in \mathcal{A}$, these cross-covariances are no longer negligible. Using the first variance estimate (Eq.~\ref{eq:rs-fMRI_diffVar1}) and the estimate of the mean (Eq.~\ref{eq:rs-fMRI_diffMean}), we have the following asymptotic distribution of the rs-fMRI diff
%
\begin{equation}
\text{d}^\text{ROI}_{ij}(a) \overset{.}{\sim} \mathcal{N}\left(\frac{2(p-1)}{\sqrt{\pi}},\frac{2(\pi - 2)(p-1)}{\pi}\right),
\end{equation}
%
where $a$ is a single fixed ROI.

\subsection*{Normalized Manhattan \texorpdfstring{($q=1$)}{} for rs-fMRI}

Substituting the non-normalized mean (Eq.~\ref{eq:mu_DDistr_rs-fMRI}) into the equation for the mean of the max-min normalized rs-fMRI metric (Eq.~\ref{eq:max-min_DDistr_normal_rs-fMRI}), we have the following
%
\begin{equation}\label{eq:mu_max-min_rs-fMRI}
\begin{aligned}
\text{E}\left(D^\text{fMRI*}_{ij}\right) &= \frac{\mu_{D^\text{fMRI}_{ij}}}{2\mu^{(1)}_\text{max}(m,p)} \\
&= \frac{p(p-1)}{\sqrt{\pi}\mu^{(1)}_\text{max}(m,p)},
\end{aligned}
\end{equation}
%
where $\mu^{(1)}_\text{max}(m,p)$ (Eq.~\ref{eq:mean_max_rs-fMRI}) is the expected maximum of a single ROI in a data set with $m$ instances and $p$ ROIs.

Similarly, the variance of $D^\text{fMRI*}_{ij}$ is given by
%
\begin{equation}\label{eq:var_max-min_rs-fMRI}
\begin{aligned}
\text{Var}\left(D^\text{fMRI*}_{ij}\right) &= \frac{6\sigma^2_{D^\text{fMRI}_{ij}}\text{log}[m(p-1)]}{\pi^2 + 24\left[\mu^{(1)}_\text{max}(m,p)\right]^2\text{log}[m(p-1)]} \\
&= \frac{27(\pi-2)\text{log}[m(p-1)](p-1)p}{2\pi\left(\pi^2 + 24\left[\mu^{(1)}_\text{max}(m,p)\right]^2\text{log}[m(p-1)]\right)},
\end{aligned}
\end{equation}
%
where $\mu^{(1)}_\text{max}(m,p)$ (Eq.~\ref{eq:mean_max_rs-fMRI}) is the expected maximum of a single ROI in a data set with $m$ instances and $p$ ROIs.

We summarize the moment estimates for the rs-fMRI metrics for correlation-based data derived from time series (Fig.~\ref{tab:dist_distr_rs-fMRI}). We organize this summary by standard and attribute range-normalized rs-fMRI distance metric, statistic (mean or variance), and asymptotic formula.


\begin{figure}[H]
	\centering
	%\includegraphics[width=0.98\textwidth]{updated_distributions_table-rs-fMRI(6-16-2020).pdf}
	\caption{{\bf Aymptotic means and variances for the new standard (Eq.~\ref{eq:D_rs-fMRI}) and max-min normalized (Eq.~\ref{eq:max-min_diff_rs-fMRI}) rs-fMRI distance metrics.}}
	\label{tab:dist_distr_rs-fMRI}
\end{figure}

\section*{Comparison of theoretical and sample moments}

We compare our analytical asymptotic estimates of sample moments for distributions of pairwise distances in high attribute dimension by generating random data for various dimensions $m$ and $p$ (Fig.~\ref{fig:compare_theoretical_sample_moments}). We fix $m=100$ samples and compute Manhattan (Eq.~\ref{eq:D}) distance matrices from standard normal data for $p=1000,2000,3000,4000,$ and $5000$ attributes. For each value of $p$, we generate 20 random datasets and compute the mean and standard deviation of pairwise distances. We then average these 20 simulated means and standard deviations. For comparison, we compute the theoretical moments (Eqs.~\ref{eq:normalManMean} and \ref{eq:normalManVar}) for each value of $p$ and fixed $m=100$ from the theoretical formulas. Scatter plots of theoretical versus simulated mean (Fig.~\ref{fig:compare_theoretical_sample_moments}A) and theoretical versus simulated standard deviation (Fig.~\ref{fig:compare_theoretical_sample_moments}B) indicate that our theoretical asymptotic formulas for sample moments are reliable for both large and relatively small numbers of attributes. For other combinations of data type, distance metric, sample size $m$, and number of attributes $p$, we find similar agreement between theoretical formulas and simulated moments (not shown).

\begin{figure}[H]
	\centering
	%\includegraphics[scale = 0.4]{compared_moments_normal_manhattan5.pdf}
	\caption{{\bf Comparison of theoretical and sample moments of Manhattan (Eq.~\ref{eq:D}) distances in standard normal data. (\textbf{A}) Scatter plot of theoretical vs simulated mean Manhattan distance (Eq.~\ref{eq:normalManMean}).} Each point represents a different number of attributes $p$. For each value of $p$ we fixed $m=100$ and generated 20 distance matrices from standard normal data and computed the average simulated pairwise distance from the 20 iterations. The corresponding theoretical mean was then computed for each value of $p$ for comparison. The dashed line represents the identity (or $y=x$) line for reference. (\textbf{B}) Scatter plot of theoretical vs simulated standard deviation of Manhattan (Eq.~\ref{eq:D}) distance (Eq.~\ref{eq:normalManVar}). These standard deviations come from the same random distance matrices for which mean distance was computed for \textbf{A}. Both theoretical mean and standard deviation approximate the simulated moments quite well.}\label{fig:compare_theoretical_sample_moments}
\end{figure}

\section*{Effects of correlation on distances}\label{sec:correlation}

All of the derivations presented in previous sections were for the cases where there is no correlation between instances or between attributes. We assumed that any pair $(X_{ia},X_{ja})$ of data points for instances $i$ and $j$ and fixed attribute $a$ were independent and identically distributed. This was assumed in order to determine asymptotic estimates in null data. That is, data with no main effects, interaction effects, or pairwise correlations between attributes. Within this simplified context, our asymptotic formulas for distributional moments are reliable. However, in real data are numerous statistical effects that impact distance distributional properties. We find that deviation from normality is caused primarily by large magnitude pairwise correlation between attributes. Pairwise attribute correlation can be the result of main effects, where attributes have different within-group means. On the other hand, there could be an underlying interaction network in which there are strong associations between attributes. If attributes are differentially correlated between phenotype groups, then interactions exist that change the distance distribution. In the following few sections, we consider particular cases of the $L_q$ metric for continuous and discrete data under the effects of pairwise attribute correlation.

\subsection*{Continuous data}

Without loss of generality, suppose we have $X^{(m \times p)}$ where $X_{ia} \sim \mathcal{N}(0,1)$ for all $i=1,2,\dots,m$ and $a=1,2,\dots,p$, and let $m=p=100$. We consider only the $L_2$ (Euclidean) metric (Eq.~\ref{eq:D}, $q=2$). We explore the effects of correlation on these distances by generating simulated data sets with increasing strength of pairwise attribute correlation and then plotting the density curve of the induced distances (Fig.~\ref{fig:null_vs_correlated_ridge}A). Deviation from normality in the distance distribution is directly related to the average absolute pairwise correlation that exists in the simulated data. This measure is given by
%
\begin{equation}\label{eq:abs_corr}
\bar{r}_\text{abs} = \frac{2}{p(p-1)}\sum^{p-1}_{i=1} \sum_{j > i} r_{ij}
\end{equation}
%
where $r_{ak}$ is the correlation between attributes $a,k \in \mathcal{A}$ across all instances $m$. Distances generated on data without correlation closely approximate a Gaussian. The mean (Eq.~\ref{eq:normalEucMeanImproved}) and variance (Eq.~\ref{eq:normalEucVar}) of the uncorrelated distance distribution are given by substituting $p=100$ for the mean. As $\bar{r}_\text{abs}$ increases, positive skewness and increased variability in distances emerges. The predicted and sample means, however, are approximately the same between correlated and uncorrelated distances due to linearity of the expectation operator. Because of the dependencies between attributes, the predicted variance of 1 for $L_2$ on standard normal data obviously no longer holds. 

In order to introduce a controlled level of correlation between attributes, we created correlation matrices based on a random graph with specified connection probability, where attributes correspond to the vertices in each graph. We assigned high correlations to connected attributes from the random graph and low correlations to all non-connections. Using the upper-triangular Cholesky factor $U$ for uncorrelated data matrix $X$, we computed the following product to create correlated data matrix $X^\text{corr}$
%
\begin{equation}\label{eq:cholesky}
X^\text{corr} = X U^\text{T}.
\end{equation}

The new data matrix $X^\text{corr}$ has approximately the same correlation structure as the randomly generated correlation matrix created from a random graph.

\subsection*{GWAS data}

Analogous to the previous section, we explore the effects of pairwise attribute correlation in the context of GWAS data. Without loss of generality, we let $m=p=100$ and consider only the TiTv metric (Eq.~\ref{eq:D_TiTv}). To create correlated GWAS data, we first generated standard normal data with random correlation structure, just as in the previous section. We then applied the standard normal cumulative distribution function (CDF) to this correlated data in order transform the correlated standard normal variates into uniform data with preserved correlation structure. We then subsequently applied the inverse binomial CDF to the correlated uniform data with random success probabilities $f_a \text{ for all } a \in \mathcal{A}$. Each attribute $a \in \mathcal{A}$ corresponds to an individual SNP in the data matrix. The resulting GWAS data set is binomial with $n=2$ trials and has roughly the same correlation matrix as the original correlated standard normal data with which we started. Average absolute pairwise correlation $\bar{r}_\text{abs}$ induces positive skewness in GWAS data at lower levels than in correlated standard normal data (Fig.~\ref{fig:null_vs_correlated_ridge}B). This could have important implications in nearest neighborhoods in NPDR and similar methods.
%
%\begin{figure}[H]
%	\centering
%	\framebox{\includegraphics[width=0.98\textwidth]{null_vs_correlated_gwas_TiTv_TrangEdit.pdf}}
%	\caption{Density curves of TiTv distances computed on data with correlated vs uncorrelated features. The average absolute pairwise correlation, given by $r$ (Eq.~\ref{eq:abs_corr}), is a measure of the deviation from normality in distances. There is very little difference between correlated vs uncorrelated distances when $r=0.086$. When $r=0.147$, correlated distances begin to show positive skewness. Increasing to $r=0.227$ and $r=0.299$, correlated distances show extreme skewness. Compared to Fig.~\ref{fig:null-vs-corr-normal}, it appears that correlation more drastically affects distances in discrete GWAS data than $L_q$ distances in continuous data. This could have important implications for the choice of neighborhood parameters in nearest-neighbor distance-based feature selection. As in continuous data, the average correlated and uncorrelated TiTv distances are approximately the same with clear differences in standard deviations.}\label{fig:null-vs-corr-titv}
%\end{figure}

\subsection*{Time-series derived correlation-based datasets}

For our correlation data-based metric (Eq.~\ref{eq:D_rs-fMRI}), we consider additional effects of correlation between features. Without loss of generality, we let $m=100$ and $p=30$. We show an illustration of the effects of correlated features in this context (Fig.~\ref{fig:null_vs_correlated_ridge}C). Based on the density estimates, it appears that correlation between features introduces positive skewness at low values of $\bar{r}_\text{abs}$. We introduced correlation to the transformed data matrix (Fig.~\ref{fig:rs-fMRI_matrix}) with the cholesky method used previously.
%
%\begin{figure}[H]
%	\centering
%	\framebox{\includegraphics[width=0.98\textwidth]{null_vs_correlated_rsfMRI_TrangEdit.pdf}}
%	\caption{Density curves of rs-fMRI distances on data with correlated vs uncorrelated features. The average absolute pairwise correlation, given by $r$ (Eq.~\ref{eq:abs_corr}), is a measure of the deviation from normality in distances. Even with the relatively small $r=0.09$, there is significant deviation from uncorrelated distances with increased variance and some positive skewness. A small increase to $r=0.118$ causes rather extreme positive skewness to develop in correlated distances. As we increase to $r=0.163$ and $r=0.218$, the differences between correlated and uncorrelated rs-fMRI distances become much more pronounced. It appears that the feature-feature dependencies have the largest impact on time series correlation-based data like rs-fMRI. The data already consists of pairwise correlations between ROIs, which are transformed into a single $m \times p(p-1)$ data set. Correlation is then added on top of these transformed ROI-ROI correlations to give what is shown in this figure. The average distances in uncorrelated and correlated distances are still approximately the same for this data type, with obvious differences in variance.}\label{fig:null-vs-corr-rsfMRI}
%\end{figure}

%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.98\textwidth]{TiTv_distance_histograms_MAFs.pdf}
%	\caption{Histograms of simulated TiTv distance distributions for different average MAFs. The Ti/Tv ratio was fixed to be 2 in all simulations. Average MAF is computed as the expected value of the uniform distribution from which minor allele success probabilities ($f_a$) are drawn. The upper bounds for each success probability uniform distribution are $\{0.1,0.2,0.3,0.4\}$, which are the maximum possible MAF for a given locus $a$. The corresponding lower bounds were $\{0.01,0.1,0.2,0.3\}$. Sample and predicted means, as well as standard deviations, are overlaid on each histogram. Each distance distribution comes from a simulated data set with $m=100$ instances and $p=100$ features.}\label{fig:TiTv_hist}
%\end{figure}

%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.98\textwidth]{TiTv_distance_histogram_TiTvs.pdf}
%	\caption{Histograms of simulated TiTv distance distributions for different Ti/Tv ratios. Average MAF was fixed to be 0.055. The Ti/Tv ratio was taken to be 2, 1.5, 1, and 0.5. The average distance increases as the Ti/Tv ratio decreases, which is intuitive because the TiTv distance is greater for transversions than transitions. Sample and predicted means, as well as standard deviations, are overlaid on each histogram. Each distance distribution comes from a simulated data set with $m=100$ instances and $p=100$ features.}\label{fig:TiTv_hist2}
%\end{figure}

\begin{figure}[H]
	\centering
	%\framebox{\includegraphics[width=0.98\textwidth]{null_corr_BryanEdit.pdf}}
	\caption{{\bf Distance densities from uncorrelated vs correlated bioinformatics data.} (\textbf{A}) Euclidean distance densities for random normal data with and without correlation. Correlated data was created by multiplying random normal data by upper-triangular Cholesky factor from randomly generated correlation matrix. We created correlated data for average absolute pairwise correlation (Eq.~\ref{eq:abs_corr}) $\bar{r}_\text{abs} = 0.105, 0.263, 0.458, \text{ and } 0.612$. (\textbf{B}) TiTv distance densities for random binomial data with and without correlation. Correlated data was created by first generating correlated standard normal data using the Cholesky method from (A). Then we applied the standard normal CDF to create correlated uniformly distributed data, which was then transformed by the inverse binomial CDF with $n=2$ trials and success probabilites $f_a \text{ for all } a \in \mathcal{A}$. (\textbf{C}) Time series correlation-based distance densities for random rs-fMRI data (Fig.~\ref{fig:rs-fMRI_matrix}) with and without additional pairwise feature correlation. Correlation was added to the transformed rs-fMRI data matrix (Fig.~\ref{fig:rs-fMRI_matrix}) using the Cholesky algorithm from (A).}\label{fig:null_vs_correlated_ridge}
\end{figure}

\section*{Discussion}\label{sec:discussion}

Nearest-neighbor distance-based feature selection is a class of methods that are relatively simple to implement, and they perform well at detecting interaction effects in high dimensional data. Theoretical analysis of the limiting behavior of distance distributions for various data types and dimensions may lead to improved hyperparameter estimates of these feature selection methods. Furthermore, these theoretical results may help guide the choice of distance metric for a given dataset. Most often, distance-based feature selection methods use the $L_q$ metric (Eq.~\ref{eq:D}) with $q=1$ or $q=2$. However, these two realizations of the $L_q$ metric have considerably different expressions for the mean and variance of their respective limiting distributions. For instance, the expected distance for $L_1$ and $L_2$ for standard normal data is on the order of $p$ (Eq.~\ref{eq:normalManMean} and Fig.~\ref{tab:dist_distr_standardL1L2}) and $\sqrt{p}$ (Eq.~\ref{eq:normalEucMean} and Fig.~\ref{tab:dist_distr_standardL1L2}), respectively. In addition, $L_1$ and $L_2$ on standard normal data have asymptotic variances on the order of $p$ and 1, respectively (Eqs.~\ref{eq:normalManVar} and \ref{eq:normalEucVar}). 

These results can inform the choice of $L_1$ or $L_2$ depending context. For instance, distances become harder to distinguish from one another in high dimensions, which is one of the curses of dimensionality. In the case of $L_2$, the asymptotic distribution ($\mathcal{N}(\sqrt{2p - 1},1)$) indicates that the limiting $L_2$ distribution can be thought of simply as a positive translation of the standard normal distribution ($\mathcal{N}(0,1)$). The $L_2$ distribution ($\mathcal{N}(\sqrt{2p - 1},1)$) also indicates that most neighbors are contained in a thin shell far from the instance in high dimension ($p \gg 1$). On the other hand, the $L_1$ distances become more dispersed due to the fact that the variance of the limiting distribution is proportional to the attribute dimension $p$. This $L_1$ dispersion could be more desirable when determining nearest neighbors because instances may be easier to distinguish with this metric. If using $L_1$, then it may be best to use a fixed-k algorithm instead of fixed-radius. This is because fixed-radius neighborhood order could vary quite a bit considering the $L_1$ variance is proportional to attribute dimension $p$, which in turn could affect the quality of selected attributes. If $L_2$ is being used, then perhaps either fixed-k or fixed-radius may perform equally well because most distances will be within 1 standard deviation away from the mean. 

%In any neighborhood selection algorithm, it is important to know what the average distance is and how dispersed these distances become as the feature dimension $p$ grows. 
In our analysis, we derived distance asymptotics for some of the most commonly used metrics in nearest-neighbor distance-based feature selection, as well as two new metrics for GWAS (Eq.~\ref{eq:D_TiTv}) and time series correlation-based data (Eqs.~\ref{eq:D_rs-fMRI} and \ref{eq:max-min_diff_rs-fMRI}) like resting-state fMRI. We also extended the asymptotic results for the mean and variance of the attribute range-normalized $L_q$ (max-min) distance for standard normal (Eq.~\ref{eq:max-min_DDistr_normal}) and standard uniform (Eq.~\ref{eq:max-min_DDistr_uniform}) data using extreme value theory. Our derivations provide an important reference for those using nearest-neighbor feature selection or classification methods in common bioinformatics data. 

In this work, we expanded nearest-neighbor distance-based feature selection into the context of time series correlation-based data. Our motivation for this is partly based on the application to resting-state fMRI data. In order for this to be possible, we created a new metric (Eq.~\ref{eq:diff_rs-fMRI}) that allows us to use regions of interest (ROIs) as attributes. Not all ROIs will be relevant to a particular phenotype in case-control studies, and nearest-neighbor feature selection would be a useful to tool to determine important ROIs due to interactions and to help elucidate the network structure of the brain as it relates to the phenotype of interest.

The recently introduced transition-transversion metric (Eq.~\ref{eq:diff_TiTv}) provides an additional dimension to the commonly used discrete metrics in GWAS nearest-neighbor distance-based feature selection. In this work, we have provided the asymptotic mean and variance of the limiting TiTv distance distribution. This novel result, as well as asymptotic estimates for the GM (Eq.~\ref{eq:diff_GM}) and AM (Eq.~\ref{eq:diff_AM}) metrics, provides an important reference to aid in neighborhood parameter selection in this context. We have also shown how the Ti/TV ratio $\eta$ (Eq.~\ref{eq:TiTv_constraints2}) and minor allele frequency (or success probability) $f_a$ affects these discrete distances. For the GM and AM metrics, the distance is solely determined by the minor allele frequencies because the genotype encoding is not taken into account. We showed how both minor allele frequency and Ti/Tv ratio uniquely affects the TiTv distance (Figs.~\ref{fig:TiTv_ridge}A and \ref{fig:TiTv_ridge}C). Because transversions are more drastic forms of mutation than transitions, this additional dimension of information is important to consider, which is why we have provided asymptotic results for this metric.

In addition to asymptotic $L_q$ distance distributions, we have also provided the exact distributions for the one-dimensional projection of the $L_q$ distance onto individual attributes (Sections.~\ref{sec:continuous_diff}, \ref{sec:GWAS_diff}, and \ref{sec:rs-fMRI_diff}). These distributions are important for all nearest-neighbor distance-based feature selection algorithms, such as Relief or NPDR, because the $L_q$ distance is a function of the one-dimensional attribute projection (diff). In particular, these projected distance distributions are important for improving inference for predictors in NPDR, which are one-dimensional attribute projections.

Correlations between attributes and instances can cause significant deviations from the asymptotic results for uncorrelated data we have derived in this work. To illustrate this behavior, we showed how strong correlations lead to positive skewness in the distance distribution of random normal, binomial, and rs-fMRI data (Figs.~\ref{fig:null_vs_correlated_ridge}A, \ref{fig:null_vs_correlated_ridge}B, and \ref{fig:null_vs_correlated_ridge}C). Pairwise correlation between attributes does not change the average distance, so our asymptotic results for uncorrelated data also apply when attributes are not independent. In contrast, the sample variance of distances deviates from the uncorrelated case substantially as the average absolute pairwise attribute correlation increases (Eq.~\ref{eq:abs_corr}). For fixed-radius neighborhood methods, this deviation increases the probability of including neighbors for a given instance and may reduce the power to detect interactions. The increased variability for distances with correlated data may inform the choice of metric and optimization of neighborhoods in nearest-neighbor feature selection. 

\section*{Supporting information}

% Include only the SI item label in the paragraph heading. Use the \nameref{label} command to cite SI items in the text.
\paragraph*{S1 Fig.}
\label{S1_Fig}
{\bf Convergence to Gaussian for Manhattan and Euclidean distances for simulated standard uniform data with $m=100$ instances and $p=10, 100,$ and $10000$ attributes.} Convergence to Gaussian occurs rapidly with increasing $p$, and Gaussian is a good approximation for $p$ as low as $10$ attributes. The number of attributes in bioinformatics data is typically much larger, at least on the order of $10^3$. The Euclidean metric has stronger convergence to normal than Manhattan. P values from Shapiro-Wilk test, where the null hypothesis is a Gaussian distribution.

\paragraph*{S2 Fig.}
\label{S2_Fig}
{\bf Convergence to Gaussian for Manhattan and Euclidean distances for simulated standard normal data with $m=100$ instances and $p=10, 100,$ and $10000$ attributes.} Convergence to Gaussian occurs rapidly with increasing $p$, and Gaussian is a good approximation for $p$ as low as $10$ attributes. The number of attributes in bioinformatics data is typically much larger, at least on the order of $10^3$. The Euclidean metric has stronger convergence to normal than Manhattan.  P values from Shapiro-Wilk test, where the null hypothesis is a Gaussian distribution.

\paragraph*{S3 Fig.}
\label{S3_Fig}
{\bf Convergence to Gaussian for max-min normalized Manhattan and Euclidean distances for simulated standard normal data with $m=100$ instances and $p=10, 100,$ and $10000$ attributes.} Convergence to Gaussian occurs rapidly with increasing $p$, and Gaussian is a good approximation for $p$ as low as $10$ attributes. The number of attributes in bioinformatics data is typically much larger, at least on the order of $10^3$. The Euclidean metric has stronger convergence to normal than Manhattan.  P values from Shapiro-Wilk test, where the null hypothesis is a Gaussian distribution.

\paragraph*{S4 Fig.}
\label{S4_Fig}
{\bf Convergence to Gaussian for GM distances for simulated binomial GWAS data with $m=100$ instances and $p=10, 100, 1000,$ and $10000$ attributes.} The average MAF was set to 0.205 for all simulations. Convergence to Gaussian occurs more gradually with increasing $p$ than in continuous data. Significant convergence seems to occur when $p \geq 1000$, however, this is actually a relatively small number of features in the context of GWAS. Considering a realistic number of features for GWAS, the normality assumption of GM distances holds. This metric has the slowest convergence to Gaussian among all we have considered. P values from Shapiro-Wilk test, where the null hypothesis is a Gaussian distribution.

\paragraph*{S5 Fig.}
\label{S5_Fig}
{\bf Convergence to Gaussian for AM distances for simulated binomial GWAS data with $m=100$ instances and $p=10, 100, 1000,$ and $10000$ attributes.} The average MAF was set to 0.205 for all simulations. Convergence to Gaussian occurs more gradually with increasing $p$ than in continuous data. Significant convergence seems to occur when $p \geq 1000$, however, this is actually a relatively small number of features in the context of GWAS. Considering a realistic number of features for GWAS, the normality assumption of AM distances holds. This metric has the slightly faster convergence to Gaussian than the GM metric, which is probably due to the fact that the AM metric has one more value in its range (e.g., 1/2). P values from Shapiro-Wilk test, where the null hypothesis is a Gaussian distribution.

\paragraph*{S6 Fig.}
\label{S6_Fig}
{\bf Convergence to Gaussian for TiTv distances for simulated binomial GWAS data with $m=100$ instances and $p=10, 100, 1000,$ and $10000$ attributes.} The average MAF was set to 0.205 for all simulations and the Ti/Tv ratio ($\eta$) was set to 2. Convergence to Gaussian occurs more gradually with increasing $p$ than in continuous data. Significant convergence seems to occur when $p \geq 1000$, however, this is actually a relatively small number of features in the context of GWAS. Considering a realistic number of features for GWAS, the normality assumption of TiTv distances holds. This metric has the significantly faster convergence to Gaussian than the AM metric, which is probably due to the fact that the TiTv metric contains 2 more values in its range (e.g., 1/4 \& 3/4). P values from Shapiro-Wilk test, where the null hypothesis is a Gaussian distribution.

\paragraph*{S7 Fig.}
\label{S7_Fig}
{\bf Convergence to Gaussian for rs-fMRI distances for simulated correlation matrices with $m=100$ instances and $p=10, 50, 150,$ and $300$ attributes (or ROIs).} Correlation matrices were generated for each instance from random normal $m \times p$ data matrices. Each correlation matrix was then stretched out into a long vector, Fisher r-to-z transformed, stored in a $p(p-1) \times m$ matrix, and standardized so that the $m$ columns are mean 0 and unit variance. Convergence to Gaussian occurs very rapidly for this data because the dimensions are larger than a typical $m \times p$ data set. The large attribute dimension $p(p-1)$ means that there are significantly more terms in each sum to compute pairwise distances. Therefore, Classical Central Limit Theorem dictates that distances in this context will be closer to Gaussian. P values from Shapiro-Wilk test, where the null hypothesis is a Gaussian distribution.

\paragraph*{S8 Fig.}
\label{S8_Fig}
{\bf Convergence to Gaussian for max-min normalized rs-fMRI distances for simulated correlation matrices with $m=100$ instances and $p=10, 50, 150,$ and $300$ attributes (or ROIs).} Correlation matrices were generated for each instance from random normal $m \times p$ data matrices. Each correlation matrix was then stretched out into a long vector, Fisher r-to-z transformed, stored in a $p(p-1) \times m$ matrix, and standardized so that the $m$ columns are mean 0 and unit variance. Convergence to Gaussian occurs approximately as rapidly as the standard rs-fMRI metric. Just as in the standard rs-fMRI metric, the large attribute dimension $p(p-1)$ means that there are significantly more terms in each sum to compute pairwise distances. Therefore, Classical Central Limit Theorem dictates that distances in this context will be closer to Gaussian. P values from Shapiro-Wilk test, where the null hypothesis is a Gaussian distribution.

\paragraph*{S9 Fig.}
\label{S9_Fig}
{\bf Comparison of theoretical and simulated moments of Manhattan distances in standard normal data.} (\textbf{A}) Scatter plot of theoretical vs simulated mean Manhattan distance. Each point represents a different number of attributes $p$. For each value of $p$ we fixed $m=100$ and generated 20 distance matrices from standard normal data and computed the average simulated pairwise distance from the 20 iterations. The corresponding theoretical mean was then computed for each value of $p$ for comparison. The dashed line represents the identity (or $y=x$) line for reference. (\textbf{B}) Scatter plot of theoretical vs simulated standard deviation of Manhattan distance. These standard deviations come from the same random distance matrices for which mean distance was computed for \textbf{A}. Both theoretical mean and standard deviation approximate the simulated moments quite well.

\paragraph*{S10 Fig.}
\label{S10_Fig}
{\bf Comparison of theoretical and simulated moments of max-min normalized Manhattan distances in standard normal data.} (\textbf{A}) Scatter plot of theoretical vs simulated mean max-min normalized Manhattan distance. Each point represents a different number of attributes $p$. For each value of $p$ we fixed $m=100$ and generated 20 distance matrices from standard normal data and computed the average simulated pairwise distance from the 20 iterations. The corresponding theoretical mean was then computed for each value of $p$ for comparison. The dashed line represents the identity (or $y=x$) line for reference. (\textbf{B}) Scatter plot of theoretical vs simulated standard deviation of max-min normalized Manhattan distance. These standard deviations come from the same random distance matrices for which mean distance was computed for \textbf{A}. Both theoretical mean and standard deviation approximate the simulated moments quite well.

\paragraph*{S11 Fig.}
\label{S11_Fig}
{\bf Comparison of theoretical and simulated moments of Euclidean distances in standard normal data.} (\textbf{A}) Scatter plot of theoretical vs simulated mean Euclidean distance. Each point represents a different number of attributes $p$. For each value of $p$ we fixed $m=100$ and generated 20 distance matrices from standard normal data and computed the average simulated pairwise distance from the 20 iterations. The corresponding theoretical mean was then computed for each value of $p$ for comparison. The dashed line represents the identity (or $y=x$) line for reference. (\textbf{B}) Scatter plot of theoretical vs simulated standard deviation of Euclidean distance. These standard deviations come from the same random distance matrices for which mean distance was computed for \textbf{A}. Theoretical and simulated means lie approximately on the identity line because the mean is proportional to attribute dimension $p$. Theoretical standard deviation is constant, which is why each horizontal coordinate is the same for $p=1000,2000,3000,4000,$ and $5000$. The variation in sample standard deviation of Euclidean distance is quite small, so each simulated moment is clustered about 1.

\paragraph*{S12 Fig.}
\label{S12_Fig}
{\bf Comparison of theoretical and simulated moments of max-min normalized Euclidean distances in standard normal data.} (\textbf{A}) Scatter plot of theoretical vs simulated mean max-min normalized Euclidean distance. Each point represents a different number of attributes $p$. For each value of $p$ we fixed $m=100$ and generated 20 distance matrices from standard normal data and computed the average simulated pairwise distance from the 20 iterations. The corresponding theoretical mean was then computed for each value of $p$ for comparison. The dashed line represents the identity (or $y=x$) line for reference. (\textbf{B}) Scatter plot of theoretical vs simulated standard deviation of max-min normalized Euclidean distance. These standard deviations come from the same random distance matrices for which mean distance was computed for \textbf{A}. Theoretical and simulated means lie approximately on the identity line because the mean is proportional to $\sqrt{p}$. Theoretical standard deviation is a function of the fixed attribute dimension $m$, which is why each horizontal coordinate is the same for $p=1000,2000,3000,4000,$ and $5000$. The variation in sample standard deviation of max-min normalized Euclidean distance is quite small, so each simulated moment is clustered about the theoretical value that depends on $m$.

\paragraph*{S13 Fig.}
\label{S13_Fig}
{\bf Comparison of theoretical and simulated moments of Manhattan distances in standard uniform data.} (\textbf{A}) Scatter plot of theoretical vs simulated mean Manhattan distance. Each point represents a different number of attributes $p$. For each value of $p$ we fixed $m=100$ and generated 20 distance matrices from standard uniform data and computed the average simulated pairwise distance from the 20 iterations. The corresponding theoretical mean was then computed for each value of $p$ for comparison. The dashed line represents the identity (or $y=x$) line for reference. (\textbf{B}) Scatter plot of theoretical vs simulated standard deviation of Manhattan distance. These standard deviations come from the same random distance matrices for which mean distance was computed for \textbf{A}. Both theoretical mean and standard deviation approximate the simulated moments quite well.

\paragraph*{S14 Fig.}
\label{S14_Fig}
{\bf Comparison of theoretical and simulated moments of Euclidean distances in standard uniform data.} (\textbf{A}) Scatter plot of theoretical vs simulated mean Euclidean distance. Each point represents a different number of attributes $p$. For each value of $p$ we fixed $m=100$ and generated 20 distance matrices from standard uniform data and computed the average simulated pairwise distance from the 20 iterations. The corresponding theoretical mean was then computed for each value of $p$ for comparison. The dashed line represents the identity (or $y=x$) line for reference. (\textbf{B}) Scatter plot of theoretical vs simulated standard deviation of Euclidean distance. These standard deviations come from the same random distance matrices for which mean distance was computed for \textbf{A}. Theoretical and simulated means lie approximately on the identity line because the mean is proportional to attribute dimension $p$. Theoretical standard deviation is constant, which is why each horizontal coordinate is the same for $p=1000,2000,3000,4000,$ and $5000$. The variation in sample standard deviation of Euclidean distance is quite small, so each simulated moment is clustered about 7/120.

\paragraph*{S15 Fig.}
\label{S15_Fig}
{\bf Comparison of theoretical and simulated moments of max-min normalized Euclidean distances in standard uniform data.} (\textbf{A}) Scatter plot of theoretical vs simulated mean max-min normalized Euclidean distance. Each point represents a different number of attributes $p$. For each value of $p$ we fixed $m=100$ and generated 20 distance matrices from standard uniform data and computed the average simulated pairwise distance from the 20 iterations. The corresponding theoretical mean was then computed for each value of $p$ for comparison. The dashed line represents the identity (or $y=x$) line for reference. (\textbf{B}) Scatter plot of theoretical vs simulated standard deviation of max-min normalized Euclidean distance. These standard deviations come from the same random distance matrices for which mean distance was computed for \textbf{A}. Theoretical and simulated means lie approximately on the identity line because the mean is proportional to $\sqrt{p}$. Theoretical standard deviation is a function of the fixed attribute dimension $m$, which is why each horizontal coordinate is the same for $p=1000,2000,3000,4000,$ and $5000$. The variation in sample standard deviation of max-min normalized Euclidean distance is quite small, so each simulated moment is clustered about the theoretical value that depends on $m$.

\paragraph*{S16 Fig.}
\label{S16_Fig}
{\bf Comparison of theoretical and simulated moments of GM distances in binomial GWAS data.} (\textbf{A}) Scatter plot of theoretical vs simulated mean GM distance. Each point represents a different number of attributes $p$. For each value of $p$ we fixed $m=100$ and generated 20 distance matrices from binomial GWAS data and computed the average simulated pairwise distance from the 20 iterations. The corresponding theoretical mean was then computed for each value of $p$ for comparison. The dashed line represents the identity (or $y=x$) line for reference. (\textbf{B}) Scatter plot of theoretical vs simulated standard deviation of GM distance. These standard deviations come from the same random distance matrices for which mean distance was computed for \textbf{A}. Both theoretical mean and standard deviation approximate the simulated moments quite well.

\paragraph*{S17 Fig.}
\label{S17_Fig}
{\bf Comparison of theoretical and simulated moments of AM distances in binomial GWAS data.} (\textbf{A}) Scatter plot of theoretical vs simulated mean AM distance. Each point represents a different number of attributes $p$. For each value of $p$ we fixed $m=100$ and generated 20 distance matrices from binomial GWAS data and computed the average simulated pairwise distance from the 20 iterations. The corresponding theoretical mean was then computed for each value of $p$ for comparison. The dashed line represents the identity (or $y=x$) line for reference. (\textbf{B}) Scatter plot of theoretical vs simulated standard deviation of AM distance. These standard deviations come from the same random distance matrices for which mean distance was computed for \textbf{A}. Both theoretical mean and standard deviation approximate the simulated moments quite well.

\paragraph*{S18 Fig.}
\label{S18_Fig}
{\bf Comparison of theoretical and simulated moments of TiTv distances in binomial GWAS data.} For each simulated data set, the average MAF was set to be 0.205 and the Ti/Tv ratio ($\eta$) was fixed to be 2. (\textbf{A}) Scatter plot of theoretical vs simulated mean TiTv distance. Each point represents a different number of attributes $p$. For each value of $p$ we fixed $m=100$ and generated 20 distance matrices from binomial GWAS data and computed the average simulated pairwise distance from the 20 iterations. The corresponding theoretical mean was then computed for each value of $p$ for comparison. The dashed line represents the identity (or $y=x$) line for reference. (\textbf{B}) Scatter plot of theoretical vs simulated standard deviation of TiTv distance. These standard deviations come from the same random distance matrices for which mean distance was computed for \textbf{A}. Both theoretical mean and standard deviation approximate the simulated moments quite well.

\paragraph*{S19 Fig.}
\label{S19_Fig}
{\bf Comparison of theoretical and simulated moments of rs-fMRI distances from random correlation matrices.} For each instance, we generated a $p \times p$ correlation matrix from a random $m \times p$ standard normal data set. We then stretched out each correlation matrix into a long vector, Fisher r-to-z transformed the correlations, stored the vector in a column of a large $p(p-1) \times m$ matrix, and then standardized columns to be mean 0 and unit variance. (\textbf{A}) Scatter plot of theoretical vs simulated mean rs-fMRI distance. Each point represents a different number of attributes $p$. For each value of $p$ we fixed $m=100$ and generated 20 distance matrices from rs-fMRI data and computed the average simulated pairwise distance from the 20 iterations. The corresponding theoretical mean was then computed for each value of $p$ for comparison. The dashed line represents the identity (or $y=x$) line for reference. (\textbf{B}) Scatter plot of theoretical vs simulated standard deviation of rs-fMRI distance. These standard deviations come from the same random distance matrices for which mean distance was computed for \textbf{A}. Both theoretical mean and standard deviation approximate the simulated moments quite well.

\paragraph*{S20 Fig.}
\label{S20_Fig}
{\bf Comparison of theoretical and simulated moments of max-min normalized rs-fMRI distances from random correlation matrices.} For each instance, we generated a $p \times p$ correlation matrix from a random $m \times p$ standard normal data set. We then stretched out each correlation matrix into a long vector, Fisher r-to-z transformed the correlations, stored the vector in a column of a large $p(p-1) \times m$ matrix, and then standardized columns to be mean 0 and unit variance. (\textbf{A}) Scatter plot of theoretical vs simulated mean max-min normalized rs-fMRI distance. Each point represents a different number of attributes $p$. For each value of $p$ we fixed $m=100$ and generated 20 distance matrices from rs-fMRI data and computed the average simulated pairwise distance from the 20 iterations. The corresponding theoretical mean was then computed for each value of $p$ for comparison. The dashed line represents the identity (or $y=x$) line for reference. (\textbf{B}) Scatter plot of theoretical vs simulated standard deviation of max-min normalized rs-fMRI distance. These standard deviations come from the same random distance matrices for which mean distance was computed for \textbf{A}. Both theoretical mean and standard deviation approximate the simulated moments quite well.

\paragraph*{S21 Fig.}
\label{S21_Fig}
{\bf Comparison of theoretical and simulated moments of rs-fMRI diff metric from random correlation matrices.} For each instance, we generated a $p \times p$ correlation matrix from a random $m \times p$ standard normal data set. We then stretched out each correlation matrix into a long vector, Fisher r-to-z transformed the correlations, stored the vector in a column of a large $p(p-1) \times m$ matrix, and then standardized columns to be mean 0 and unit variance. (\textbf{A}) Scatter plot of theoretical vs simulated mean rs-fMRI diff. Each point represents a different number of attributes $p$. For each value of $p$ we fixed $m=100$ and generated 20 diff metric values from the rs-fMRI data and computed the average simulated diff from the 20 iterations. The corresponding theoretical mean was then computed for each value of $p$ for comparison. The dashed line represents the identity (or $y=x$) line for reference. (\textbf{B}) Scatter plot of theoretical vs simulated standard deviation of rs-fMRI diff. These standard deviations come from the same random diff values from which mean diffs were computed for \textbf{A}. Both theoretical mean and standard deviation approximate the simulated moments quite well.

\paragraph*{S22 Fig.}
\label{S22_Fig}
{\bf Density curves for one-dimensional projected distances (diffs) onto a fixed attribute $a$ for standard normal and standard uniform data.} (\textbf{A}) Density curves for the distribution of attribute diff in standard normal data for $q=1,2,3,4,$ and $5$. This density is that of a Generalized Gamma distribution. For $q=1$, this is also known as a half-normal distribution. (\textbf{B}) Density curves for the distribution of attribute diff in standard uniform data for $q=1,2,3,4,$ and $5$. This density is that of a Kumaraswamy distribution. For $q=1$, this is also known as a triangular distribution.

\paragraph*{S23 Fig.}
\label{S23_Fig}
{\bf One-dimensional projected GM distance (diff) onto an attribute vs minor allele frequency (MAF).} For each possible value of the GM diff ($\text{d} = 0,1$), the exact density of the GM diff is plotted for all possible values of MAF. The expected value of MAF at a particular locus $a$ is $f_a$ for all $a \in \mathcal{A}$, where $f_a$ is the probability of a minor allele occurring at locus $a$. For each element $X_{ia}$ of the data matrix for a fixed attribute $a$, we have $X_{ia} \sim \mathcal{B}(2,f_a)$. Depending on the MAF, the frequency of GM diff taking on a value of 0 or 1 changes. For small MAF, the GM diff will be 0 most often. As MAF increases beyond 0.5, the minor allele switches.

\paragraph*{S24 Fig.}
\label{S24_Fig}
{\bf One-dimensional projected AM distance (diff) onto an attribute vs minor allele frequency (MAF).} For each possible value of the AM diff ($\text{d} = 0,1/2,1$), the exact density of the AM diff is plotted for all possible values of MAF. The expected value of MAF at a particular locus $a$ is $f_a$ for all $a \in \mathcal{A}$, where $f_a$ is the probability of a minor allele occurring at locus $a$. For each element $X_{ia}$ of the data matrix for a fixed attribute $a$, we have $X_{ia} \sim \mathcal{B}(2,f_a)$. Depending on the MAF, the frequency of AM diff taking on a value of 0, 1/2, or 1 changes. For small MAF, the AM diff will be 0 most often. For large MAF ($\approx$0.5), the AM diff will be mostly 1/2 with 1 being the second most common value. The least common value of the AM diff is 1. As MAF increases beyond 0.5, the minor allele switches.

\paragraph*{S25 Fig.}
\label{S25_Fig}
{\bf One-dimensional projected TiTv distance (diff) onto an attribute vs minor allele frequency (MAF).} For each possible value of the TiTv diff ($\text{d} = 0,1/4,1/2,3/4,1$), the exact density of the TiTv diff is plotted for all possible values of MAF. The Ti/Tv ratio $\eta$ was fixed to be 2. The expected value of MAF at a particular locus $a$ is $f_a$ for all $a \in \mathcal{A}$, where $f_a$ is the probability of a minor allele occurring at locus $a$. For each element $X_{ia}$ of the data matrix for a fixed attribute $a$, we have $X_{ia} \sim \mathcal{B}(2,f_a)$. Depending on the MAF, the frequency of TiTv diff taking on a value of 0, 1/4, 1/2, 3/4, or 1 changes. The density of the TiTv diff for $\text{d} = 1/4, 1/2, \text{ and } 3/4$ has the same resulting curve as a function of MAF. The most common value of TiTv diff at any MAF is 0, the second most common is 1/4, 1/2, or 3/4, and the least common is 1. As MAF increases beyond 0.5, the minor allele switches.

%\section*{Acknowledgments}
%Cras egestas velit mauris, eu mollis turpis pellentesque sit amet. Interdum et malesuada fames ac ante ipsum primis in faucibus. Nam id pretium nisi. Sed ac quam id nisi malesuada congue. Sed interdum aliquet augue, at pellentesque quam rhoncus vitae.

\nolinenumbers

% Either type in your references using
% \begin{thebibliography}{}
% \bibitem{}
% Text
% \end{thebibliography}
%
% or
%
% Compile your BiBTeX database using our plos2015.bst
% style file and paste the contents of your .bbl file
% here. See http://journals.plos.org/plosone/s/latex for 
% step-by-step instructions.
% 

%\bibliographystyle{plos2015}

%\begin{thebibliography}{10}

\bibliography{BoD}   % name of bib file

%\end{thebibliography}



\end{document}

