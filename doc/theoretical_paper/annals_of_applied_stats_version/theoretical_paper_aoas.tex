% Template for the submission to:
%   The Annals of Probability           [aop]
%   The Annals of Applied Probability   [aap]
%   The Annals of Statistics            [aos] 
%   The Annals of Applied Statistics    [aoas]
%   Stochastic Systems                  [ssy]
%
%Author: In this template, the places where you need to add information
%        (or delete line) are indicated by {???}.  Mostly the information
%        required is obvious, but some explanations are given in lines starting
%Author:
%All other lines should be ignored.  After editing, there should be
%no instances of ??? after this line.

% use option [preprint] to remove info line at bottom
% journal options: aop,aap,aos,aoas,ssy
% natbib option: authoryear
\documentclass[aoas]{imsart}

\usepackage{amsthm,amsmath,natbib}
\RequirePackage[colorlinks,citecolor=blue,urlcolor=blue]{hyperref}

\usepackage{float}

% amssymb package useful for mathematical formulas and symbols
\usepackage{amssymb}

% color can be used to apply background shading to table cells only
\usepackage[table]{xcolor}

% array package and thick rules for tables
\usepackage{array}

% adjust width of tikz tables or figures
\usepackage{adjustbox}

% bold math symbols package
\usepackage{bm}

% nice figures and captions
\usepackage{graphicx}

% diagrams or complicated equations
\usepackage{tikz}

% provide arXiv number if available:
%\arxiv{arXiv:0000.0000}

% put your definitions there:
\startlocaldefs
% define theorem and definition environments commands
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\endlocaldefs

\begin{document}

\begin{frontmatter}

% "Title of the paper"
\title{Theoretical properties of nearest-neighbor distance distributions and novel metrics for high dimensional bioinformatics data}
\runtitle{nearest-neighbor distances in high dimensions}

\begin{aug}
	\author{\fnms{Bryan A.} \snm{Dawkins}\thanksref{t1,t2,m1}\ead[label=e1]{bryan-dawkins@utulsa.edu}},
	\author{\fnms{Trang T.} \snm{Le}\thanksref{t3,m1,m2}\ead[label=e2]{trang-le@utulsa.edu}}
	\\
	\and \author{\fnms{Brett A.} \snm{McKinney}\thanksref{t1,m2}
		\ead[label=e3]{brett-mckinney@utulsa.edu}
		\ead[label=u1,url]{insilico.utulsa.edu}}
	
	\thankstext{t1}{Some comment}
	\thankstext{t2}{First supporter of the project}
	\thankstext{t3}{Second supporter of the project}
	\runauthor{B. Dawkins et al.}
	
	\affiliation{University of Tulsa\thanksmark{m1} and University of Pennsylvania\thanksmark{m2}}
	
	\address{Department of Mathematics\\
		University of Tulsa\\
		Tulsa, OK 74104, USA\\
		Usually a few lines long\\
		\printead{e1}}
	
		\address{Department of Biostatistics\\
			Epidemiology and Informatics\\
			University of Pennsylvania\\
			Philadelphia, PA 19104\\
		\printead{e2}}
	
	\address{Tandy School of Computer Science\\ 
		University of Tulsa\\ 
		Tulsa, OK 74104, USA\\
		\printead{e3}\\
		\printead{u1}}
\end{aug}

\begin{abstract}
	The performance of nearest-neighbor feature selection and prediction methods depends on the metric for computing neighborhoods and the distribution type of the underlying data. The effects of the distribution and metric, as well as the presence of correlation and interactions, are reflected in the expected moments of the distribution of pairwise distances. We derive general analytical expressions for the mean and variance of pairwise distances for $L_q$ metrics for Gaussian and uniform data with $p$ attributes and $m$ instances. We use extreme value theory to derive results for metrics normalized by the max and min of attributes. These expressions are applicable to the analysis of continuous data such as gene expression. We derive similar analytical expressions for a new metric for genetic variants in genome-wide association studies (GWAS) data (categorical predictors) that accounts for minor allele frequency and transition/transversion ratio. We introduce a new metric for resting-state fMRI data that is applicable to correlation-based predictors from time series data. Derivations assume independent data, but empirically we also consider the effect of correlation. This study provides detailed derivations and expressions parameterized by $q$, $p$, $m$ and other properties for a broad collection of bioinformatics data types. 
\end{abstract}

\begin{keyword}[class=MSC]
	\kwd[Primary ]{62P10} % applications to biology and medical sciences
	\kwd{92B15} % general biostatistics
	\kwd[; secondary ]{62G20} % asymptotic properties
	\kwd{62G30} % order statistics; empirical distribution functions
	\kwd{62G32} % statistics of extreme values; tail inference
	\kwd{62E20} % asymptotic distribution theory
	\kwd{62E15} % exact distribution theory
	\kwd{92D10} % genetics
	\kwd{92D20} % protein sequences, DNA sequences
	%\kwd[; secondary ]{60K35}
\end{keyword}

\begin{keyword}
	\kwd[Key words and phrases. ]{Nearest neighbor methods,}
	\kwd{Distance distributions,}
	\kwd{Feature selection,}
	\kwd{Relief-based algorithms,}
	\kwd{High-dimensional data,}
	\kwd{Gene expression,}
	\kwd{Genome-wide association studies,}
	\kwd{Resting-state fMRI.}
\end{keyword}

\end{frontmatter}

\section*{Introduction}
Statistical models can deviate from expected behavior depending on whether certain properties of the underlying data are satisfied, such as having a Gaussian distribution. Nearest neighbor methods are further influenced by the choice of metric, such as Euclidean or Manhattan. For random normal data ($\mathcal{N}(0,1)$), for example, the variance of the pairwise distances of a Manhattan metric is proportional to the number of attributes ($p$) whereas the variance is constant for a Euclidean metric. Relief methods~\cite{urbanowicz17,urbanowicz17b,robnik2003} and nearest-neighbor projected distance regression (NDPR)~\cite{npdr}. use nearest neighbors to compute attribute importance scores and often use adaptive neighborhoods that rely on the mean and variance of the distance distribution. Thus, knowledge of the expected values for a given metric and data distribution may improve the performance of these feature selection methods. 

For continuous data, the metrics most commonly used in nearest neighbor methods are $L_q$ with $q=1$ (Manhattan) or $q=2$ (Euclidean). For data from standard normal ($\mathcal{N}(0,1)$) or standard uniform ($\mathcal{U}(0,1)$) distributions, the asymptotic behavior of the $L_q$ metrics is known. However, detailed derivations of these distance distribution asymptotics are not readily available in the literature. We provide detailed derivations of generalized expressions parameterized by metric $q$, attributes $p$, and samples $m$. We build on this mathematical formalism to derive the asymptotic properties of a new metric for categorical data in GWAS data~\cite{arabnejad2018}. We also derive asymptotic properties of a new metric introduced in the current study for resting-state fMRI (rs-fMRI) correlation data, where correlation is computed from time series.

We derive asymptotic formulas for the mean and variance for three recently introduced GWAS metrics~\cite{arabnejad2018}. These metrics were developed for Relief-based feature selection to account for binary genotype differences (two levels), allelic differences (three levels), and transition/transversion differences (five levels). The mean and variance expressions we derive for these multi-level categorical data types are parameterized by the minor allele frequency and the transition/transversion ratio. 

The analysis of rs-fMRI data is a growing area for machine learning and feature selection~\cite{venkataraman2010,hay2017,sundermann2014,vergun2013}. For a given subject, a time series correlation or similar matrix is computed between brain regions of interest (ROIs) from their time series. Each time series represents functional activity of the ROI while the subject not performing a task, and the ROI typically corresponds to a region with known function for emotion or cognition.  Thus, the dataset consists of pairwise ROI correlations for each of the m subjects. Nearest-neighbor based feature selection was applied to rs-fMRI in the private evaporative cooling method \cite{le17}, where the predictors were pairwise correlations between ROIs. The use of pairwise correlation predictors is a common practice because of convenience and differential connectivity between brain regions may be of biological importance~\cite{gotts2012}. However, one may be interested in the importance of features at the ROI level. Thus, in the current study we introduce a new metric to be used in NPDR~\cite{npdr} with resting state correlation matrices that provides feature importance for ROIs. This metric is applicable to general time series-correlation based data, and we derive asymptotic estimates for the mean and variance of distance distributions induced by our new ts-corr based metric.

% [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3437021/]

The ability of nearest neighbor feature selection to identify association effects, like main effects or interaction effects, depends on neighborhood parameters, such as neighborhood radius or number of neighbors k. As k increases, nearest neighbor distance based algorithms are more sensitive to detecting main effects~\cite{stir}. On the other hand, their ability to detect interaction effects decreases if k becomes too large~\cite{stir,mckinney13}. Correlation and interaction effects impact distance distributions by introducing positive skewness and increased variance, which can lead to changes in neighborhood inclusion. In order to understand how these statistical effects impact distance distributions in continuous and discrete data types, we first derive distance asymptotics for independently and identically distributed data. Using these derivations as a baseline, we can then determine how outcome associated effects and correlation change distance distribution properties from the null case, or no outcome association. 

In Section~\ref{sec:notation_and_CLT}, we introduce preliminary notation and apply the Central Limit Theorem (CLT) and the Delta Method to derive asymptotics for pairwise distances. In Section~\ref{sec:moment_derivations}, we present general derivations for continuously distributed data sets with $m$ instances and $p$ features. We begin with the cases of standard normal ($\mathcal{N}(0,1)$) and standard uniform ($\mathcal{U}(0,1)$) data distributions, but we derive analytical expressions parameterized by $p$ and $q$. In Section~\ref{sec:extremes} we use Extreme Value Theory (EVT) to address max-min normalized versions of $L_q$ metrics, which are often used in Relief-based algorithms~\cite{robnik2003}. In Section~\ref{sec:gwas_distances}, we extend the derivations to categorical data with a binomial distribution for GWAS data with multiple metric types. In Section~\ref{sec:rs-fMRI_distances}, we present a new time series correlation-based distance metric, with a particular emphasis on rs-fMRI data, and we derive the corresponding asymptotic distance distribution results. Lastly, in Section~\ref{sec:correlation}, we demonstrate the effect of correlation in the attribute space on distance distributional properties. 

\section{Limit distribution for \texorpdfstring{$L_q$}{} on null data}\label{sec:notation_and_CLT}
In the application of nearest-neighbor distance-based methods to continuous data, the distance between instances ($i,j \in \mathcal{I}, |\mathcal{I}|=m$) in the data set $X^{m \times p}$ of $m$ instances and $p$ attributes (or features) is calculated in the space of all attributes ($a \in \mathcal{A}$, $|\mathcal{A}|=p$) using a metric such as
\begin{equation}\label{eq:D}
% D^{(q)}_{ij}=\left(\sum_{a\in A}|\text{d}^{\text{type}}_{ij}(a)|^q\right)^{1/q},
D^{(q)}_{ij}=\left(\sum_{a\in \mathcal{A}}|\text{d}_{ij}(a)|^q\right)^{1/q},
\end{equation}
which is typically Manhattan ($q=1$) in Relief-based methods but may also be Euclidean ($q=2$). We use the terms ``feature" and ``attribute" interchangeably for the remainder of this work. The quantity 
% $\text{d}^{\text{type}}_{ij}(a)$, 
$\text{d}_{ij}(a)$,
known as a ``$\text{diff}$'' in Relief literature, is the projection of the distance between instances $i$ and $j$ onto the attribute $a$ dimension. The 
% ``type'' refers to the data type of the attribute
function $\text{d}_{ij}(a)$ supports any type of attributes
(e.g., numeric and categorical).
For example, the projected difference between two instances $i$ and $j$ for a continuous numeric ($\text{d}^{\text{num}}$) attribute $a$ may be
%\begin{equation}\label{eq:diff}
%\text{diff}^{(\text{num})}(a,(\ri,\rj))=\frac{|\text{value}(a,\ri)-\text{value}(a,\rj)%|}{\max(a)-\min(a)}.
%\end{equation}
\begin{equation}\label{eq:diff}
\begin{aligned}
\text{d}^{\text{num}}_{ij}(a)&=\text{diff}(a,(i,j))\\
& = {|\hat{X}_{ia}-\hat{X}_{ja}|},
\end{aligned}
\end{equation}
where $\hat{X}$ represents the standardized data matrix $X$.
We use a simplified d$_{ij}(a)$ notation in place of the $\text{diff}(a,(i,j))$ notation that is customary in Relief-based methods.
In NPDR, we omit the division by $\max(a)-\min(a)$ used by Relief to constrain scores to the interval from $-1$ to $1$, where $\max(a) = \max_{k \in \mathcal{I}}\{X_{ka}\}$ and $\min(a) = \min{k \in \mathcal{I}}\{X_{ka}\}$. The numeric d$^{\text{num}}_{ij}(a)$ projection is simply the absolute difference between row elements $i$ and $j$ of the data matrix $X^{m \times p}$ for the attribute column $a$. 

All derivations in the following sections are applicable to nearest-neighbor distance-based methods in general, which includes not only NPDR, but also Relief-based algorithms. Each of these methods uses a distance metric (Eq.~\ref{eq:D}) to compute neighbors for each instance $i \in \mathcal{I}$. Therefore, our derivations of asymptotic distance distributions are applicable to all methods that compute neighbors in order to weight features. The predictors used by NPDR, however, are the one-dimensional projected distances between two instances $i,j \in \mathcal{I}$ (Eq.~\ref{eq:diff}). Hence, all asymptotic estimates we derive for diff metrics (Eq.~\ref{eq:diff}) are particularly relevant to NPDR. Since the standard distance metric (Eq.~\ref{eq:D}) is a function of the one-dimensional projection (Eq.~\ref{eq:diff}), asymptotic estimates derived for this projection (Eq.~\ref{eq:diff}) are implicitly relevant to older nearest-neighbor distance-based methods like Relief-based algorithms. We proceed in the following section by applying the Classical Central Limit Theorem and the Delta Method to derive the limit distribution of pairwise distances on any data distribution that is induced by the standard distance metric (Eq.~\ref{eq:D}).

\subsection{Asymptotic normality of pairwise distances}
%Discuss Central Limit Theorem argument for the distribution being Gaussian. We can always say this is an assumption, but I think we can invoke the CLT. [I would be cautious here - TTL. We will be cautious invoking the CLT - BAM.]

Suppose that 

\noindent $X_{ia}, X_{ja} \overset{iid}{\sim} \mathcal{F}_X\left(\mu_X,\sigma^2_X\right)$ for two fixed and distinct instances $i,j \in \mathcal{I}$ and a fixed attribute $a \in \mathcal{A}$.
$\mathcal{F}_X$ represents any data distribution with mean $\mu_X$ and variance $\sigma^2_X$.

It is clear that $|X_{ia} - X_{ja}|^q = |\text{d}_{ij}(a)|^q$ is another random variable. Let $Z^q_a \sim \mathcal{F}_{Z^q_a}\left(\mu_{z^q_a},\sigma^2_{z^q_a}\right)$ be the random variable such that
%
\begin{equation}\label{eq:diffDistr}
Z^q_a = |\text{d}_{ij}(a)|^q = |X_{ia} - X_{ja}|^q, \quad a \in \mathcal{A}.
\end{equation}

Furthermore, the collection $\{Z^q_a | a \in \mathcal{A}\}$ is a random sample of size $p$ of mutually independent random variables. Hence, the sum of $Z^q_a$ over all $a \in \mathcal{A}$ is asymptotically normal by the Classical Central Limit Theorem (CCLT). More explicitly, this implies that
%
\begin{equation}\label{eq:DqAsympt}
\left(D^{(q)}_{ij}\right)^q = \sum_{a \in \mathcal{A}} |\text{d}_{ij}(a)|^q = \sum_{a \in \mathcal{A}} |X_{ia} - X_{ja}|^q = \sum_{a \in \mathcal{A}} Z^q_a \overset{.}{\sim} \mathcal{N}\left(\mu_{z^q_a}p,\sigma^2_{z^q_a}p\right).
\end{equation}

Consider the smooth function $g(z) = z^{1/q}$ that is continuously differentiable for $z>0$. Assuming that $\mu_{z^q_a}>0$, the Delta Method \cite{allStats} can be applied to show that 

\begin{equation}\label{eq:DqDeltaMethod}
\begin{aligned}
g\left(\left(D^{(q)}_{ij}\right)^q\right) &= g\left(\displaystyle \sum_{a \in \mathcal{A}}^{p}Z^q_a\right) \\
&= \left(\sum_{a \in \mathcal{A}} |X_{ia} - X_{ja}|^q\right)^{1/q} \\
&= D^{(q)}_{ij} \overset{.}{\sim} \mathcal{N}\left(g\left(\mu_{z^q}p\right),\left[g^\prime \left(\mu_{z^q_a}p\right)\right]^2\sigma^2_{z^q_a}p\right) \\
\Rightarrow &\hphantom{=} D^{(q)}_{ij} \overset{.}{\sim} \mathcal{N}\left(\left(\mu_{z^q_a}p\right)^{1/q},\frac{\sigma^2_{z^q_a}p}{q^2\left(\mu_{z^q_a}p\right)^{2\left(1 - \frac{1}{q}\right)}}\right).
\end{aligned}
\end{equation}

Therefore, the distance between two fixed, distinct instances $i$ and $j$ given by Eq. \ref{eq:D} is asymptotically normal.
Specifically, when $q = 2$, the distribution of $D_{ij}^{(2)}$ asymptotically approaches $\mathcal{N}\left(\sqrt{\mu_{z^2_a}p}, \frac{\sigma^2_{z^2_a}}{4\mu_{z^2_a}}\right)$. 
When $p$ is small, however, we observe empirically that a closer estimate of the sample mean is 
% \begin{equation}\label{eq:DqMeanImproved}
% \text{E}\left(D^{(q)}_{ij}\right) = \left(\mu_{z^q}p - \frac{\sigma^2_{z^q}p}{q^2\left(\mu_{z^q}p\right)^{2\left(1 - \frac{1}{q}\right)}}\right)^{1/q}
% \end{equation}
%
\begin{equation}\label{eq:DqImprovedExplained}
\begin{aligned}
\text{E}\left(D^{(2)}_{ij}\right) &= \sqrt{\text{E}\left[\left(D^{(2)}_{ij}\right)^2\right] - \text{Var}\left(D^{(2)}_{ij}\right)} \\
&= \sqrt{\mu_{z^2_a}p - \frac{\sigma^2_{z^2_a}}{4\mu_{z^2_a}}}.
\end{aligned}
\end{equation}
%
We estimate rate of convergence to normality for Euclidean ($q=2$) and Manhattan ($q=1$)  metrics by comparing the distribution of pairwise distances in simulated data to a Gaussian (Fig.~\ref{fig:central_limit_convergence}). We compute the distance between all pairs of instances in simulated datasets of uniformly distributed random data. We simulate data with fixed $m=100$ instances, and, by varying the number of attributes ($p=10,100,10000$), we observe rapid convergence to Gaussian. For $p$ as low as $10$ attributes, Gaussian is a good approximation. The number of attributes in bioinformatics data is typically quite large, at least on the order of $10^3$. The Euclidean metric has stronger convergence to a Gaussian than Manhattan. This may be due to Euclidean's use of the square root, which is a common transformation of data in statistics. Normality was assessed using the Shapiro-Wilk test. 

To show asymptotic normality of distances, we did not specify whether the data distribution $\mathcal{F}_X$ was discrete or continuous. This is because asymptotic normality is a general phenomenon in high attribute dimension $p$ for any data distribution $\mathcal{F}_X$ satisfying the assumptions we have made. Therefore, the simulated distances we have shown (Fig.~\ref{fig:central_limit_convergence}) has an analogous representation for discrete data, as well as all other continuous data distributions.
%
%The null hypothesis of this test is that the distribution is normally distributed. In each case, the $W$-statistics is approximately equal to 1. In the case of the Manhattan metric, convergence does not occur as rapidly as Euclidean. The $W$-statistic is significant at the 0.05 level for both $p=10$ and $p=100$ attributes, which would seem to indicate that there is sufficient evidence to conclude that the distribution is not normal. However, it is still safe to assume normality for most purposes despite the significant P values. In certain circumstances it may be better to use the Euclidean metric due to the apparently increased rate of convergence.
%
\begin{figure}[H]
	\centering
	\includegraphics[width=0.98\textwidth]{central_limit_hist_uniform-data.pdf}
	\caption{Convergence to Gaussian for Manhattan and Euclidean distances for simulated standard uniform data with $m=100$ instances and $p=10, 100,$ and $10000$ attributes. Convergence to Gaussian occurs rapidly with increasing $p$, and Gaussian is a good approximation for $p$ as low as $10$ attributes. The number of attributes in bioinformatics data is typically much larger, at least on the order of $10^3$. The Euclidean metric has stronger convergence to normal than Manhattan.  P values from Shapiro-Wilk test, where the null hypothesis is a Gaussian distribution.}
	\label{fig:central_limit_convergence}
\end{figure}

%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.98\textwidth]{central_limit_hist_normal-data.pdf}
%	\caption{Convergence to Gaussian for Manhattan and Euclidean distances for simulated standard normal data with $m=100$ instances and $p=10, 100,$ and $10000$ attributes. Convergence to Gaussian occurs rapidly with increasing $p$, and Gaussian is a good approximation for $p$ as low as $10$ attributes. The number of attributes in bioinformatics data is typically much larger, at least on the order of $10^3$. The Euclidean metric has stronger convergence to normal than Manhattan.  P values from Shapiro-Wilk test, where the null hypothesis is a Gaussian distribution.}
%	\label{fig:central_limit_convergence_normal}
%\end{figure}

%Although some P values are significant at the 0.05 level for Manhattan ($q=1$), a visual inspection of the corresponding QQ-plots shown in Fig.~\ref{fig:qq-plots} indicate the normality assumption holds reasonably well.

%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.98\textwidth]{qq-plots.pdf}
%	\caption{QQ-plots corresponding to the simulated distances in Fig.~\ref{fig:central_limit_convergence}. Although there are significant P values for the case of Manhattan for $p=10,100$, it is clear that the assumption of normality is safe due to strong relationship between sample and theoretical quantiles.}\label{fig:qq-plots}
%\end{figure}

%\begin{figure}[ht!]
%\centering
%		\framebox{\includegraphics[width=0.95\textwidth]{central_limit_distances-Manhattan-Normal-diffstar.pdf}}
%		\caption{Convergence to normality of Manhattan distances between iid random normal instances. For each simulated distance distribution, we fixed $m=100$ instances but varied $p$ from 10 to 10000. It is clear that convergence is rapid, and approximate normality can be safely assumed for even $p=10$.}\label{fig:manhattanConverge}
%\end{figure} 

%\begin{figure}[ht!]
%\centering
%		\framebox{\includegraphics[width=0.95\textwidth]{central_limit_distances-Euclidean-Normal-diffstar.pdf}}
%		\caption{Convergence to normality of Euclidean distances between iid random normal instances. For each simulated distance distribution, we fixed $m=100$ instances but varied $p$ from 10 to 10000. It is clear that convergence is rapid, and approximate normality can be safely assumed for even $p=10$.}\label{fig:euclideanConverge}
%\end{figure}

For distance based learning methods, all pairwise distances are used to determine relative importances for attributes. The collection of all distances above the diagonal in an $m \times m$ distance matrix does not satisfy the independence assumption used in the previous derivations. This is because of the redundancy that is inherent to the distance matrix calculation. However, this collection is still asymptotically normal with mean and variance approximately equal to those we have previously given (Eq.~\ref{eq:DqDeltaMethod}). In the next section, we assume actual data distributions in order to define more specific general formulas for standard $L_q$ and max-min normalized $L_q$ metrics. We also derive asymptotic moments for a new discrete metric in GWAS data and a new metric for time series correlation-based data, such as, resting-state fMRI.

%Hence, all fixed-radius methods will use a fixed radius that is some fraction of the expected pairwise distance for a given metric and data type. This implies that the probability of a fixed instance $j$ being within a fixed radius of a given instance $i$ can be parameterized by the expected pairwise distance and the variance of the pairwise distance. This probability is obtained by evaluating the normal cumulative distribution function (CDF), with corresponding mean and variance, at the quantile given by some function of the fixed radius. Therefore, we can derive the expected number of neighbors in the neighborhood of a fixed instance $i$. In other words, for sufficiently large data sets, the sample mean of the number of neighbors in a given neighborhood is well approximated by the product between the total number of possible neighbors and the expected probability of an instance being in a given neighborhood. The total number of possible neighbors for a fixed instance $i$ is always $m-1$, but this becomes approximately $\lfloor{\frac{m - 1}{2}}\rfloor$ when delineating between possible hits and misses for balanced data.

\section{\texorpdfstring{$L_q$}{} metric moments for continuous data distributions}\label{sec:moment_derivations}

In this section, we begin by deriving general formulas for asymptotic means and variances of the $L_q$ distance (Eq.~\ref{eq:D}) for standard normal and standard uniform data. With our general formulas for continuous data, we compute moments associated with Manhattan ($L_1$) and Euclidean ($L_2$) metrics. We then consider the max-min normalized version of the $L_q$ distance, where the magnitude difference (Eq.~\ref{eq:diff}) is divided by the range of each feature $a$. Using Extreme Value Theory (EVT), we derive formulas for the moments of feature range in standard normal and standard uniform data. Transitioning into discrete data distributions relevant to GWAS, we derive asymptotic moments for two well known metrics and one new metric. In addition, we derive distance asymptotics for time series correlation-based data, such as, resting-state fMRI. 

\subsection{Distribution of \texorpdfstring{$|\text{d}_{ij}(a)|^q = |X_{ia} - X_{ja}|^q$}{}}

Suppose that $X_{ia}, X_{ja} \overset{iid}{\sim} \mathcal{F}_X(\mu_x,\sigma^2_x)$ and define $Z^q_a = |\text{d}_{ij}(a)|^q = |X_{ia} - X_{ja}|^q$, where $a \in \mathcal{A}$ and $|\mathcal{A}| = p$. In order to find the distribution of $Z^q_a$, we will use the following theorem given in \cite{freund2004}.

\begin{theorem}\label{thm:freund}
	Let $f(x)$ be the value of the probability density of the continuous random variable $X$ at $x$. If the function given by $y = u(x)$ is differentiable and either increasing or decreasing for all values within the range of $X$ for which $f(x) \neq 0$, then, for these values of $x$, the equation $y = u(x)$ can be uniquely solved for $x$ to give $x = w(y)$, and for the corresponding values of $y$ the probability density of $Y = u(X)$ is given by
	
	\[g(y) = f[w(y)] \cdot |w^\prime(y)| \quad \text{ provided } u^\prime(x) \neq 0\]
	
	\noindent Elsewhere, $g(y) = 0$.
\end{theorem}

We have the following cases that result from solving for $X_{ja}$ in the equation given by $Z^q_a = |X_{ia} - X_{ja}|^q$:
\begin{itemize}
	\item[(i)] Suppose that $X_{ja} = X_{ia} - \left(Z^q_a\right)^{1/q}$. Based on the iid assumption for $X_{ia}$ and $X_{ja}$, it follows from Thm. \ref{thm:freund} that the joint density function $g^{(1)}$ of $X_{ia}$ and $Z^q_a$ is given by
	%
	\begin{equation}
	\begin{aligned}
	g^{(1)}(x_{ia},z_a) &= f_X(x_{ia},x_{ja})\biggl|\frac{\partial x_{ja}}{\partial z_a}\biggr| \\
	&= f_X(x_{ia})f_X(x_{ja})\biggl|\frac{-1}{q} \left(z^q_a\right)^{\frac{1}{q}-1}\biggr| \\
	&= \frac{1}{q \left(z^q_a\right)^{1 - \frac{1}{q}}}f_X(x_{ia})f_X\left(x_{ia}-\left(z^q_a\right)^{1/q}\right), \quad z_a > 0
	\end{aligned}
	\end{equation}
	
	The density function $f^{(1)}_{Z^q_a}$ of $Z^q_a$ is then defined as
	%
	\begin{equation}
	\begin{aligned}
	f^{(1)}_{Z^q_a}(z^q_a) &= \int_{-\infty}^{\infty} g^{(1)}(x_{ia},z^q_a)\text{d}x_{ia} \\
	&= \frac{1}{q \left(z^q_a\right)^{1 - \frac{1}{q}}}\int_{-\infty}^{\infty} f_X(x_{ia})f_X\left(x_{ia}-\left(z^q_a\right)^{1/q}\right)\text{d}x_{ia}, \quad z_a > 0.
	\end{aligned}
	\end{equation}
	
	\item[(ii)] Suppose that $X_{ja} = X_{ia} + \left(Z^q_a\right)^{1/q}$. Based on the iid assumption for $X_{ia}$ and $X_{ja}$, it follows from Thm. \ref{thm:freund} that the joint density function $g^{(2)}$ of $X_{ia}$ and $Z_a$ is given by
	%
	\begin{equation}
	\begin{aligned}
	g^{(2)}(x_{ia},z_a) &= f_X(x_{ia},x_{ja})\biggl|\frac{\partial x_{ja}}{\partial z_a}\biggr| \\
	&= f_X(x_{ia})f_X(x_{ja})\biggl|\frac{1}{q} \left(z^q_a\right)^{\frac{1}{q}-1}\biggr| \\
	&= \frac{1}{q \left(z^q_a\right)^{1 - \frac{1}{q}}}f_X(x_{ia})f_X\left(x_{ia}-\left(z^q_a\right)^{1/q}\right), \quad z_a > 0.
	\end{aligned}
	\end{equation}
	
	The density function $f^{(2)}_{Z^q_a}$ of $Z^q_a$ is then defined as
	%
	\begin{equation}
	\begin{aligned}
	f^{(2)}_{Z^q_a}(z^q_a) &= \int_{-\infty}^{\infty} g^{(2)}(x_{ia},z^q_a)\text{d}x_{ia} \\
	&= \frac{1}{q \left(z^q_a\right)^{1 - \frac{1}{q}}}\int_{-\infty}^{\infty} f_X(x_{ia})f_X\left(x_{ia}+\left(z^q_a\right)^{1/q}\right)\text{d}x_{ia}, \quad z_a > 0.
	\end{aligned}
	\end{equation}
\end{itemize}

Let $F_{Z^q_a}$ denote the distribution function of the random variable $Z^q_a$. Furthermore, we define the events $E^{(1)}$ and $E^{(2)}$ as
%
\begin{equation}\label{eq:E(1)}
E^{(1)} = \bigl\{|X_{ia}-X_{ja}|^q \leq z^q_a : X_{ja} = X_{ia} - \left(Z^q_a\right)^{1/q}\bigr\}
\end{equation}
%
and
%
\begin{equation}\label{eq:E(2)}
E^{(2)} = \bigl\{|X_{ia}-X_{ja}|^q \leq z^q_a : X_{ja} = X_{ia} + \left(Z^q_a\right)^{1/q}\bigr\}.
\end{equation}

Then it follows from fundamental rules of probability that
%
\begin{equation}\label{eq:DqCDF}
\begin{aligned}
F_{Z^q_a}(z^q_a) &= \text{P}\left[Z^q_a \leq z^q_a\right] \\
&= \text{P}\left[|X_{ia} - X_{ja}|^q \leq z^q_a\right] \\
&= \text{P}\left[E^{(1)} \cup E^{(2)}\right] \\
&= \text{P}\bigl[E^{(1)}\bigr] + \text{P}\bigl[E^{(2)}\bigr] - \text{P}\bigl[E^{(1)} \cap E^{(2)}\bigr] \\
&= \text{P}\bigl[E^{(1)}\bigr] + \text{P}\bigl[E^{(2)}\bigr] \\
&= \int_{-\infty}^{z^q_a} f^{(1)}_{Z^q_a}(t) \text{d}t + \int_{-\infty}^{z^q_a} f^{(2)}_{Z^q_a}(t) \text{d}t \\
&= \int_{-\infty}^{z^q_a} \left(f^{(1)}_{Z^q_a}(t) + f^{(2)}_{Z^q_a}(t)\right) \text{d}t \\
&= \frac{1}{q \left(z^q_a\right)^{1 - \frac{1}{q}}} \int_{-\infty}^{z^q_a} {\Biggl \{}{\Biggl(}\int_{-\infty}^{\infty}f_X(x_{ia}){\bigl[}f_X(x_{ia} - t) \\
&\hspace{4.5cm}+ f_X(x_{ia} + t){\bigr]} \text{d}x_{ia}{\Biggr)}{\Biggr\}}\text{d}t,
\end{aligned}
\end{equation}
%
where $z_a > 0$.

It follows directly from the previous result (Eq.~\ref{eq:DqCDF}) that the density function of the random variable $Z^q_a$ is given by
%
\begin{equation}\label{eq:DqPDF}
\begin{aligned}
f_{Z^q_a}(z^q_a) &= \frac{\partial}{\partial z^q_a} F_{Z^q_a}(z^q_a) \\
&= \frac{1}{q \left(z^q_a\right)^{1 - \frac{1}{q}}}\int_{-\infty}^{\infty} {\Biggl\{}f_X(x_{ia}){\biggl[}f_X\left(x_{ia} - \left(z^q_a\right)^{1/q}\right) \\
&\hspace{4.5cm}+ f_X\left(x_{ia} + \left(z^q_a\right)^{1/q}\right){\biggr]}{\Biggr\}} \text{d}x_{ia},
\end{aligned}
\end{equation}
%
where $z_a > 0$.

Using the previous result (Eq.~\ref{eq:DqPDF}), we can compute the mean and variance of the random variable $Z^q_a$ as
%
\begin{equation}\label{eq:1DDqMean}
\mu_{z^q_a} = \int_{-\infty}^{\infty} z^q_a f_{Z^q_a}(z^q_a) \text{d}z^q_a
\end{equation}
%
and 
%
\begin{equation}\label{eq:1DDqVar}
\sigma^2_{z^q_a} = \int_{-\infty}^{\infty} \left(z^q_a\right)^2 f_{Z^q_a}(z^q_a) \text{d}z^q_a - \mu^2_{z^q_a}.
\end{equation}

It follows immediately from the mean (Eq.~\ref{eq:1DDqMean}) and variance (Eq.~\ref{eq:1DDqVar}) and the Classical Central Limit Theorem (CCLT) that
%
\begin{equation}\label{eq:DqDistr}
\left(D^{(q)}_{ij}\right)^q = \sum_{a \in \mathcal{A}} Z^q_a = \sum_{a \in \mathcal{A}} |X_{ia} - X_{ja}|^q \overset{.}{\sim} \mathcal{N}\left(\mu_{z^q}p,\sigma^2_{z^q}p\right).
\end{equation}

Applying the convergence result we derived previously (Eq.~\ref{eq:DqDeltaMethod}), the distribution of $D^{(q)}_{ij}$ is given by
%
\begin{equation}\label{eq:DDistr}
D^{(q)}_{ij} \overset{.}{\sim} \mathcal{N}\left(\left(\mu_{z^q_a}p\right)^{1/q},\frac{\sigma^2_{z^q_a}p}{q^2\left(\mu_{z^q_a}p\right)^{2\left(1 - \frac{1}{q}\right)}}\right), \quad \mu_{z^q_a} > 0,
\end{equation}
%
where we have an improved estimate of the mean for $q=2$ (Eq.~\ref{eq:DqImprovedExplained}).

\subsubsection{Standard normal data}

If $X_{ia},X_{ja} \overset{iid}{\sim} \mathcal{N}(0,1)$, then the marginal density functions with respect to $X$ for $X_{ia}$, $X_{ia} - \left(Z^q_a\right)^{1/q}$, and $X_{ia} + \left(Z^q_a\right)^{1/q}$ are defined as
%
\begin{equation}\label{eq:normalXmarg}
f_X(x_{ia}) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2_{ia}},
\end{equation}
%
\begin{equation}\label{eq:normalXMinusZmarg}
f_X\left(x_{ia} - \left(z^q_a\right)^{1/q}\right) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}\left(x_{ia} - \left(z^q_a\right)^{1/q}\right)^2}, \quad z_a > 0, \text{ and}
\end{equation}
%
\begin{equation}\label{eq:normalXPlusZmarg}
f_X\left(x_{ia} + \left(z^q_a\right)^{1/q}\right) = \frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}\left(x_{ia} + \left(z^q_a\right)^{1/q}\right)^2}, \quad z_a > 0.
\end{equation}

Substituting these marginal densities (Eqs.~\ref{eq:normalXmarg}-\ref{eq:normalXPlusZmarg}) into the general density function for $Z^q_a$ (Eq.~\ref{eq:DqPDF}) and completing the square on $x_{ia}$ in the exponents, we have
%
\begin{equation}\label{eq:normalPDF}
\begin{aligned}
f_{Z^q_a}(z^q_a) &= \frac{1}{2 q \pi \left(z^q_a\right)^{1 - \frac{1}{q}}} e^{-\frac{1}{4}\left(z^q_a\right)^{2    /q}}\int_{-\infty}^{\infty} \biggl(e^{-\frac{1}{2}\left[\sqrt{2}x_{ia} - \frac{\sqrt{2}}{2}\left(z^q_a\right)^{1/q}\right]^2} \\
&\hspace{2in} + e^{-\frac{1}{2}\left[\sqrt{2}x_{ia} + \frac{\sqrt{2}}{2}\left(z^q_a\right)^{1/q}\right]^2}\biggr) \text{d}x_{ia} \\
&= \frac{1}{2 q \sqrt{\pi} \left(z^q_a\right)^{1 - \frac{1}{q}}} e^{-\frac{1}{4}\left(z^q_a\right)^{2/q}} \int_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}} \left(e^{-\frac{1}{2}u^2} + e^{-\frac{1}{2}u^2}\right) \text{d}u \\
&= \frac{1}{2 q \sqrt{\pi} \left(z^q_a\right)^{1 - \frac{1}{q}}} e^{-\frac{1}{4}\left(z^q_a\right)^{2/q}} (1 + 1) \\
&= \frac{1}{q \sqrt{\pi}}\left(z^q_a\right)^{\frac{1}{q} - 1} e^{-\frac{1}{4}\left(z^q_a\right)^{2/q}} \\
&= \frac{\frac{2}{q}}{\left(2^q\right)^{1/q} \Gamma\left(\frac{\frac{1}{q}}{\frac{2}{q}}\right)}\left(z^q_a\right)^{\frac{1}{q} - 1} e^{-\left(\frac{z^q_a}{2^q}\right)^{2/q}}.
\end{aligned}
\end{equation}

The density function given previously (Eq.~\ref{eq:normalPDF}) is a Generalized Gamma density with parameters $b = \frac{2}{q}$, $c = 2^q$, and $d = \frac{1}{q}$. This distribution has mean and variance given by
%
\begin{equation}\label{eq:1DnormalDqMean}
\begin{aligned}
\mu_{z^q_a} &= \frac{c\Gamma\left(\frac{d+1}{b}\right)}{\Gamma\left(\frac{d}{b}\right)} \\
&= \frac{2^q \Gamma\left(\frac{q + 1}{2}\right)}{\sqrt{\pi}}
\end{aligned}
\end{equation}
%
and
%
\begin{equation}\label{eq:1DnormalDqVar}
\begin{aligned}
\sigma^2_{z^q_a} &= c^2\left[\frac{\Gamma\left(\frac{d+2}{b}\right)}{\Gamma\left(\frac{d}{b}\right)} - \left(\frac{\Gamma\left(\frac{d+1}{b}\right)}{\Gamma\left(\frac{d}{b}\right)}\right)^2\right] \\
&= 4^{q}\left[\frac{\Gamma\left(q + \frac{1}{2}\right)}{\sqrt{\pi}} - \frac{\Gamma^2\left(\frac{1}{2}q + \frac{1}{2}\right)}{\pi}\right].
\end{aligned}
\end{equation}

By linearity of the expected value and variance operators under the iid assumption, the mean (Eq.~\ref{eq:1DnormalDqMean}) and variance (Eq.~\ref{eq:1DnormalDqVar}) of the random variable $Z^q_a$ allow the $p\text{-dimensional}$ mean and variance of the $\left(D^{(q)}_{ij}\right)^q$ distribution to be computed directly as
%
\begin{equation}\label{eq:normalDqMean}
\begin{aligned}
\mu_{\left(D^{(q)}_{ij}\right)^q} = \text{E}\left[\left(D^{(q)}_{ij}\right)^q\right] = \text{E}\left(\sum_{a \in \mathcal{A}} Z^q_a\right) = \sum_{a \in \mathcal{A}} \text{E}\left(Z^q_a\right) &= \sum_{a \in \mathcal{A}} \frac{2^q \Gamma\left(\frac{q + 1}{2}\right)}{\sqrt{\pi}} \\
&=  \frac{2^q\Gamma\left(\frac{q + 1}{2}\right)}{\sqrt{\pi}}p
\end{aligned}
\end{equation}
%
and
%
\begin{equation}\label{eq:normalVar}
\begin{split}
\sigma^2_{\left(D^{(q)}_{ij}\right)^q} = \text{Var}\left[\left(D^{(q)}_{ij}\right)^q\right] &= \text{Var}\left(\sum_{a \in \mathcal{A}} Z^q_a\right) \\
&= \sum_{a \in \mathcal{A}} \text{Var}\left(Z^q_a\right) \\
&= \sum_{a \in \mathcal{A}} 4^{q}\left[\frac{\Gamma\left(q + \frac{1}{2}\right)}{\sqrt{\pi}} - \frac{\Gamma^2\left(\frac{1}{2}q + \frac{1}{2}\right)}{\pi}\right] \\
&= 4^{q}\left[\frac{\Gamma\left(q + \frac{1}{2}\right)}{\sqrt{\pi}} - \frac{\Gamma^2\left(\frac{1}{2}q + \frac{1}{2}\right)}{\pi}\right]p.
\end{split}
\end{equation}

Therefore, the asymptotic distribution of $D^{(q)}_{ij}$ for standard normal data is
%
\begin{equation}\label{eq:normalDistr}
\begin{aligned}
\mathcal{N}{\fontsize{0.7cm}{1pt}\selectfont \Biggl(}&\left(2^q\frac{\Gamma\left(\frac{q + 1}{2}\right)}{\sqrt{\pi}}p\right)^{1/q}, \\
&\frac{4^q p}{q^2 \left(\frac{2^q \Gamma\left(\frac{1}{2}q + \frac{1}{2}\right)}{\sqrt{\pi}}p\right)^{2\left(1 - \frac{1}{q}\right)}}\left[\frac{\Gamma\left(q + \frac{1}{2}\right)}{\sqrt{\pi}} - \frac{\Gamma^2\left(\frac{1}{2}q + \frac{1}{2}\right)}{\pi}\right]{\fontsize{0.7cm}{1pt}\selectfont \Biggr)}.
\end{aligned}
\end{equation}

We provide a summary table of the moment estimates (Eq.~\ref{eq:normalDistr}) for the $L_q$ metric on standard normal data (Table~\ref{tab:dist_distr_general1}). The summary is organized by data type, type of statistic (mean or variance), and corresponding asymptotic formula. In the next section, we derive the $L_q$ distance distribution on standard uniform data in a similar fashion.

\subsubsection{Standard uniform data}

If $X_{ia},X_{ja} \overset{iid}{\sim} \mathcal{U}(0,1)$, then the marginal density functions with respect to $X$ for $X_{ia}$, $X_{ia} - \left(Z^q_a\right)^{1/q}$, and $X_{ia} + \left(Z^q_a\right)^{1/q}$ are defined as
%
\begin{equation}\label{eq:uniformXmarg}
f_X(x_{ia}) = 1, \quad 0 \leq x_{ia} \leq 1
\end{equation}
%
\begin{equation}\label{eq:uniformXMinusZmarg}
f_X\left(x_{ia} - \left(z^q_a\right)^{1/q}\right) = 1, \quad 0 \leq x_{ia} - \left(z^q_a\right)^{1/q} \leq 1, \text{ and}
\end{equation}
%
\begin{equation}\label{eq:uniformXPlusZmarg}
f_X\left(x_{ia} + \left(z^q_a\right)^{1/q}\right) = 1, \quad 0 \leq x_{ia} + \left(z^q_a\right)^{1/q} \leq 1.
\end{equation}

Substituting these marginal densities (Eqs.~\ref{eq:uniformXmarg}-\ref{eq:uniformXPlusZmarg}) into the more general density function for $Z^q_a$ (Eq.~\ref{eq:DqPDF}), we have
%
\begin{equation}\label{eq:uniformDqPDF}
\begin{aligned}
f_{Z^q_a}(z^q_a) &= \frac{1}{q\left(z^q_a\right)^{1 - \frac{1}{q}}}\int_{-\infty}^{\infty}{\Biggl\{}f_X(x_{ia}){\biggl[}f_X\left(x_{ia} - \left(z^q_a\right)^{1/q}\right)\\ 
&\hspace{3.7cm}+ f_X\left(x_{ia} + \left(z^q_a\right)^{1/q}\right){\biggr]}{\Biggl\}}\text{d}x_{ia}\\
&= \frac{1}{q\left(z^q_a\right)^{1 - \frac{1}{q}}}\int_{0}^{1}\left[f_X(x_{ia} - \left(z^q_a\right) + f_X\left(x_{ia} + \left(z^q_a\right)^{1/q}\right)\right]\text{d}x_{ia} \\
&= \frac{1}{q\left(z^q_a\right)^{1 - \frac{1}{q}}}\int_{\left(z^q_a\right)}^{1}1\text{d}x_{ia} + \int_{0}^{1 - \left(z^q_a\right)}1\text{d}x_{ia}\\
&= \frac{1}{q\left(z^q_a\right)^{1 - \frac{1}{q}}}\left[\left(1 - \left(z^q_a\right)\right) + \left(1 - \left(z^q_a\right)\right)\right]\\
&= \frac{1}{q} \cdot 2 \left(z^q_a\right)^{\frac{1}{q} - 1}\left[1 - \left(z^q_a\right)^{1/q}\right]^{2 - 1},
\end{aligned}
\end{equation}
%
where $0 < z_a \leq 1$.

The previous density (Eq.~\ref{eq:uniformDqPDF}) is a Kumaraswamy density with parameters $b = \frac{1}{q}$ and $c = 2$ with moment generating function (MGF) given by
%
\begin{equation}\label{eq:uniformDqMGF}
\begin{aligned}
M_n &=  \frac{c\Gamma\left(1 + \frac{n}{b}\right) \Gamma(c)}{\Gamma\left(1 + c + \frac{n}{b}\right)}\\
&= \frac{2}{(nq + 2)(nq + 1)}.
\end{aligned}
\end{equation}

Using this MGF (Eq.~\ref{eq:uniformDqMGF}), the mean and variance of $Z^q_a$ are computed as
%
\begin{equation}\label{eq:1DuniformDqMean}
\mu_{z^q_a} = \frac{2}{(q + 2)(q + 1)}
\end{equation}
%
and
%
\begin{equation}\label{eq:1DuniformDqVar}
\sigma^2_{z^q_a} = \frac{1}{(q + 1)(2q + 1)} - \left(\frac{2}{(q + 2)(q + 1)}\right)^2.
\end{equation}

By linearity of the expected value and variance operators under the iid assumption, the mean (Eq.~\ref{eq:1DuniformDqMean}) and variance (Eq.~\ref{eq:1DuniformDqVar}) of the random variable $Z^q_a$ allow the $p \text{-dimensional}$ mean and variance of the $\left(D^{(q)}_{ij}\right)^q$ distribution to be computed directly as
%
\begin{equation}\label{eq:uniformDqMean}
\begin{split}
\mu_{\left(D^{(q)}_{ij}\right)^q} = \text{E}\left[\left(D^{(q)}_{ij}\right)^q\right] &= \text{E}\left(\sum_{a \in \mathcal{A}}Z^q_a\right) \\
&= \sum_{a \in \mathcal{A}} \text{E}(Z^q_a) \\
&= \sum_{a \in \mathcal{A}} \frac{2}{(q + 2)(q + 1)} \\
&= \frac{2p}{(q + 2)(q + 1)}
\end{split}
\end{equation}
%
and
%
\begin{equation}\label{eq:uniformDqVar}
\begin{split}
\sigma^2_{\left(D^{(q)}_{ij}\right)^q} = \text{Var}\left[\left(D^{(q)}_{ij}\right)^q\right] &= \text{Var}\left(\sum_{a \in \mathcal{A}} Z^q_a\right) \\
&= \sum_{a \in \mathcal{A}} \text{Var}\left(Z^q_a\right) \\
&= \sum_{a \in \mathcal{A}} \left[\frac{1}{(q + 1)(2q + 1)} - \left(\frac{2}{(q + 2)(q + 1)}\right)^2\right] \\
&= \left[\frac{1}{(q + 1)(2q + 1)} - \left(\frac{2}{(q + 2)(q + 1)}\right)^2\right]p.
\end{split}
\end{equation}

Therefore, the asymptotic distribution of $D^{(q)}_{ij}$ for standard uniform data is
%
\begin{equation}\label{eq:uniformDistr}
\begin{split}
\mathcal{N}{\text{\LARGE $\Biggl($}}& \left(\frac{2p}{(q + 2)(q + 1)}\right)^{1/q}, \\
& \frac{p}{q^2\left(\frac{2p}{(q + 2)(q + 1)}\right)^{2\left(1 - \frac{1}{q}\right)}}\left[\frac{1}{(q + 1)(2q + 1)} - \left(\frac{2}{(q + 2)(q + 1)}\right)^2\right]{\text{\LARGE $\Biggr)$}}.
\end{split}
\end{equation}

We provide a summary table of the moment estimates (Eq.~\ref{eq:uniformDistr}) for the $L_q$ metric on standard uniform data (Table~\ref{tab:dist_distr_general1}). The summary is organized by data type, type of statistic (mean or variance), and corresponding asymptotic formula. In the next section, we use our general $L_q$ distance distribution derivations to provide Manhattan ($q=1$) and Euclidean ($q=2$) asymptotic moments on both standard normal and standard uniform data. These are the most commonly applied metrics in the context of nearest-neighbor feature selection, so they are of particular interest.

\subsection{Manhattan \texorpdfstring{($L_1$)}{}}

With our general formulas for the asymptotic 

\noindent mean and variance (Eqs.~\ref{eq:normalDistr} and \ref{eq:uniformDistr}) for any value of $q \in \mathbb{N}$, we can simply substitute a particular value of $q$ in order to determine the asymptotic distribution of the corresponding distance $L_q$ metric. We demonstrate this with the example of the Manhattan metric ($L_1$) for standard normal and standard uniform data (Eq.~\ref{eq:D}, $q=1$).

\subsubsection{Standard normal data}

Substituting $q=1$ into the asymptotic formula for the mean $L_q$ distance (Eq.~\ref{eq:normalDistr}), we have the following for expected $L_1$ distance between two independently sample instances $i,j \in \mathcal{I}$ in standard normal data
%
\begin{equation}\label{eq:normalManMean}
\begin{aligned}
\text{E}\left(D^{(1)}_{ij}\right) &= \left(2\frac{\Gamma\left(\frac{1 + 1}{2}\right)}{\sqrt{\pi}}p\right)^{1/1} \\
&= \frac{2p}{\sqrt{\pi}}.
\end{aligned}
\end{equation}

We see in the formula for the expected Manhattan distance (Eq.~\ref{eq:normalManMean}) that $D^{(1)}_{ij} \sim p$ in the limit, which implies that this distance is unbounded as feature dimension $p$ increases.

Substituting $q=1$ into the formula for the asymptotic variance of $D^{(1)}_{ij}$ (Eq.~\ref{eq:normalDistr}) leads to the following
%
\begin{equation}\label{eq:normalManVar}
\begin{aligned}
\text{Var}\left(D^{(1)}_{ij}\right) &= \frac{4^1p}{1^2\left(\frac{2^1\Gamma\left(\frac{1}{2}(1) + \frac{1}{2}\right)}{\sqrt{\pi}}p\right)^{2\left(1 - \frac{1}{1}\right)}}\left[\frac{\Gamma\left(1 + \frac{1}{2}\right)}{\sqrt{\pi}} - \frac{\Gamma^2\left(\frac{1}{2}(1) + \frac{1}{2}\right)}{\pi}\right] \\
&= \frac{2(\pi - 2)p}{\pi}.
\end{aligned}
\end{equation}

Similar to the mean (Eq.~\ref{eq:normalManMean}), the limiting variance of $D^{(1)}_{ij}$ (Eq.~\ref{eq:normalManVar}) grows on the order of feature dimension $p$, which implies that points become more dispersed as the dimension increases. The moment estimates given in this section (Eqs.~\ref{eq:normalManMean} and \ref{eq:normalManVar}) are summarized in a table that is organized by metric, data type, statistic (mean or variance), and asymptotic formula (Table~\ref{tab:dist_distr_standardL1L2}).

\subsubsection{Standard uniform data}

Substituting $q=1$ into the asymptotic formula of the mean (Eq.~\ref{eq:uniformDistr}), we have the following for the expected $L_1$ distance between two independently sampled instances $i,j \in \mathcal{I}$ in standard uniform data
%
\begin{equation}\label{eq:uniformManMean}
\begin{aligned}
\text{E}\left(D^{(1)}_{ij}\right) &= \left(\frac{2p}{(1+2)(1+1)}\right)^{1/1} \\
&= \frac{p}{3}.
\end{aligned}
\end{equation}

Once again, we see that the mean of $D^{(1)}_{ij}$ (Eq.~\ref{eq:uniformManMean}) grows on the order of $p$ just as in the case of standard normal data.

Substituting $q=1$ into the formula of the asymptotic variance of $D^{(1)}_{ij}$ (Eq.~\ref{eq:uniformDistr}) leads to the following
%
\begin{equation}\label{eq:uniformManVar}
\begin{aligned}
\text{Var}\left(D^{(1)}_{ij}\right) &= \frac{p}{1^2\left(\frac{2p}{(1 + 2)(1 + 1)}\right)^{2\left(1 - \frac{1}{1}\right)}} {\fontsize{0.5cm}{1pt}\selectfont \Biggr\{}{\Biggl[}\frac{1}{(1 + 1)(2(1) + 1)}\\
&\hspace{4.5cm}- \left(\frac{2}{(1 + 2)(1 + 1)}\right)^2{\Biggr]}{\fontsize{0.5cm}{1pt}\selectfont \Biggr\}} \\
&= \frac{p}{18}.
\end{aligned}
\end{equation}

As in the case of the $L_1$ metric on standard normal data, we have a variance (Eq.~\ref{eq:uniformManVar}) that grows on the order of $p$. The distances between points in high-dimensional uniform data become more widely dispersed with this metric. The moment estimates given in this section (Eqs.~\ref{eq:uniformManMean} and \ref{eq:uniformManVar}) are summarized in a table that is organized by metric, data type, statistic (mean or variance), and asymptotic formula (Table~\ref{tab:dist_distr_standardL1L2}).

\subsubsection{Distribution of one-dimensional projected pairwise distance onto an attribute}\label{sec:continuous_diff}

In nearest-neighbor distance-based feature selection like NPDR and Relief-based algorithms, the one-dimensional projection of the pairwise distance onto an attribute (Eq.~\ref{eq:diff}) is particularly fundamental to feature quality for association with an outcome. For instance, this distance projection is the predictor used to determine beta coefficients in NPDR. In particular, understanding distributional properties of the projected distances is necessary for defining pseudo P values for NPDR. In this section, we summarize the exact distribution of the one-dimensional projected distance onto an attribute $a \in \mathcal{A}$. These results apply to continuous data, such as gene expression. 

In previous sections, we derived the exact density function (Eq.~\ref{eq:DqPDF}) and moments (Eqs.~\ref{eq:1DDqMean} and \ref{eq:1DDqVar}) for the distribution of $Z^q_a=|X_{ia}-X_{ja}|^q$. We then derived the exact density (Eq.~\ref{eq:normalPDF}) and moments (Eqs.~\ref{eq:1DnormalDqMean} and \ref{eq:1DnormalDqVar}) for standard normal data. Analogously, we formulated the exact density (Eq.~\ref{eq:uniformDqPDF}) and moments (Eqs.~\ref{eq:1DuniformDqMean} and \ref{eq:1DuniformDqVar}) for standard uniform data. From these exact densities and moments, we simply substitute $q=1$ to define the distribution of the one-dimensional projected distance onto an attribute $a \in \mathcal{A}$.

Assuming data is standard normal, we substitute $q=1$ into the density function of $Z^q_a$ (Eq.~\ref{eq:normalPDF}) to arrive at the following density function
%
\begin{equation}\label{eq:projected_distance_density_normal1}
\begin{aligned}
f_{Z^1_a}\left(z^1_a\right) &= \frac{\frac{2}{1}}{\left(2^1\right)^{1/1}\Gamma\left(\frac{\frac{1}{1}}{\frac{2}{1}}\right)}\left(z^1_a\right)^{1/1 - 1} e^{-\left(\frac{z^1_a}{2^1}\right)^{2/1}}, \quad z_a > 0\\
&= \frac{1}{\sqrt{\pi}} z_a e^{-\frac{1}{4}z^2_a}, \quad z_a > 0.
\end{aligned}
\end{equation}

The mean corresponding to this Generalized Gamma density is computed by substituting $q=1$ into the formula for the mean of $Z^q_a$ (Eq.~\ref{eq:1DnormalDqMean}). This result is given by
%
\begin{equation}\label{eq:1DnormalD1Mean}
\begin{aligned}
\mu_{Z^1_a} &= \frac{2^1 \Gamma\left(\frac{1 + 1}{2}\right)}{\sqrt{\pi}} \\
&= \frac{2}{\sqrt{\pi}}.
\end{aligned}
\end{equation}

Substituting $q=1$ into Eq.~\ref{eq:1DnormalDqVar} for the variance, we have the following
%
\begin{equation}\label{eq:1DnormalD1Var}
\begin{aligned}
\sigma^2_{Z^1_a} &= 4^1\left[\frac{\Gamma\left(1 + \frac{1}{2}\right)}{\sqrt{\pi}} - \frac{\Gamma^2\left(\frac{1}{2}\cdot 1 + \frac{1}{2}\right)}{\pi}\right] \\
&= \frac{2(\pi-2)}{\pi}.
\end{aligned}
\end{equation}

These last few results (Eqs.~\ref{eq:projected_distance_density_normal1}-\ref{eq:1DnormalD1Var}) provide us with the distribution for NPDR predictors when the data is from the standard normal distribution. 

If we have standard uniform data, we substitute $q=1$ into the density function of $Z^q_a$ (Eq.~\ref{eq:uniformDqPDF}) to obtain the following density function
%
\begin{equation}\label{eq:projected_distance_density_uniform1}
\begin{aligned}
f_{Z^1_a} &= \frac{1}{1}\cdot 2\left(z^1_a\right)^{1/1 - 1}\left[1 - \left(z^1_a\right)^{1/1}\right]^{2-1}, \quad 0 < z_a \leq 1 \\
&= 2z_a(1 - z_a), \quad 0 < z_a \leq 1.
\end{aligned}
\end{equation}

The mean corresponding to this Kumaraswamy density is computed by substituting $q=1$ into the formula for the mean of $Z^q_a$ (Eq.~\ref{eq:1DuniformDqMean}). After substitution, we have the following result
%
\begin{equation}\label{eq:1DuniformD1Mean}
\begin{aligned}
\mu_{Z^1_a} &= \frac{2}{(1 + 2)(1 + 1)} \\
&= \frac{1}{3}.
\end{aligned}
\end{equation}

Substituting $q=1$ into the formula for the variance of $Z^q_a$ (Eq.~\ref{eq:1DuniformDqVar}), we have the following
%
\begin{equation}\label{eq:1DuniformD1Var}
\begin{aligned}
\sigma^2_{Z^1_a} &= \frac{1}{(1 + 1)(2\cdot 1 + 1)} - \left(\frac{2}{(1 + 2)(1 + 1)}\right)^2 \\
&= \frac{1}{18}.
\end{aligned}
\end{equation}

In the event that the data distribution is standard uniform, the density function (Eq.~\ref{eq:projected_distance_density_uniform1}), the mean (Eq.~\ref{eq:1DuniformD1Mean}, and the variance (Eq.~\ref{eq:1DuniformD1Var}) sufficiently define the distribution for NPDR predictors. The means (Eqs.~\ref{eq:1DnormalD1Mean} and \ref{eq:1DuniformD1Mean}) and variances (Eqs.\ref{eq:1DnormalD1Var} and \ref{eq:1DuniformD1Var}) come from the exact distribution of pairwise distances with respect to a single attribute $a \in \mathcal{A}$. This is the distribution of the so-called ``projection" of the pairwise distance onto a single attribute to which we have been referring, which is a direct implication from our more general derivations. In a similar manner, one can substitute any value of $q \geq 2$ into the general densities of $Z^q_a$ for standard normal (Eq.~\ref{eq:normalPDF}) and standard uniform (Eq.~\ref{eq:uniformDqPDF}) to derive the associated density of $Z^q_a = |X_{ia} - X_{ja}|^q$ for the given data type.

\subsection{Euclidean \texorpdfstring{($L_2$)}{}}

Moment estimates for the Euclidean metric are obtained by substituting $q=2$ into the asymptotic moment formulas for standard normal data (Eq.~\ref{eq:normalDistr}) and standard uniform data (Eq.~\ref{eq:uniformDistr}). As in the case of the Manhattan metric in the previous sections, we initially proceed by deriving Euclidean distance moments in standard normal data.

\subsubsection{Standard normal data}

Substituting $q=2$ into the asymptotic formula of the mean (Eq. \ref{eq:normalDistr}), we have the following for expected $L_2$ distance between two independently sampled instances $i,j \in \mathcal{I}$ in standard normal data
%
\begin{equation}\label{eq:normalEucMean}
\begin{aligned}
\text{E}\left(D^{(2)}_{ij}\right) &= \left(2\frac{\Gamma\left(\frac{2 + 1}{2}\right)}{\sqrt{\pi}}p\right)^{1/2} \\
&= \sqrt{2p}.
\end{aligned}
\end{equation}

In the case of $L_2$ on standard normal data, we see that the mean of $D^{(2)}_{ij}$ (Eq.~\ref{eq:normalEucMean}) grows on the order of $\sqrt{p}$. Hence, the Euclidean distance does not increase as quickly as the Manhattan distance on standard normal data.

Substituting $q=2$ into the formula for the asymptotic variance of $D^{(2)}_{ij}$ (Eq.~\ref{eq:normalDistr}) leads to the following
%
\begin{equation}\label{eq:normalEucVar}
\begin{aligned}
\text{Var}\left(D^{(2)}_{ij}\right) &= \frac{4^2p}{2^2\left(\frac{2^2\Gamma\left(\frac{1}{2}(2) + \frac{1}{2}\right)}{\sqrt{\pi}}p\right)^{2\left(1 - \frac{1}{2}\right)}}\left[\frac{\Gamma\left(2 + \frac{1}{2}\right)}{\sqrt{\pi}} - \frac{\Gamma^2\left(\frac{1}{2}(2) + \frac{1}{2}\right)}{\pi}\right] \\
&= 1.
\end{aligned}
\end{equation}

Surprisingly, the asymptotic variance (Eq.~\ref{eq:normalEucVar}) is just 1. Regardless of data dimensions $m$ and $p$, the variance of Euclidean distances on standard normal data tends to 1. Therefore, most instances are contained within a ball of radius 1 about the mean in high feature dimension $p$. This means that the Euclidean distance distribution on standard normal data is simply a horizontal shift to the right of the standard normal distribution.

For the case in which the number of attributes $p$ is small, we have an improved estimate of the mean (Eq.~\ref{eq:DqImprovedExplained}). The lower dimensional estimate of the mean is given by
%
\begin{equation}\label{eq:normalEucMeanImproved}
\begin{aligned}
\text{E}\left(D^{(2)}_{ij}\right) &= \left(2\frac{\Gamma\left(\frac{2 + 1}{2}\right)}{\sqrt{\pi}}p - 1\right)^{1/2} \\
&= \sqrt{2p - 1}.
\end{aligned}
\end{equation}

For high dimensional data sets like gene expression \cite{brazma2000,wang2018}, which typically contain thousands of genes (or features), it is clear that the magnitude of $p$ will be sufficient to use the standard asymptotic estimate (Eq.~\ref{eq:normalEucMean}) since $\sqrt{2p} \approx \sqrt{2p - 1}$ in that case. The moment estimates given in this section (Eqs.~\ref{eq:normalEucMeanImproved} and \ref{eq:normalEucVar}) are summarized in a table that is organized by metric, data type, statistic (mean or variance), and asymptotic formula (Table~\ref{tab:dist_distr_standardL1L2}).

\subsubsection{Standard uniform data}

Substituting $q=2$ into the asymptotic formula of the mean (Eq.~\ref{eq:uniformDistr}), we have the following for expected $L_2$ distance between two independently sampled instances $i,j \in \mathcal{I}$ in standard uniform data
%
\begin{equation}\label{eq:uniformEucMean}
\begin{aligned}
\text{E}\left(D^{(2)}_{ij}\right) &= \left(\frac{2p}{(2+2)(2+1)}\right)^{1/2} \\
&= \sqrt{\frac{p}{6}}.
\end{aligned}
\end{equation}

As in the case of standard normal data, the expected value of $D^{(2)}_{ij}$ (Eq.~\ref{eq:uniformEucMean}) grows on the order of $\sqrt{p}$. 

Substituting $q=2$ into the formula for the asymptotic variance of $D^{(2)}_{ij}$ (Eq.~\ref{eq:uniformDistr}) leads to the following
%
\begin{equation}\label{eq:uniformEucVar}
\begin{aligned}
\text{Var}\left(D^{(2)}_{ij}\right) &= \frac{p}{2^2\left(\frac{2p}{(2 + 2)(2 + 1)}\right)^{2\left(1 - \frac{1}{2}\right)}} {\fontsize{0.5cm}{1pt}\selectfont \Biggl\{}{\Biggl[}\frac{1}{(2 + 1)(2(2) + 1)} \\
&\hspace{4.5cm}- \left(\frac{2}{(2 + 2)(2 + 1)}\right)^2{\Biggr]}{\fontsize{0.5cm}{1pt}\selectfont \Biggr\}} \\
&= \frac{7}{120}.
\end{aligned}
\end{equation}

Once again, the variance of Euclidean distance surprisingly approaches a constant.

For the case in which the number of attributes $p$ is small, we have an improved estimate of the mean (Eq.~\ref{eq:DqImprovedExplained}). The lower dimensional estimate of the mean is given by
%
\begin{equation}\label{eq:uniformEucMeanImproved}
\begin{aligned}
\text{E}\left(D^{(2)}_{ij}\right) &= \left(\frac{2p}{(2+2)(2+1)} - \frac{7}{120}\right)^{1/2} \\
&= \sqrt{\frac{p}{6} - \frac{7}{120}}.
\end{aligned}
\end{equation}

The moment estimates given in this section (Eqs.~\ref{eq:uniformEucMeanImproved} and \ref{eq:uniformEucVar}) are summarized in a table that is organized by metric, data type, statistic (mean or variance), and asymptotic formula (Table~\ref{tab:dist_distr_standardL1L2}). This concludes our analysis with continuous data distributions and the standard $L_q$ metric. In the next section, we will use extreme value theory to derive the distribution of the sample maximum and minimum for standard normal and standard uniform data. This will lead us to asymptotics for the max-min normalized $L_q$ metric used frequently in Relief-based algorithms \cite{urbanowicz17,robnik2003} for scoring features.

\subsection{Distribution of max-min normalized \texorpdfstring{$L_q$}{} metric}\label{sec:extremes}

For Relief-based methods \cite{robnik2003,urbanowicz17}, the standard numeric diff metric is given by
%
\begin{equation}\label{eq:normDiff}
\text{d}^{\text{num}}_{ij}(a) = \text{diff}(a,(i,j)) = \frac{|X_{ia} - X_{ja}|}{\text{max}(a) - \text{min}(a)},
\end{equation}
%
where $\text{max}(a) = \displaystyle \max_{k \in \mathcal{I}}\{X_{ka}\}$, $\text{min}(a) = \displaystyle \min_{k \in \mathcal{I}}\{X_{ka}\}$, and $\mathcal{I} = \{1,2,\dots,m\}$. 

The pairwise distance using this max-min normalized diff metric is then computed as
%
\begin{equation}\label{eq:D*}
\begin{aligned}
D^{(q*)}_{ij} &= \left(\sum_{a\in \mathcal{A}}|\text{d}_{ij}(a)|^q\right)^{1/q} \\
&= \left(\sum_{a\in \mathcal{A}}\left(\frac{|X_{ia} - X_{ja}|}{\text{max}(a) - \text{min}(a)}\right)^q\right)^{1/q}.
\end{aligned}
\end{equation}

In order to determine moments of asymptotic max-min normalized distance (Eq.~\ref{eq:normDiff}) distributions induced by, we must first derive the asymptotic extreme value distributions of the attribute maximum and minimum. Although the exact distribution of the maximum or minimum requires an assumption about the data distribution, the Fisher-Tippett-Gnedenko Theorem is an important result that allows one to generally categorize the extreme value distribution for a collection of independent and identically distributed random variables into one of three distributional families. This theorem does not, however, tell us the exact distribution of the maximum that we require in order to determine asymptotic results for the max-min normalized distance (Eq.~\ref{eq:D*}). We mention this theorem simply to provide some background on convergence of extreme values. Before stating the theorem, we first need the following definition
%
\begin{definition}
	A distribution $\mathcal{F}_X$ is said to be \textbf{degenerate} if its density function $f_X$ is the Dirac delta $\delta(x - c_0)$ centered at a constant $c_0 \in \mathbb{R}$, with corresponding distribution function $F_X$ defined as
	
	\[F_X(x)=\begin{cases}
	1, & x \geq c_0, \\
	0, & x < c_0.
	\end{cases}
	\]
\end{definition}
%
\begin{theorem}[Fisher-Tippett-Gnedenko]\label{thm:EVT}
	
	Let $X_{ia} \overset{iid}{\sim} \mathcal{F}_X\left(\mu_x,\sigma^2_x\right)$ for all $i \in \mathcal{I}$ and let $X^\text{max}_a = \displaystyle \max_{k \in \mathcal{I}}\{X_{ka}\}$. If there exists two non-random sequences $b_m>0$ and $c_m$ such that
	%
	\[\lim_{m \to \infty} \text{P}\left(\frac{X^\text{max}_a - c_m}{b_m} \leq x\right) = G_X(x),\]
	%
	\noindent where $G_X$ is a non-degenerate distribution function, then the limiting distribution $\mathcal{G}_X$ is in the Gumbel, Fr\'{e}chet, or Wiebull family.
\end{theorem}

The three distribution families given in Thm.~\ref{thm:EVT} are actually special cases of the Generalized Extreme Value Distribution. In the context of extreme values, Thm.~\ref{thm:EVT} is analogous to the Central Limit Theorem for the distribution of sample mean.  Although we will not explicitly invoke this theorem, it does tell us something very important about the asymptotic behavior of sample extremes under certain necessary conditions. For illustration of this general phenomenon of sample extremes, we derive the distribution of the maximum for standard normal data to show that the limiting distribution is in the Gumbel family, which is a well known result. In the case of standard uniform data, we will derive the distribution of the maximum and minimum directly. Regardless of data type, the distribution of the sample maximum can be derived as follows
%
\begin{equation}\label{eq:exact_max}
\begin{aligned}
\text{P}[X^\text{max}_a \leq x] &= \text{P}\left[\max_{k \in \mathcal{I}}\{X_{ka}\} \leq x\right] \\
&= \text{P}[X_{1a} \leq x, X_{2a} \leq x, \dots, X_{ma} \leq x] \\
&= \prod_{k = 1}^{m} \text{P}[X_{ka} \leq x] \\
&= \prod_{k=1} F_X(x) \\
&= [F_X(x)]^m.
\end{aligned}
\end{equation}

Using more precise notation, the distribution function of the sample maximum in standard normal data is
%
\begin{equation}\label{eq:exact_max_distr_fn}
F_\text{max}(x) = [F_X(x)]^m,
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived and $F_X$ is the distribution function corresponding to the data sample. This means that the distribution of the sample maximum relies only on the distribution function of the data from which extremes are drawn $F_X$ and the size of the sample $m$.

Differentiating the distribution function (Eq.~\ref{eq:exact_max_distr_fn}) gives us the following density function for the distribution of the maximum
%
\begin{equation}\label{eq:exact_max_dens_fn}
\begin{aligned}
f_\text{max}(x) &= \frac{\text{d}}{\text{d}x} F_\text{max}(x) \\
&= \frac{\text{d}}{\text{d}x} [F_X(x)]^m \\
&= m [F_X(x)]^{m-1} f_X(x),
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived, $F_X$ is the distribution function corresponding to the data sample, and $f_X$ is the density function corresponding to the data sample. Similar to the distribution function for the sample maximum (Eq.~\ref{eq:exact_max_distr_fn}), the density function (Eq~\ref{eq:exact_max_dens_fn}) relies only on the distribution and density function of the data from which extremes are derived.

The distribution of the sample minimum, $X^\text{min}_a$, can be derived as follows
%
\begin{equation}\label{eq:exact_min}
\begin{aligned}
\text{P}[X^\text{min}_a \leq x] &= 1 - \text{P}[X^\text{min}_a \geq x] \\
&= 1 - \text{P}\left[\min_{k \in \mathcal{I}}\{X_{ka}\} \geq x\right] \\
&= 1 - \text{P}[X_{1a} \geq x, X_{2a} \geq x, \dots, X_{ma} \geq x] \\
&= 1 - \prod_{k=1}^{m}\text{P}[X_{ka} \geq x] \\
&= 1 - \left[\text{P}[X_{1a} \geq x]\right]^m \\
&= 1 - \left[1 - \text{P}[X_{1a} \leq x]\right]^m \\
&= 1 - \left[1 - F_X(x)\right]^m,
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived and $F_X$ is the distribution function corresponding to the data sample. Therefore, the distribution of sample minimum also relies only on the distribution function of the data from which extremes are derived.

With more precise notation, we have the following expression for the distribution function of the minimum
%
\begin{equation}\label{eq:exact_min_distr_fn}
F_\text{min}(x) = 1 - [1 - F_X(x)]^m.
\end{equation}
%
where $m$ is the size of the sample from which the minimum is derived and $F_X$ is the distribution function corresponding to the data sample.

Differentiating the distribution function (Eq.~\ref{eq:exact_min_distr_fn}) gives us the following density function for the distribution of sample minimum
%
\begin{equation}\label{eq:exact_min_dens_fn}
\begin{aligned}
f_\text{min}(x) &= \frac{\text{d}}{\text{d}x} F_\text{min}(x) \\
&= \frac{\text{d}}{\text{d}x} \left(1 - [1 - F_X(x)]^m\right) \\
&= m\left[1 - F_X(x)\right]^{m-1}f_X(x),
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which the minimum is derived, $F_X$ is the distribution function corresponding to the data sample, and $f_X$ is the density function corresponding to the data sample. As in the case of the density function for sample maximum (Eq.~\ref{eq:exact_max_dens_fn}), the density function for sample minimum relies only on the distribution $F_X$ and density $f_X$ functions of the data from which extremes are derived and the sample size $m$.

Given the densities of the distribution of sample maximum and minimum, we can easily compute the raw moments and variance. The first moment about the origin of the distribution of sample maximum is given by the following
%
\begin{equation}\label{eq:mu_max}
\begin{aligned}
\mu^{(1)}_\text{max}(m) = \text{E}(X^\text{max}_a) &= \int_{-\infty}^{\infty}x f_\text{max}(x)\text{d}x \\
&= \int_{-\infty}^{\infty}x \left(m [F_X(x)]^{m-1} f_X(x)\right)\text{d}x \\
&= m \int_{-\infty}^{\infty}x f_X(x) [F_X(x)]^{m-1}\text{d}x,
\end{aligned}
\end{equation}
%
where $m$ is the sample size, $F_X$ is the distribution function, and $f_X$ is the density function of the data from which the maximum is derived.

The second raw moment of the distribution of sample maximum is derived similarly as follows
%
\begin{equation}\label{eq:mu2_max}
\begin{aligned}
\mu^{(2)}_\text{max}(m) = \text{E}[(X^\text{max}_a)^2] &= \int_{-\infty}^{\infty}x^2 f_\text{max}(x)\text{d}x \\
&= \int_{-\infty}^{\infty}x^2 \left(m [F_X(x)]^{m-1} f_X(x)\right)\text{d}x \\
&= m \int_{-\infty}^{\infty}x^2 f_X(x) [F_X(x)]^{m-1}\text{d}x
\end{aligned}
\end{equation}
%
where $m$ is the sample size, $F_X$ is the distribution function, and $f_X$ is the density function of the data from which the maximum is derived.

Using the first (Eq.~\ref{eq:mu_max}) and second (Eq.~\ref{eq:mu2_max}) raw moments of the distribution of sample maximum, the variance is given by
\begin{equation}\label{eq:sig_max}
\sigma^2_\text{max}(m) = \mu^{(2)}_\text{max}(m) - \left[\mu^{(1)}_\text{max}(m)\right]^2,
\end{equation}
%
where $m$ is the sample size of the data from which the maximum is derived and $\mu^{(1)}_\text{max}(m)$ and $\mu^{(2)}_\text{max}$ are the first and second raw moments, respectively, of the distribution of sample maximum.

Moving on to the distribution of sample minimum, the first raw moment is given by the following
%
\begin{equation}\label{eq:mu_min}
\begin{aligned}
\mu^{(1)}_\text{min}(m) = \text{E}(X^\text{min}_a) &= \int_{-\infty}^{\infty}x f_\text{min}(x)\text{d}x \\
&= \int_{-\infty}^{\infty}x \left(m [F_X(x)]^{m-1} f_X(x)\right)\text{d}x \\
&= m \int_{-\infty}^{\infty}x f_X(x) [F_X(x)]^{m-1}\text{d}x,
\end{aligned}
\end{equation}
%
where $m$ is the sample size, $F_X$ is the distribution function, and $f_X$ is the density function of the data from which the minimum is derived.

Similarly, the second raw moment of the distribution of sample minimum is given by the following
%
\begin{equation}\label{eq:mu2_min}
\begin{aligned}
\mu^{(2)}_\text{min}(m) = \text{E}[(X^\text{min}_a)^2] &= \int_{-\infty}^{\infty}x^2 f_\text{min}(x)\text{d}x \\
&= \int_{-\infty}^{\infty}x^2 \left(m [F_X(x)]^{m-1} f_X(x)\right)\text{d}x \\
&= m \int_{-\infty}^{\infty}x^2 f_X(x) [F_X(x)]^{m-1}\text{d}x,
\end{aligned}
\end{equation}
%
where $m$ is the sample size, $F_X$ is the distribution function, and $f_X$ is the density function of the data from which the minimum is derived.

Using the first (Eq.~\ref{eq:mu_min}) and second (Eq.~\ref{eq:mu2_min}) raw moments of the distribution of sample minimum, the variance is given by
%
\begin{equation}\label{eq:sig_min}
\sigma^2_\text{min}(m) = \mu^{(2)}_\text{min}(m) - \left[\mu^{(1)}_\text{min}(m)\right]^2,
\end{equation}
%
where $m$ is the sample size of the data from which the maximum is derived and $\mu^{(1)}_\text{min}(m)$ and $\mu^{(2)}_\text{min}$ are the first and second raw moments, respectively, of the distribution of sample maximum.

Using the expected attribute maximum (Eq.~\ref{eq:mu_max}) and minimum (Eq.~\ref{eq:mu_min}) for sample size $m$, the following expected attribute range results from linearity of the expectation operator
%
\begin{equation}\label{eq:exp_rng}
\begin{aligned}
\text{E}(X^\text{max}_a - X^\text{min}_a) &= \text{E}(X^\text{max}_a) - \text{E}(X^\text{min}_a) \\
&= \mu^{(1)}_\text{max}(m) - \mu^{(1)}_\text{min}(m).
\end{aligned}
\end{equation}
%
where $\mu^{(1)}_\text{max}(m)$ is the expected sample maximum (Eq.~\ref{eq:mu_max}) and $\mu^{(1)}_\text{min}(m)$ is the expected sample minimum.

For a data distribution that has zero skewness and support that is symmetric about 0, the expected attribute range (Eq.~\ref{eq:exp_rng}) can be simplified to the following expression
%
\begin{equation}\label{eq:exp_rng_symm}
\text{E}(X^\text{max}_a - X^\text{min}_a) = 2 \mu^{(1)}_\text{max}(m),
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived. Hence, the expected attribute range is simply twice the expected attribute maximum (Eq.~\ref{eq:mu_max}). This result naturally applies to standard normal data, which is symmetric about its mean at 0 and without any skewness.  

For large samples ($m >> 1$) from an exponential type distribution that has infinite support and all moments, the covariance between the sample maximum and minimum is approximately zero \cite{gumbel1947}. In this case, the variance of the attribute range of a sample of size $m$ is given by the following
%
\begin{equation}\label{eq:var_rng}
\begin{aligned}
\text{Var}(X^\text{max}_a - X^\text{min}_a) &\approx \text{Var}(X^\text{max}_a) + \text{Var}(X^\text{min}_a) \\
&= \sigma^2_\text{max}(m) + \sigma^2_\text{min}(m).
\end{aligned}
\end{equation}

Under the assumption of zero skewness, infinite support that is symmetric about 0, sufficiently large sample size $m$, and distribution of an exponential type with all moments, the variance of attribute range (Eq.~\ref{eq:var_rng}) simplifies to become the following
%
\begin{equation}\label{eq:var_rng_symm}
\begin{aligned}
\text{Var}(X^\text{max}_a - X^\text{min}_a) &= 2 \text{Var}(X^\text{max}_a) \\
&= 2 \sigma^2_\text{max}.
\end{aligned}
\end{equation} 

Let $\mu_{D^{(q)}_{ij}}$ and $\sigma^2_{D^{(q)}_{ij}}$ (Eq.~\ref{eq:DDistr}) denote the mean and variance of the standard $L_q$ distance metric (Eq.~\ref{eq:D}). Then the expected value of the max-min normalized distance (Eq.~\ref{eq:D*}) distribution is given by the following
%
\begin{equation}\label{eq:max-min_D_mean}
\begin{aligned}
\mu_{D^{(q*)}_{ij}} &= \text{E}\left[\left(\sum_{a \in \mathcal{A}}\left(\frac{|X_{ia} - X_{ja}|}{X^\text{max}_a - X^\text{min}_a}\right)^q\right)^{1/q}\right] \\
&\approx \frac{1}{\text{E}(X^\text{max}_a - X^\text{min}_a)}\text{E}\left[\left(\sum_{a \in \mathcal{A}}|X_{ia} - X_{ja}|^q\right)^{1/q}\right] \\
&= \frac{\mu_{D^{(q)}_{ij}}}{\text{E}(X^\text{max}_a) - \text{E}(X^\text{min}_a)} \\
&= \frac{\mu_{D^{(q)}_{ij}}}{\mu^{(1)}_\text{max}(m) - \mu^{(1)}_\text{min}(m)},
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived, $\mu^{(1)}_\text{max}(m)$ is the expected value of the sample maximum (Eq.~\ref{eq:mu_max}), and $\mu^{(1)}_\text{min}$ is the expected value of the sample minimum. 

The variance of the max-min normalized distance (Eq.~\ref{eq:D*}) distribution is given by the following
%
\begin{equation}\label{eq:max-min_D_var}
\begin{aligned}
\sigma^2_{D^{(q*)}_{ij}} &= \text{Var}\left[\left(\sum_{a \in \mathcal{A}}\left(\frac{|X_{ia} - X_{ja}|}{X^\text{max}_a - X^\text{min}_a}\right)^q\right)^{1/q}\right] \\
&= \text{E}\left[\left(\sum_{a \in \mathcal{A}}\left(\frac{|X_{ia} - X_{ja}|}{X^\text{max}_a - X^\text{min}_a}\right)^q\right)^{2/q}\right] \\
&\hspace{3cm}- \left(\text{E}\left[\left(\sum_{a \in \mathcal{A}}\left(\frac{|X_{ia} - X_{ja}|}{X^\text{max}_a - X^\text{min}_a}\right)^q\right)^{1/q}\right]\right)^2 \\
&\approx \frac{\text{E}\left[\left(\displaystyle \sum_{a \in \mathcal{A}}|X_{ia} - X_{ja}|^q\right)^{2/q}\right]}{\text{E}[(X^\text{max}_a - X^\text{min}_a)^2]} - \frac{\left(\text{E}\left[\left(\displaystyle \sum_{a \in \mathcal{A}}|X_{ia} - X_{ja}|^q\right)^{1/q}\right]\right)^2}{\text{E}[(X^\text{max}_a - X^\text{min}_a)^2]} \\
&= \frac{\sigma^2_{D^{(q)}_{ij}} + \mu^2_{D^{(q)}_{ij}}}{\text{E}[(X^\text{max}_a - X^\text{min}_a)^2]} - \frac{\mu^2_{D^{(q)}_{ij}}}{\text{E}[(X^\text{max}_a - X^\text{min}_a)^2]} \\
&= \frac{\sigma^2_{D^{(q)}_{ij}}}{\text{E}[(X^\text{max}_a - X^\text{min}_a)^2]} \\
&= \frac{\sigma^2_{D^{(q)}_{ij}}}{\text{E}[(X^\text{max}_a)^2] - 2\text{E}(X^\text{max}_a)\text{E}(X^\text{min}_a) + \text{E}(X^\text{min}_a)} \\
&= \frac{\sigma^2_{D^{(q)}_{ij}}}{\mu^{(2)}_\text{max}(m) - 2\mu^{(1)}_\text{max}(m)\mu^{(1)}_\text{min}(m) + \mu^{(2)}_\text{min}(m)},
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived, $\mu^{(1)}_\text{max}(m)$ is the expected value of the sample maximum (Eq.~\ref{eq:mu_max}), and $\mu^{(1)}_\text{min}$ is the expected value of the sample minimum.  

With the mean (Eq.~\ref{eq:max-min_D_mean}) and variance (Eq.~\ref{eq:max-min_D_var}) of the max-min normalized distance (Eq.~\ref{eq:D*}), we have the following generalized estimate for the asymptotic distribution of the max-min normalized distance distribution
%
\begin{equation}\label{eq:max-min-DDistr-general}
\begin{aligned}
D^{(q*)}_{ij} \overset{.}{\sim} \mathcal{N}{\fontsize{0.5cm}{1pt}\selectfont \Biggl(}&\frac{\mu_{D^{(q)}_{ij}}}{\mu^{(1)}_\text{max}(m) - \mu^{(1)}_\text{min}(m)}, \\
&\frac{\sigma^2_{D^{(q)}_{ij}}}{\mu^{(2)}_\text{max}(m) - 2 \mu^{(1)}_\text{max}(m) \mu^{(1)}_\text{min}(m) + \mu^{(2)}_\text{min}(m)}{\fontsize{0.5cm}{1pt}\selectfont \Biggr)},
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived, $\mu^{(1)}_\text{max}(m)$ is the expected value of the sample maximum (Eq.~\ref{eq:mu_max}), and $\mu^{(1)}_\text{min}$ is the expected value of the sample minimum.

For data with zero skewness and support that is symmetric about 0, the expected sample maximum is the additive inverse of the expected sample minimum. This allows us to express the expected max-min normalized pairwise distance (Eq.~\ref{eq:max-min_D_mean}) exclusively in terms of the expected sample maximum. This result is given by the following
%
\begin{equation}\label{eq:max-min_D_mean_symm}
\mu_{D^{(q*)}_{ij}} \approx \frac{\mu_{D^{(q)}_{ij}}}{2\mu^{(1)}_\text{max}(m)},
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived and $\mu^{(1)}_\text{max}(m)$ is the expected value of the sample maximum (Eq.~\ref{eq:mu_max}).

A similar substitution gives us the following expression for the variance of the max-min normalized distance distribution
%
\begin{equation}\label{eq:max-min_D_var_symm}
\begin{aligned}
\sigma^2_{D^{(q*)}_{ij}} &\approx \frac{\sigma^2_{D^{(q)}_{ij}}}{2\mu^{(2)}_\text{max}(m) + 2\left[\mu^{(1)}_\text{max}(m)\right]^2} \\
&= \frac{\sigma^2_{D^{(q)}_{ij}}}{2\left(\sigma^2_\text{max}(m) + \left[\mu^{(1)}_\text{max}(m)\right]^2\right)},
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived, $\mu^{(1)}_\text{max}(m)$ is the expected value of the sample maximum (Eq.~\ref{eq:mu_max}), and $\sigma^2_\text{max}(m)$ is the variance of the sample maximum (Eq.~\ref{eq:sig_max}).

Therefore, the asymptotic distribution of the max-min normalized distance distribution (Eq.~\ref{eq:max-min-DDistr-general}) becomes
%
\begin{equation}\label{eq:max-min_DDistr}
D^{(q*)}_{ij} \overset{.}{\sim} \mathcal{N}\left(\frac{\mu_{D^{(q)}_{ij}}}{2\mu^{(1)}_\text{max}(m)}, \frac{\sigma^2_{D^{(q)}_{ij}}}{2\left(\sigma^2_\text{max}(m) + \left[\mu^{(1)}_\text{max}(m)\right]^2\right)}\right),
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived, $\mu^{(1)}_\text{max}(m)$ is the expected value of the sample maximum (Eq.~\ref{eq:mu_max}), and $\sigma^2_\text{max}(m)$ is the variance of the sample maximum (Eq.~\ref{eq:sig_max}). 

We have now derived asymptotic estimates of the moments of the max-min normalized $L_q$ distance metric (Eq.~\ref{eq:D*}) for any continuous data distribution. In the next two sections, we examine the max-min normalized $L_q$ distance on standard normal and standard uniform data. As in previous sections in which we analyzed the standard $L_q$ metric (Eq.~\ref{eq:D}), we will use the more general results for the max-min $L_q$ metric to derive asymptotic estimates for normalized Manhattan ($q=1$) and Euclidean ($q=2$).

\subsubsection{Standard normal data}

The standard normal distribution has zero skewness, infinite support that is symmetric about 0, and all moments. This implies that the corresponding mean and variance of the distribution of sample range can be expressed exclusively in terms of the sample maximum. Given the nature of the density function of the sample maximum for sample size $m$, the integration required to determine the moments (Eqs.~\ref{eq:mu_max} and \ref{eq:mu2_max}) is not possible. These moments can either be approximated numerically or we can use extreme value theory to determine the form of the asymptotic distribution of the sample maximum. Using the latter method, we will show that the asymptotic distribution of the sample maximum for standard normal data is in the Gumbel family. Let $c_m = -\Phi^{-1}\left(\frac{1}{m}\right)$ and $b_m = \frac{1}{c_m}$, where $\Phi$ is the standard normal cumulative distribution function. Using Taylor's Theorem, we have the following expansion
%
\begin{equation}\label{eq:log_expand}
\begin{aligned}
\text{log}\Phi(-c_m - b_m x) &= \text{log}\Phi(-c_m) - b_m x \frac{\phi(-c_m)}{\Phi(-c_m)} + \mathcal{O}(b^2_m x^2) \\
&= \text{log}\left(\frac{1}{m}\right) - x \frac{\phi(-c_m)}{c_m \Phi(-c_m)} + \mathcal{O}(b^2_m x^2),
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived.

In order to simplify the right-hand side of this expansion (Eq.~\ref{eq:log_expand}), we will use the well known Mills Ratio Bounds \cite{chatterjee2014} given by the following
%
\begin{equation}\label{eq:mills}
1 \leq \frac{\phi(x)}{x \Phi(-x)} \leq 1 + \frac{1}{x^2} \quad , x > 0,
\end{equation}
%
where $\Phi$ and $\phi$ once again represent the cumulative distribution function and density function, respecively, of the standard normal distribution.

The inequalities given above (Eq.~\ref{eq:mills}) show that 
%
\[\frac{\phi(x)}{x \Phi(-x)} \rightarrow 1 \text{ as } x \rightarrow \infty.
\] 

This further implies that 
%
\[
\frac{\phi(c_m)}{c_m \Phi(-c_m)} \rightarrow 1 \text{ as } m \rightarrow \infty
\] 
%
since 
%
\[
c_m = -\Phi^{-1}\left(\frac{1}{m}\right) \rightarrow \infty \text{ as } m \rightarrow \infty.
\] 

This gives us the following approximation of the right-hand side of the expansion (Eq.~\ref{eq:log_expand}) given previously
%
\begin{equation}\label{eq:approx_log_expand}
\begin{aligned}
\text{log}\Phi(-c_m - b_m x) &\approx \text{log}\left(\frac{1}{m}\right) - x + \mathcal{O}(b^2_m x^2) \\
\Rightarrow \Phi(-c_m - b_m x) &\approx \frac{1}{m}e^{-x + \mathcal{O}(b^2_m x^2)} \\
\Rightarrow \Phi(c_m + b_m x) &\approx 1 - \frac{1}{m}e^{-x + \mathcal{O}(b^2_m x^2)},
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived.

Using the approximation of expansion given previously (Eq.~\ref{eq:approx_log_expand}), we now derive the limit distribution for the sample maximum in standard normal data as
%
\begin{equation}\label{eq:prob_normal_max}
\begin{aligned}
\text{P}\left(\frac{X^\text{max}_a - c_m}{b_m} \leq x\right) &= \text{P}(X^\text{max}_a \leq c_m + b_m x) \\
&= \Phi^m(c_m + b_m x) \\
&\approx \left(1 - \frac{1}{m}e^{-x + \mathcal{O}(b^2_m x^2)}\right)^m \\
&= \left(1 - \frac{1}{m}e^{-x + \mathcal{O}\left(\frac{1}{c^2_m} x^2\right)}\right)^m \\
&\approx \left(1 - \frac{1}{m}e^{-x}\right)^m \\
\Rightarrow \lim_{m \to \infty} \text{P}\left(\frac{X^\text{max}_a - c_m}{b_m} \leq x\right) &= \lim_{m \to \infty} \left(1 - \frac{1}{m}e^{-x}\right)^m \\
&= e^{-e^{-x}},
\end{aligned}
\end{equation}
%
which is the cumulative distribution function of the standard Gumbel distribution. The mean of this distribution is given by the following
%
\begin{equation}\label{eq:mu_max_normal}
\text{E}(X^\text{max}_a) = \mu^{(1)}_\text{max} = -\Phi^{-1} \left(\frac{1}{m}\right) - \frac{\gamma}{\Phi^{-1}\left(\frac{1}{m}\right)},
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived and $\gamma$ is the well known Euler-Mascheroni constant. This constant has many equivalent definitions, one of which is given by
%
\[
\gamma = \lim_{m \to \infty} \left(-\text{log}(m) + \sum^{m}_{k=1}\frac{1}{k}\right).
\]

Perhaps a more convenient definition of the Euler-Mascheroni constant is simply
%
\[
\gamma = - \Gamma^\prime (1) = \frac{\text{d}}{\text{d}t} \left(\int^{\infty}_{0} z^{t-1} e^{-z} \text{d}z\right) \biggr|_{t=1},
\]
%
which is just the additive inverse of the first derivative of the gamma function evaluated at 1.

The median of the distribution of the maximum for standard normal data is given by
%
\begin{equation}\label{eq:med_max_normal}
\overset{\sim}{\mu}_\text{max} = \frac{\text{log}(\text{log}(2))}{\Phi^{-1}\left(\frac{1}{m}\right)} - \Phi^{-1}\left(\frac{1}{m}\right),
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived.

Finally, the variance of the asymptotic distribution of the sample maximum is given by
%
\begin{equation}\label{eq:var_max_normal}
\text{Var}(X^\text{max}_a) = \frac{\pi^2}{6}\left(\frac{1}{-\Phi^{-1}\left(\frac{1}{m}\right)}\right)^2,
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived.

For typical sample sizes $m$ in high-dimensional spaces, the variance estimate (Eq.~\ref{eq:var_max_normal}) exceeds the variance of the sample maximum significantly. Using the fact that
%
\[
-\Phi^{-1}\left(\frac{1}{m}\right) \overset{.}{\sim} \sqrt{2 \text{log}(m)} \text{\cite{cramer1999}}
\] 
%
and 
%
\[
\frac{1}{2 \text{log}(m)} \leq \left(\frac{1}{-\Phi^{-1}\left(\frac{1}{m}\right)}\right)^2, \quad m \geq 2,
\] 
%
we can get a more accurate approximation of the variance with the following
%
\begin{equation}\label{eq:var_max_normal_improved}
\begin{aligned}
\sigma^2_\text{max}(m) = \text{Var}(X^\text{max}_a) &\approx \frac{\pi^2}{6}\left(\frac{1}{\sqrt{2\text{log}(m)}}\right)^2 \\
&= \frac{\pi^2}{12\text{log}(m)}.
\end{aligned}
\end{equation}

Therefore, the mean of the range of $m$ iid standard normal random variables is given by
%
\begin{equation}\label{eq:mu_rng_normal}
\text{E}(X^\text{max}_a - X^\text{min}_a) = 2\mu^{(1)}_\text{max}(m) = 2\left[-\Phi^{-1} \left(\frac{1}{m}\right) - \frac{\gamma}{\Phi^{-1}\left(\frac{1}{m}\right)}\right],
\end{equation}
%
where $\gamma$ is the Euler-Mascheroni constant.

It is well known that the sample extremes from the standard normal distribution are approximately uncorrelated for large sample size $m$ \cite{gumbel1947}. This implies that we can approximate the variance of the range of $m$ iid standard normal random variables with the following result
%
\begin{equation}\label{eq:var_rng_normal}
\begin{aligned}
\text{Var}(X^\text{max}_a - X^\text{min}_a) &\approx \text{Var}(X^\text{max}_a) + \text{Var}(X^\text{min}_a) \\
&= \sigma^2_\text{max}(m) + \sigma^2_\text{min}(m) \\
&= 2\sigma^2_\text{max}(m) \\
&\approx 2\left(\frac{\pi^2}{12\text{log}(m)}\right) \\
&= \frac{\pi^2}{6\text{log}(m)}.
\end{aligned}
\end{equation}

For the purpose of approximating the mean and variance of the max-min normalized distance distribution, we observe empirically that the formula for the median of the distribution of the attribute maximum (Eq.~\ref{eq:med_max_normal}) yields more accurate results. More precisely, the approximation of the expected maximum (Eq.~\ref{eq:mu_max_normal}) overestimates the sample maximum slightly. The formula for the median of the sample maximum (Eq.~\ref{eq:med_max_normal}) provides a more accurate estimate of this sample extreme. Therefore, the following estimate for the mean of the attribute range will be used instead
%
\begin{equation}\label{eq:mu_rng_normal_improved}
\text{E}(X^\text{max}_a - X^\text{min}_a) = 2\mu^{(1)}_\text{max}(m) \approx 2\left[\frac{\text{log}(\text{log}(2))}{\Phi^{-1}\left(\frac{1}{m}\right)} - \Phi^{-1}\left(\frac{1}{m}\right)\right],
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived.

We have already determined the mean and variance (Eq.~\ref{eq:normalDistr}) for the $L_q$ metric (Eq.~\ref{eq:D}) on standard normal data. Using the expected value of the sample maximum (Eq.~\ref{eq:mu_rng_normal_improved}), the variance of the sample maximum (Eq.~\ref{eq:var_rng_normal}), and the general formulas for the mean and variance of the max-min normalized distance distribution (Eq.~\ref{eq:max-min_DDistr}), this leads us to the following asymptotic estimate for the distribution of the max-min normalized distances for standard normal data
%
\begin{equation}\label{eq:max-min_DDistr_normal}
D^{(q*)}_{ij} \overset{.}{\sim} \mathcal{N}\left(\frac{\mu_{D^{(q)}_{ij}}}{2\mu^{(1)}_\text{max}(m)}, \frac{6 \text{log}(m) \sigma^2_{D^{(q)}_{ij}}}{\pi^2 + 24 \left[\mu^{(1)}_\text{max}(m)\right]^2 \text{log}(m)}\right).
\end{equation}
%
where $m$ is the size of the sample from which the maximum is derived, $\mu^{(1)}_\text{max}$ is the median of the sample maximum (Eq.~\ref{eq:med_max_normal}), $\mu_{D^{(q)}_{ij}}$ is the expected $L_q$ pairwise distance (Eq.~\ref{eq:normalDqMean}), and $\sigma^2_{D^{(q)}_{ij}}$ is the variance of the $L_q$ pairwise distance (Eq.~\ref{eq:normalVar}). The moments of the max-min normalized $L_q$ distance metric in standard normal data (Eq.~\ref{eq:max-min_DDistr_normal}) are summarized in a table that is organized by metric, data type, statistic (mean or variance), and asymptotic formula (Table~\ref{tab:dist_distr_general1}).

\subsubsection{Standard uniform data}

Standard uniform data does not have support that is symmetric about 0. Due to the simplicity of the density function, however, we can derive the distribution of the maximum and minimum of a sample of size $m$ explicitly. Using the general forms of the distribution functions of the maximum (Eq.~\ref{eq:exact_max_distr_fn}) and minimum (Eq.~\ref{eq:exact_min_distr_fn}), we have the following distribution functions for standard uniform data
%
\begin{equation}\label{eq:uniform_max_distr}
F_\text{max}(x) = x^m
\end{equation}
%
and
%
\begin{equation}\label{eq:uniform_min_distr}
F_\text{min}(x) = 1 - (1 - x)^m,
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived.

Using the general forms of the density functions of the maximum (Eq.~\ref{eq:exact_max_dens_fn}) and minimum (Eq.~\ref{eq:exact_min_dens_fn}), we have the following density functions for standard uniform data
%
\begin{equation}\label{eq:uniform_max_dens}
f_\text{max}(x) = m x^{m-1}
\end{equation}
%
and
%
\begin{equation}\label{eq:uniform_min_dens}
f_\text{min}(x) = m(1 - x)^{m-1},
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived.

Then the expected maximum and minimum are computed through 

\noindent straightforward integration as follows
%
\begin{equation}\label{eq:mu_max_uniform}
\begin{aligned}
\text{E}(X^\text{max}_a) = \mu^{(1)}_\text{max}(m) &= \int_{0}^{1} x f_\text{max}(x) \text{d}x \\
&= \int_{0}^{1} x [m x^{m-1}] \text{d}x \\
&= \frac{m}{m+1}
\end{aligned}
\end{equation}
%
and
%
\begin{equation}\label{eq:mu_min_uniform}
\begin{aligned}
\text{E}(X^\text{min}_a) = \mu^{(1)}_\text{min}(m) &= \int_{0}^{1} x f_\text{min}(x) \text{d}x \\
&= \int_{0}^{1} x [m (1 - x)^{m-1}] \text{d}x \\
&= \frac{1}{m+1},
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived.

We can compute the second moment about the origin of the sample range as follows
%
\begin{equation}\label{eq:mu2_rng_uniform}
\begin{aligned}
\text{E}[(X^\text{max}_a - X^\text{min}_a)^2] &= \text{E}[(X^\text{max}_a)^2 - 2 X^\text{max}_a X^\text{min}_a + (X^\text{min}_a)^2] \\
&= \text{E}[(X^\text{max}_a)^2] - 2 \text{E}(X^\text{max}_a) \text{E}(X^\text{min}_a) + \text{E}[(X^\text{min}_a)^2] \\
&= \mu^{(2)}_\text{max}(m) - 2 \mu^{(1)}_\text{max}(m) \mu^{(1)}_\text{min}(m) + \mu^{(2)}_\text{min}(m) \\
&= \int_{0}^{1} x^2 [m x^{m-1}] \text{d}x - 2 \left(\frac{m}{m+1}\right) \left(\frac{1}{m+1}\right) \\
& \hspace{3.12cm} + \int_{0}^{1} x^2 [m (1 - x)^{m-1}] \text{d}x \\
&= \frac{m}{m+2} - \frac{2m}{(m+1)^2} + \frac{2}{(m+1)(m+2)} \\
&= \frac{m^3 - m + 2}{(m+2)(m+1)^2},
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived.

Using the general asymptotic distribution of max-min normalized distances for any data type (Eq.~\ref{eq:max-min-DDistr-general}) and the mean and variance (Eq.~\ref{eq:uniformDistr}) of the standard $L_q$ distance metric (Eq.~\ref{eq:D}), we have the following asymptotic estimate for the max-min normalized distance distribution for standard uniform data
%
\begin{equation}\label{eq:max-min_DDistr_uniform}
D^{(q*)}_{ij} \overset{.}{\sim} \mathcal{N}\left(\frac{(m+1)\mu_{D^{(q)}_{ij}}}{m-1}, \frac{(m+2)(m+1)^2 \sigma^2_{D^{(q)}_{ij}}}{m^3 - m + 2}\right),
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived, $\mu_{D^{(q)}_{ij}}$ is the expected value (Eq.~\ref{eq:uniformDqMean}) of the $L_q$ metric (Eq.~\ref{eq:D}) in standard uniform data, and $\sigma^2_{D^{(q)}_{ij}}$ is the variance (Eq.~\ref{eq:uniformDqVar}) of the $L_q$ metric (Eq.~\ref{eq:D}) in standard uniform data. The moments of the max-min normalized $L_q$ distance metric in standard uniform data (Eq.~\ref{eq:max-min_DDistr_normal}) are summarized in a table that is organized by metric, data type, statistic (mean or variance), and asymptotic formula (Table~\ref{tab:dist_distr_general1}).

\subsection{Normalized Manhattan \texorpdfstring{($q=1$)}{}}

Using the general asymptotic results for mean and variance of max-min normalized distances in standard normal and standard uniform data (Eqs.~\ref{eq:max-min_DDistr_normal} and \ref{eq:max-min_DDistr_uniform}) for any value of $q \in \mathbb{N}$, we can substitute a particular value of $q$ in order to determine a more specified distribution for the normalized distance (Eq.~\ref{eq:D*}). The following results are for the max-min normalized Manhattan ($q = 1$) metric for both standard normal and standard uniform data.

\subsubsection{Standard normal data}

Substituting $q=1$ into the asymptotic formula for the expected max-min normalized distance (Eq.~\ref{eq:max-min_DDistr_normal}), we derive the expected normalized Manhattan distance in standard normal data as follows
%
\begin{equation}\label{eq:max-min_mean_normal_manhattan}
\begin{aligned}
\text{E}\left(D^{(1*)}_{ij}\right) &= \frac{\mu_{D^{(1)}_{ij}}}{2\mu^{(1)}_\text{max}(m)} \\
&= \frac{p}{\sqrt{\pi}\mu^{(1)}_\text{max}(m)},
\end{aligned}
\end{equation}
%
where $\mu^{(1)}_\text{max}(m)$ is the expected attribute maximum (Eq.~\ref{eq:med_max_normal}),$m$ is the size of the sample from which the maximum is derived, and $p$ is the total number of attributes.

Similarly, the variance of $D^{(1*)}_{ij}$ is given by
%
\begin{equation}\label{eq:max-min_var_normal_manhattan}
\begin{aligned}
\text{Var}\left(D^{(1*)}_{ij}\right) &= \frac{6\text{log}(m)\sigma^2_{D^{(1)}_{ij}}}{\pi^2 + 24\left[\mu^{(1)}_\text{max}\right]^2\text{log}(m)} \\
&= \frac{12p(\pi-2)\text{log}(m)}{\pi\left(\pi^2 + 24\left[\mu^{(1)}_\text{max}\right]^2\text{log}(m)\right)},
\end{aligned}
\end{equation}
%
where $\mu^{(1)}_\text{max}(m)$ is the expected attribute maximum (Eq.~\ref{eq:med_max_normal}), $m$ is the size of the sample from which the maximum is derived, and $p$ is the total number of attributes. Similar to the variance of the standard Manhattan distance, the variance of the max-min normalized Manhattan distance is on the order of $p$ for fixed instance dimension $m$. For fixed $p$, the variance (Eq.~\ref{eq:max-min_var_normal_manhattan}) vanishes as $m$ grows without bound. If we fix $m$, the same variance increases monotonically with increasing $p$. The moments derived in this section (Eqs.~\ref{eq:max-min_mean_normal_manhattan} and \ref{eq:max-min_var_normal_manhattan}) are summarized in a table that is organized by metric, data type, statistic (mean or variance), and asymptotic formula (Table~\ref{tab:dist_distr_normalizedL1L2}).

\subsubsection{Standard uniform data}

Substituting $q=1$ into the asymptotic formula for the expected max-min pairwise distance (Eq.~\ref{eq:max-min_DDistr_uniform}), we derive the expected normalized Manhattan distance in standard uniform data as
%
\begin{equation}\label{eq:max-min_mean_uniform_manhattan}
\begin{aligned}
\text{E}\left(D^{(1*)}_{ij}\right) &= \frac{(m+1)\mu_{D^{(1)}_{ij}}}{m-1} \\
&= \frac{(m+1)p}{3(m-1)},
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived and $p$ is the total number attributes.

Similarly, the variance of $D^{(1*)}_{ij}$ is given by
%
\begin{equation}\label{eq:max-min_var_uniform_manhattan}
\begin{aligned}
\text{Var}\left(D^{(1*)}_{ij}\right) &= \frac{(m+2)(m+1)^2\sigma^2_{D^{(1)}_{ij}}}{m^3-m+2} \\
&= \frac{(m+2)(m+1)^2p}{18(m^3 - m + 2)},
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived and $p$ is the total number of attributes. Interestingly, the variance of the max-min normalized Manhattan distance in standard uniform data approaches $p/18$ as $m$ increases without bound for a fixed number of features $p$. This is the same asymptotic value to which the variance of the standard Manhattan distance (Eq.~\ref{eq:uniformManMean}) converges. Therefore, large sample sizes make the variance of the normalized Manhattan distance approach the variance of the standard Manhattan distance in standard uniform data. The moments derived in this section (Eqs.~\ref{eq:max-min_mean_uniform_manhattan} and \ref{eq:max-min_var_uniform_manhattan}) are summarized in a table that is organized by metric, data type, statistic (mean or variance), and asymptotic formula (Table~\ref{tab:dist_distr_normalizedL1L2}).

\subsection{Normalized Euclidean \texorpdfstring{($q=2$)}{}}

Analogous to the previous section, we demonstrate the use the asymptotic moment estimates for max-min normalized distances (Eq.~\ref{eq:max-min_DDistr_normal} and \ref{eq:max-min_DDistr_uniform}) for the max-min normalized Euclidean ($q=2$) metric (Eq.~\ref{eq:D*}) for both standard normal and standard uniform data.

\subsubsection{Standard normal data}

Substituting $q=2$ into the asymptotic formula for the expected max-min normalized pairwise distance (Eq.~\ref{eq:max-min_DDistr_normal}), we derive the expected normalized Euclidean distance in standard normal data as
%
\begin{equation}\label{eq:max-min_mean_normal_euclidean}
\begin{aligned}
\text{E}\left(D^{(2*)}_{ij}\right) &= \frac{\mu^{D^{(2)}_{ij}}}{2\mu^{(1)}_\text{max}(m)} \\
&= \frac{\sqrt{2p - 1}}{2\mu^{(1)}_\text{max}(m)},
\end{aligned}
\end{equation}
%
where $\mu^{(1)}_\text{max}(m)$ is the expected attribute maximum (Eq.~\ref{eq:med_max_normal}), $m$ is the size of the sample from which the maximum is derived, and $p$ is the total number of attributes.

Similarly, the variance of $D^{(2*)}_{ij}$ is given by
%
\begin{equation}\label{eq:max-min_var_normal_euclidean}
\begin{aligned}
\text{Var}\left(D^{(2*)}_{ij}\right) &= \frac{6\text{log}(m)\sigma^2_{D^{(2)}_{ij}}}{\pi^2 + 24\left[\mu^{(1)}_\text{max}(m)\right]^2\text{log}(m)} \\
&= \frac{6\text{log}(m)}{\pi^2 + 24\left[\mu^{(1)}_\text{max}(m)\right]^2\text{log}(m)},
\end{aligned}
\end{equation}
%
where $\mu^{(1)}_\text{max}(m)$ is the expected attribute maximum (Eq.~\ref{eq:med_max_normal}) and $m$ is the size of the sample from which the maximum is derived. It is interesting to note that the variance (Eq.~\ref{eq:max-min_var_normal_euclidean}) vanishes as the sample size $m$ increases without bound, which means that all distances will be tightly clustered about the mean (Eq.~\ref{eq:max-min_mean_normal_euclidean}). This is different than the variance of the standard $L_2$ metric (Eq.~\ref{eq:normalEucVar}), which is asymptotically equal to 1. This could imply that any two pairwise distances computed with the max-min normalized Euclidean metric in a large sample space $m$ may be indistinguishable, which is another curse of dimensionality. The moments derived in this section (Eqs.~\ref{eq:max-min_mean_normal_euclidean} and \ref{eq:max-min_var_normal_euclidean}) are summarized in a table that is organized by metric, data type, statistic (mean or variance), and asymptotic formula (Table~\ref{tab:dist_distr_normalizedL1L2}).

\subsubsection{Standard uniform data}

Substituting $q=2$ into the asymptotic formula for the expected max-min normalized pairwise distance (Eq.~\ref{eq:max-min_DDistr_uniform}), we derive the expected normalized Euclidean distance in standard uniform data as
%
\begin{equation}\label{eq:max-min_mean_uniform_euclidean}
\begin{aligned}
\text{E}\left(D^{(2*)}_{ij}\right) &= \frac{(m+1)\mu_{D^{(2)}_{ij}}}{m-1} \\
&= \sqrt{\frac{p}{6} - \frac{7}{120}}\left(\frac{m+1}{m-1}\right).
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived and $p$ is the total number of attributes.

Similarly, the variance of $D^{(2*)}_{ij}$ is given by
%
\begin{equation}\label{eq:max-min_var_uniform_euclidean}
\begin{aligned}
\text{Var}\left(D^{(2*)}_{ij}\right) &= \frac{(m+2)(m+1)^2\sigma^2_{D^{(2)}_{ij}}}{m^3 - m + 2} \\
&= \frac{7(m+2)(m+1)^2}{120(m^3 - m + 2)}.
\end{aligned}
\end{equation}
%
where $m$ is the size of the sample from which extremes are derived. Similar to the variance of max-min normalized Manhattan distances in standard uniform data (Eq.~\ref{eq:max-min_var_uniform_manhattan}), the variance of normalized Euclidean distances approaches the variance of the standard Euclidean distances in uniform data (Eq.~\ref{eq:uniformEucVar}) as $m$ increases without bound. That is, the variance of the max-min normalized Euclidean distance (Eq.~\ref{eq:max-min_var_uniform_euclidean}) approaches $7/120$ as $m$ grows larger. The moments derived in this section (Eqs.~\ref{eq:max-min_mean_uniform_euclidean} and \ref{eq:max-min_var_uniform_euclidean}) are summarized in a table that is organized by metric, data type, statistic (mean or variance), and asymptotic formula (Table~\ref{tab:dist_distr_normalizedL1L2}).

We conclude this section with summary tables (Tables~\ref{tab:dist_distr_general1}-\ref{tab:dist_distr_normalizedL1L2}) that contain all of our asymptotic results for both standard and max-min normalized $L_q$ metrics in each data type we have considered. This includes our most general results for any combination of sample size $m$, number of features $p$, type of metric $L_q$, and data type (Table~\ref{tab:dist_distr_general1}). From these more general derivations, we show the results of the standard $L_1$ and $L_2$ metrics for any combination of sample size $m$, number of features $p$, and data type (Table~\ref{tab:dist_distr_standardL1L2}). Our last set of summarized results show asymptotics for the max-min normalized $L_1$ and $L_2$ metrics for any combination of sample size $m$, number of features $p$, and data type (Table~\ref{tab:dist_distr_normalizedL1L2}). For both standard and max-min normalized $L_2$ metrics (Tables~\ref{tab:dist_distr_standardL1L2} and \ref{tab:dist_distr_normalizedL1L2}), the low-dimensional improved estimates of sample means (Eqs.~\ref{eq:normalEucMeanImproved} and \ref{eq:uniformEucMeanImproved}) are used because they perform well at both low and high attribute dimension $p$.

In the next section, we make a transition into discrete GWAS data. We will discuss some commonly known metrics and then a relatively new metric, which will lead us into novel asymptotic results for this data type.

\begin{table}[H]
	\caption{Summary of distance distribution derivations for standard normal ($\mathcal{N}(0,1)$) and standard uniform ($\mathcal{U}(0,1)$) data. Asymptotic estimates are given for both standard (Eq.~\ref{eq:D}) and max-min normalized (Eq.~\ref{eq:D*}) q-metrics. These estimates are relevant for all $q \in \mathbb{N}$ and $p \gg 1$ for which the normality assumption of distances holds.}
	\label{tab:dist_distr_general1}
	\centering
	\begin{adjustbox}{center}
		\begin{tikzpicture}
		
		% trim=left botm right top
		\node at (0,0) {\includegraphics[clip,trim=0.27cm 0.0cm 0.0cm 0.05cm,width=\textwidth]{updated_distributions_table(5-23-2019).pdf}};
		
		% standard Lq metric (column 1 - row 1)
		\node[fill=white,xscale=0.83,yscale=0.83] at (-5.49,3.45) {(\textbf{Eq.}~\textbf{\ref{eq:D}})};
		
		% max-min normalized Lq metric (column 1 - row 2)
		\node[fill=white,xscale=0.83,yscale=0.83] at (-5.49,-3.55) {(\textbf{Eq.}~\textbf{\ref{eq:D*}})};
		
		% mean (row 1)
		\node[fill=white,xscale=0.8,yscale=0.8] at (3.84,5.6) {(\textbf{\ref{eq:normalDqMean}})};
		
		% var (row 2)
		\node[fill=white,xscale=0.8,yscale=0.8] at (5.42,4.4) {(\textbf{\ref{eq:normalVar}})};
		
		% mean (row 3)
		\node[fill=white,xscale=0.8,yscale=0.8] at (3.81,2.85) {(\textbf{\ref{eq:uniformDqMean}})};
		
		% var (row 4)
		\node[fill=white,xscale=0.8,yscale=0.8] at (5.67,1.75) {(\textbf{\ref{eq:uniformDqVar}})};
		
		% mean (row 5)
		\node[fill=white,xscale=0.8,yscale=0.8] at (3.25,0.3) {(\textbf{\ref{eq:max-min_DDistr_normal}})};
		
		% numerator mean (row 5)
		\node[fill=white,xscale=0.84,yscale=0.84] at (3.42,-0.63) {\ref{eq:normalDqMean}};
		
		% denominator mean (row 5)
		\node[fill=white,xscale=0.84,yscale=0.84] at (4.35,-0.63) {\ref{eq:med_max_normal}};
		
		% var (row 6)
		\node[fill=white,xscale=0.8,yscale=0.8] at (4.1,-1.8) {(\textbf{\ref{eq:max-min_DDistr_normal}})};
		
		% numerator mean (row 6)
		\node[fill=white,xscale=0.84,yscale=0.84] at (3.42,-2.815) {\ref{eq:normalDqMean}};
		
		% denominator mean (row 6)
		\node[fill=white,xscale=0.84,yscale=0.84] at (4.34,-2.815) {\ref{eq:med_max_normal}};
		
		% mean (row 7)
		\node[fill=white,xscale=0.8,yscale=0.8] at (3.32,-4) {(\textbf{\ref{eq:max-min_DDistr_uniform}})};
		
		% numerator mean (row 7)
		\node[fill=white,xscale=0.84,yscale=0.84] at (3.63,-4.7) {\ref{eq:uniformDqMean}};
		
		% var (row 8)
		\node[fill=white,xscale=0.8,yscale=0.8] at (3.81,-5.87) {(\textbf{\ref{eq:max-min_DDistr_uniform}})};
		
		% numerator var (row 8)
		\node[fill=white,xscale=0.81,yscale=0.81] at (3.62,-6.56) {\ref{eq:uniformDqVar}};
		\end{tikzpicture}
	\end{adjustbox}
\end{table}

\begin{table}[H]
	\caption{Asymptotic estimates of means and variances for the standard $L_1$ and $L_2$ ($q=1$ and $q=2$ in Table~\ref{tab:dist_distr_general1}) distance distributions. Estimates for both standard normal ($\mathcal{N}(0,1)$) and standard uniform ($\mathcal{U}(0,1)$) data are given.}
	\label{tab:dist_distr_standardL1L2}
	\centering
	\begin{adjustbox}{center}
		\begin{tikzpicture}
		% trim=left botm right top
		\node at (0,0) {\includegraphics[clip,trim=0.27cm 0.0cm 0.0cm 0.05cm,width=0.7\textwidth]{updated_typical_metrics_table.pdf}};
		
		% standard  Lq metric (column 1 - row 1)
		\node[fill=white,xscale=0.83,yscale=0.83] at (-3.43,1.8) {\bm{$L_1$} (\textbf{Eq.}~\textbf{\ref{eq:D}})};
		
		% standard Lq metric (column 1 - row 2)
		\node[fill=white,xscale=0.83,yscale=0.83] at (-3.43,-3.1) {\bm{$L_2$} (\textbf{Eq.}~\textbf{\ref{eq:D}})};
		
		% mean (row 1)
		\node[fill=white,xscale=0.8,yscale=0.8] at (3.2,3.8) {(\textbf{\ref{eq:normalManMean}})};
		
		% var (row 2)
		\node[fill=white,xscale=0.8,yscale=0.8] at (3.58,2.58) {(\textbf{\ref{eq:normalManVar}})};
		
		% mean (row 3)
		\node[fill=white,xscale=0.8,yscale=0.8] at (3.06,1.36) {(\textbf{\ref{eq:uniformManMean}})};
		
		% var (row 4)
		\node[fill=white,xscale=0.8,yscale=0.8] at (3.13,0.123) {(\textbf{\ref{eq:uniformManVar}})};
		
		% mean (row 5)
		\node[fill=white,xscale=0.8,yscale=0.8] at (3.46,-1.1) {(\textbf{\ref{eq:normalEucMean}})};
		
		% var (row 6)
		\node[fill=white,xscale=0.8,yscale=0.8] at (3.03,-2.33) {(\textbf{\ref{eq:normalEucVar}})};
		
		% mean (row 7)
		\node[fill=white,xscale=0.8,yscale=0.8] at (3.76,-3.56) {(\textbf{\ref{eq:uniformEucMean}})};
		
		% var (row 8)
		\node[fill=white,xscale=0.8,yscale=0.8] at (3.2,-4.78) {(\textbf{\ref{eq:uniformEucVar}})};
		\end{tikzpicture}
	\end{adjustbox}
\end{table}

\begin{table}[H]
	\caption{Asymptotic estimates of means and variances for the max-min normalized $L_1$ and $L_2$ distance distributions commonly used in Relief-based algorithms. Estimates for both standard normal ($\mathcal{N}(0,1)$) and standard uniform ($\mathcal{U}(0,1)$) data are given. The cumulative distribution function of the standard normal distribution is represented by $\Phi$. Furthermore, $\mu^{(1)}_\text{max}(m)$ (Eq.~\ref{eq:med_max_normal}) is the asymptotic median of the sample maximum from $m$ standard normal random samples.}
	\label{tab:dist_distr_normalizedL1L2}
	\centering
	\begin{adjustbox}{center}
		\begin{tikzpicture}
		% trim=left botm right top
		\node at (0,0) {\includegraphics[clip,trim=0.27cm 0.0cm 0.0cm 0.05cm,width=0.85\textwidth]{updated_typical_metrics_table(normalized).pdf}};
		
		% max-min normalized L1 metric (column 1 - row 1)
		\node[fill=white,text width=1.3cm,text height=0.5cm] at (-4.535,3.05) {};
		\node[xscale=0.81,yscale=0.81] at (-4.5,3.05) {\bm{$L_1$} (\textbf{Eq.}~\textbf{\ref{eq:D*}})};
		
		% max-min normalized L2 metric (column 1 - row 1)
		\node[fill=white,text width=1.3cm,text height=0.5cm] at (-4.535,-4.68) {};
		\node[xscale=0.81,yscale=0.81] at (-4.5,-4.68) {\bm{$L_2$} (\textbf{Eq.}~\textbf{\ref{eq:D*}})};
		
		% mean (row 1)
		\node[fill=white,xscale=0.78,yscale=0.78] at (3.65,6.72) {(\textbf{\ref{eq:max-min_mean_normal_manhattan}})};
		
		% var (row 2)
		\node[fill=white,xscale=0.8,yscale=0.8] at (4.72,4.133) {(\textbf{\ref{eq:max-min_var_normal_manhattan}})};
		
		% mean (row 3)
		\node[fill=white,xscale=0.8,yscale=0.8,text width=0.8cm] at (3.53,1.45) {(\textbf{\ref{eq:max-min_mean_uniform_manhattan}})};
		
		% var (row 4)
		\node[fill=white,xscale=0.8,yscale=0.8] at (4.1,0.2) {(\textbf{\ref{eq:max-min_mean_uniform_manhattan}})};
		
		% mean (row 5)
		\node[fill=white,xscale=0.8,yscale=0.8] at (3.7,-1.045) {(\textbf{\ref{eq:max-min_mean_normal_euclidean}})};
		
		% var (row 6)
		\node[fill=white,xscale=0.8,yscale=0.8] at (4.55,-3.625) {(\textbf{\ref{eq:max-min_var_normal_euclidean}})};
		
		% mean (row 7)
		\node[fill=white,xscale=0.8,yscale=0.8] at (4.36,-6.345) {(\textbf{\ref{eq:max-min_mean_uniform_euclidean}})};
		
		% var (row 8)
		\node[fill=white,xscale=0.8,yscale=0.8] at (4.015,-7.565) {(\textbf{\ref{eq:max-min_var_uniform_euclidean}})};
		\end{tikzpicture}
	\end{adjustbox}
\end{table}

\section{GWAS distance distributions}\label{sec:gwas_distances}

In genome-wide association studies, data is encoded by the minor allele at a particular locus $a$, which is just the second most common allele (adenine-A, thymine-T, cytosine-C, or guanine-G) associated with a given feature $a$ in the data set. Features in GWAS data are single nucleotide polymorphisms (SNPs), or mutations involving the substitution, deletion, or insertion of one nucleotide at some point in the DNA sequence of an organism. These are common mutations that can affect how an individual reacts to certain pathogens or the susceptibility for certain diseases. Feature selection in GWAS is typically concerned with finding interacting SNPs that are associated with disease susceptibility \cite{li2014}.

Consider a GWAS data set, which has the following encoding based on minor allele frequency
%
\begin{equation}\label{eq:gwas_data}
X_{ia} = \begin{cases}
0 & \text{ if there are no minor alleles at locus } a,  \\
1 & \text{ if there is 1 minor allele at locus } a, \\
2 & \text{ if there are 2 minor alleles at locus } a.
\end{cases}
\end{equation}

A minor allele at a particular locus $a$ is the least frequent of the two alleles at that particular locus $a$. For random GWAS data sets, we can think $X_{ia}$ as the number of successes in two Bernoulli trials. That is, $X_{ia} \sim \mathcal{B}(2,f_a)$ where $f_a$ is the probability of success. The success probability $f_a$ is the probability of a minor allele occurring at $a$. Furthermore, the minor allele probabilities are assumed to be independent and identically distributed according to $\mathcal{U}(l,u)$, where $l$ and $u$ are the lower and upper bounds, respectively, of the sampling distribution's support. Two commonly known types of distance metrics for GWAS data are the Genotype Mismatch (GM) and Allele Mismatch (AM) metrics. The GM and AM metrics are defined by
%
\begin{equation}\label{eq:diff_GM}
\text{d}^\text{GM}_{ij}(a) = \begin{cases} 
0 & \text{ if } X_{ia} \neq X_{ja}, \\
1 & \text{ otherwise}
\end{cases}
\end{equation}
%
and
%
\begin{equation}\label{eq:diff_AM}
\text{d}^\text{AM}_{ij}(a) = \frac{1}{2}\bigl|X_{ia} - X_{ja}\bigr|.
\end{equation}

A more informative metric must take into account whether differences in allele frequency at a particular locus $a$ result from transitions or transversions. A new discrete metric that accounts for transitions (Ti) and transversions (Tv) was introduced in \cite{arabnejad2018}. This metric is given by the following
%
\begin{equation}\label{eq:diff_TiTv}
\text{d}^\text{TiTv}_{ij}(a) = \begin{cases}
0 & \text{ if } X_{ia} = X_{ja} \text{ and Ti/Tv}, \\
1/4 & \text{ if } |X_{ia} - X_{ja}|=1 \text{ and Ti}, \\
1/2 & \text{ if } |X_{ia} - X_{ja}|=1 \text{ and Tv}, \\
3/4 & \text{ if } |X_{ia} - X_{ja}|=2 \text{ and Ti}, \\
1 & \text{ if } |X_{ia} - X_{ja}|=2 \text{ and Tv}.
\end{cases}
\end{equation}

With these GWAS distance metrics, we then compute the pairwise distance between two instances $i,j \in \mathcal{I}$ with
%
\begin{equation}\label{eq:D_GM}
D^\text{GM}_{ij}(a) = \sum_{a \in \mathcal{A}} \text{d}^\text{GM}_{ij}(a),
\end{equation}
%
\begin{equation}\label{eq:D_AM}
D^\text{AM}_{ij}(a) = \sum_{a \in \mathcal{A}} \text{d}^\text{AM}_{ij}(a), \text{ or}
\end{equation}
%
\begin{equation}\label{eq:D_TiTv}
D^\text{TiTv}_{ij}(a) = \sum_{a \in \mathcal{A}} \text{d}^\text{TiTv}_{ij}(a).
\end{equation}

Assuming that all data entries $X_{ia}$ are independent and identically distributed, we have already shown that the distribution of pairwise distances is asymptotically normal regardless of data distribution and value of $q$. Therefore, the distance distributions induced by each of the GWAS metrics (Eqs.~\ref{eq:diff_GM}-\ref{eq:diff_TiTv}) are asymptotically normal. With this Gaussian limiting behavior, we will proceed by deriving the mean and variance for each distance distribution induced by these three GWAS metrics. 

\subsection{GM distance distribution}

The simplest distance metric in nearest-neighbor feature selection in GWAS data is the genotype-mismatch (GM) distance metric (Eq.~\ref{eq:D_GM}). The GM attribute diff (Eq.~\ref{eq:diff_GM}) indicates only whether two genotypes are the same or not. There are many ways two genotypes could differ, but this metric does not record this information. We will now derive the moments for the GM distance (Eq.~\ref{eq:D_GM}), which are sufficient for defining its corresponding asymptotic distribution.

The expected value of the GM attribute diff metric (Eq.~\ref{eq:diff_GM}) is given by the following
%
\begin{equation}\label{eq:mean_diff_GM}
\begin{aligned}
\text{E}\left[\text{d}^\text{GM}_{ij}(a)\right] &= \sum_{k=0}^{1} k \cdot \text{P}\left[\text{d}^\text{GM}_{ij}(a) = k\right] \\
&= 0 \cdot \text{P}\left[\text{d}^\text{GM}_{ij}(a) = 0\right] + 1 \cdot \text{P}\left[\text{d}^\text{GM}_{ij}(a) = 1\right] \\
&= \text{P}\left[\text{d}^\text{GM}_{ij}(a) = 1\right] \\
&= 2\text{P}[X_{ia} = 0, X_{ja} = 1] + 2\text{P}[X_{ia} = 1, X_{ja} = 2] \\
&\hspace{4cm}+ 2\text{P}[X_{ia} = 0, X_{ja} = 2] \\
&= 4(1 - f_a)^3f_a + 4(1 - f_a)f^3_a + 2(1 - f_a)^2f^2_a \\
&= 2\left[2(1 - f_a)^3f_a + 2(1 - f_a)f^3_a + (1 - f_a)^2f^2_a\right] \\
&= 2F^\text{GM}(a),
\end{aligned}
\end{equation}
%
where $F^\text{GM}(a) = 2(1 - f_a)^3f_a + 2(1 - f_a)f^3_a + (1 - f_a)^2f^2_a$ and $f_a$ is the probability of a minor allele occurring at locus $a$.

Then the expected pairwise GM distance between instances $i,j \in \mathcal{I}$ is given by
%
\begin{equation}\label{eq:mu_DDistr_GM}
\begin{aligned}
\text{E}\left(D^\text{GM}_{ij}\right) &= \text{E}\left(\sum_{a \in \mathcal{A}} \text{d}^\text{GM}_{ij}(a)\right) \\
&= \sum_{a \in \mathcal{A}} \text{E}\left[\text{d}^\text{GM}_{ij}(a)\right] \\
&= 2 \sum_{a \in \mathcal{A}} F^\text{GM}(a),
\end{aligned}
\end{equation}
%
where $F^\text{GM}(a) = 2(1 - f_a)^3f_a + 2(1 - f_a)f^3_a + (1 - f_a)^2f^2_a$ and $f_a$ is the probability of a minor allele occurring at locus $a$. We see that the expected GM pairwise distance (Eq.~\ref{eq:mu_DDistr_GM}) relies only on the minor allele probabilities $f_a$ for all $a \in \mathcal{A}$. In real data, we can easily determine these probabilities by dividing the total number of minor alleles at locus $a$ by the twice the number of instances $m$. To be more explicit, this is just
%
\[
f_a = \frac{1}{2m}\sum_{i \in \mathcal{I}} X_{ia},  \quad \text{ for all } a \in \mathcal{A},
\]
%
where $m$ is the number of instances (or sample size). This is because each instance has two alleles, the minor major alleles, at each locus. Therefore, the total number of alleles at locus $a$ is $2m$.

The second moment about the origin for the GM distance is computed as follows
%
\begin{equation}\label{eq:mu2_DDistr_GM}
\begin{split}
\begin{aligned}
\text{E}\left[\left(D^\text{GM}_{ij}\right)^2\right] &= \text{E}\left[\left(\sum_{a \in \mathcal{A}} \text{d}^\text{GM}_{ij}(a)\right)^2\right] \\
&= \text{E}\left[\sum_{a \in \mathcal{A}} \left(\text{d}^\text{GM}_{ij}(a)\right)^2\right] + 2 \text{E}\left[\sum_{r \in \mathcal{A}} \sum_{s \leq r - 1} \text{d}^\text{GM}_{ij}(r) \cdot \text{d}^\text{GM}_{ij}(s)\right] \\
&= \sum_{a \in \mathcal{A}} \left(\sum_{k = 0}^{1} k^2 \cdot \text{P}\left[\text{d}^\text{GM}_{ij}(a) = k\right]\right)
\end{aligned}\\
+ 2\sum_{a \in \mathcal{A}} \sum_{s \leq r - 1} \left(\sum_{k = 0}^{1} k \cdot \text{P}\left[\text{d}^\text{GM}_{ij}(r) = k\right]\right) \cdot \left(\sum_{k = 0}^{1} k \cdot \text{P}\left[\text{d}^\text{GM}_{ij}(s) = k\right]\right) \\
\begin{aligned}
&= 2\sum_{a \in \mathcal{A}} F^\text{GM}(a) + 8 \sum_{r \in \mathcal{A}} \sum_{s \leq r - 1} \prod_{\lambda \in \{r,s\}} F^\text{GM}(\lambda),
\end{aligned}
\end{split}
\end{equation}
%
where $F^\text{GM}(a) = 2(1 - f_a)^3f_a + 2(1 - f_a)f^3_a + (1 - f_a)^2f^2_a$ and $f_a$ is the probability of a minor allele occurring at locus $a$.

Using the first (Eq.~\ref{eq:mu_DDistr_GM}) and second (Eq.~\ref{eq:mu2_DDistr_GM}) raw moments of the GM distance, the variance is given by
%
\begin{equation}\label{eq:var_DDistr_GM}
\begin{aligned}
\text{Var}\left(D^\text{GM}_{ij}\right) &= \text{E}\left[\left(D^\text{GM}_{ij}\right)^2\right] - \left[\text{E}\left(D^\text{GM}_{ij}\right)\right]^2 \\
&= 2\sum_{a \in \mathcal{A}} F^\text{GM}(a) + 8\sum_{r \in \mathcal{A}} \sum_{s \leq r - 1} \prod_{\lambda \in \{r,s\}} F^\text{GM}(\lambda) \\
&\hspace{4cm}- 4\left(\sum_{a \in \mathcal{A}}F^\text{GM}(a)\right)^2 \\
&= 2\sum_{a \in \mathcal{A}} F^\text{GM}(a) - 4\sum_{a \in \mathcal{A}}\left[F^\text{GM}(a)\right]^2 \\
&= 2\sum_{a \in \mathcal{A}} F^\text{GM}(a)[1 - 2F^\text{GM}(a)],
\end{aligned}
\end{equation}
%
where $F^\text{GM}(a) = 2(1 - f_a)^3f_a + 2(1 - f_a)f^3_a + (1 - f_a)^2f^2_a$ and $f_a$ is the probability of a minor allele occurring at locus $a$. Hence, the variance of the asymptotic GM distance distribution also just depends on the minor allele probabilities $f_a$ for all $a \in \mathcal{A}$. This implies that the limiting GM distance distribution is fully determined by the minor allele probabilites, which are known in real data.

With the mean and variance estimates (Eqs.~\ref{eq:mu_DDistr_GM} and \ref{eq:var_DDistr_GM}), the asymptotic GM distance distribution is given by the following
%
\begin{equation}\label{eq:DDistr_GM}
D^\text{GM}_{ij} \overset{.}{\sim} \mathcal{N}\left(2\sum_{a \in \mathcal{A}} F^\text{GM}(a), 2\sum_{a \in \mathcal{A}} F^\text{GM}(a)[1 - 2F^\text{GM}(a)]\right),
\end{equation}
%
where $F^\text{GM}(a) = 2(1 - f_a)^3f_a + 2(1 - f_a)f^3_a + (1 - f_a)^2f^2_a$ and $f_a$ is the probability of a minor allele occurring at locus $a$.

We have concluded our treatment of GM distances in random GWAS data, where the independence assumption of holds for minor allele probabilites $f_a$ and binomial samples $X_{ia} \sim \mathcal{B}(2,f_a)$ for all $a \in \mathcal{A}$. In the next section, we consider the AM distance distribution in GWAS data. This metric incorporates genotype differences at the level of individual allelic differences, so it is more informative than the GM metric.

\subsection{AM distance distribution}

As we have mentioned previously, the AM attribute diff metric (Eq.~\ref{eq:diff_AM}) is slightly more dynamic than the GM metric because the AM metric accounts for differences between the alleles of two genotypes. In this section, we derive moments of the AM distance metric (Eq.~\ref{eq:D_AM}) that adequately define its corresponding asymptotic distribution.

The expected value of the AM attribute diff metric (Eq.~\ref{eq:diff_AM}) is given by the following
%
\begin{equation}\label{eq:mean_diff_AM}
\begin{aligned}
\text{E}\left[\text{d}^\text{AM}_{ij}(a)\right] &= \sum_{k \in \mathcal{D}} k \cdot \text{P}\left[\text{d}^\text{AM}_{ij}(a) = k\right] \\
&= 0 \cdot \text{P}\left[\text{d}^\text{AM}_{ij}(a) = 0\right] + \frac{1}{2} \cdot \text{P}\left[\text{d}^\text{AM}_{ij}(a) = \frac{1}{2}\right] \\
&\hspace{3.7cm}+ 1 \cdot \text{P}\left[\text{d}^\text{AM}_{ij}(a) = 1\right] \\
&= \frac{1}{2}\left(2 \text{P}\left[X_{ia} = 0, X_{ja} = 1\right] + 2 \text{P}\left[X_{ia} = 1, X_{ja} = 2\right]\right) \\
&\hspace{4.8cm}+ 2 \text{P}\left[X_{ia} = 0, X_{ja} = 2\right] \\
&= \text{P}\left[X_{ia} = 0, X_{ja} = 1\right] + \text{P}\left[X_{ia} = 1, X_{ja} = 2\right] \\
&\hspace{4cm}+ 2 \text{P}\left[X_{ia} = 0, X_{ja} = 2\right] \\
&= 2(1 - f_a)^3f_a + 2(1 - f_a)f^3_a + 2(1 - f_a)^2 f^2_a \\
&= 2\left[(1 - f_a)^3f_a + (1 - f_a)f^3_a + (1 - f_a)^2 f^2_a\right] \\
&= 2F^\text{AM}(a),
\end{aligned}
\end{equation}
%
where $F^\text{AM}(a) = (1 - f_a)^3f_a + (1 - f_a)f^3_a + (1 - f_a)^2 f^2_a$, $\mathcal{D} = \{0,1/2,1\}$, and $f_a$ is the probability of a minor allele occurring at locus $a$.

Using the expected AM attribute diff (Eq.~\ref{eq:mean_diff_AM}), the expected pairwise AM distance (Eq.~\ref{eq:D_AM}) between instances $i,j \in \mathcal{I}$ is given by
%
\begin{equation}\label{eq:mu_DDistr_AM}
\begin{aligned}
\text{E}\left(D^\text{AM}_{ij}\right) &= \text{E}\left(\sum_{a \in \mathcal{A}} \text{d}^\text{AM}_{ij}(a)\right) \\
&= \sum_{a \in \mathcal{A}} \text{E}\left[\text{d}^\text{AM}_{ij}(a)\right] \\
&= 2 \sum_{a \in \mathcal{A}} F^\text{AM}(a).
\end{aligned}
\end{equation}
%
where $F^\text{AM}(a) = (1 - f_a)^3f_a + (1 - f_a)f^3_a + (1 - f_a)^2 f^2_a$ and $f_a$ is the probability of a minor allele occurring at locus $a$. Similar to GM distances, the expected AM distance (Eq.~\ref{eq:mu_DDistr_AM}) depends only on the minor allele probabilities $f_a$ for all $a \in \mathcal{A}$. This is to be expected because, although the AM metric is more informative, it still only accounts for simple differences between nucleotides of two instances $i,j \in \mathcal{I}$ at some locus $a$.

The second moment about the origin for the AM distance is computed as follows
%
\begin{equation}\label{eq:mu2_DDistr_AM}
\begin{split}
\begin{aligned}
\text{E}\left[\left(D^\text{AM}_{ij}\right)^2\right] &= \text{E}\left[\left(\sum_{a \in \mathcal{A}} \text{d}^\text{AM}_{ij}(a)\right)^2\right] \\
&= \text{E}\left[\sum_{a \in \mathcal{A}} \left(\text{d}^\text{AM}_{ij}(a)\right)^2\right] + 2 \text{E}\left[\sum_{r \in \mathcal{A}} \sum_{s \leq r - 1} \text{d}^\text{AM}_{ij}(r) \cdot \text{d}^\text{AM}_{ij}(s)\right] \\
&= \sum_{a \in \mathcal{A}} \left(\sum_{k \in \mathcal{D}} k^2 \cdot \text{P}\left[\text{d}^\text{AM}_{ij}(a) = k\right]\right)
\end{aligned}\\
+ 2\sum_{a \in \mathcal{A}} \sum_{s \leq r - 1} \left(\sum_{k \in \mathcal{D}} k \cdot \text{P}\left[\text{d}^\text{AM}_{ij}(r) = k\right]\right) \cdot \left(\sum_{k \in \mathcal{D}} k \cdot \text{P}\left[\text{d}^\text{AM}_{ij}(s) = k\right]\right) \\
\begin{aligned}
&= \sum_{a \in \mathcal{A}} G^\text{AM}(a) + 8 \sum_{r \in \mathcal{A}} \sum_{s \leq r - 1} \prod_{\lambda \in \{r,s\}} F^\text{AM}(\lambda),
\end{aligned}
\end{split}
\end{equation}
%
where $G^\text{AM}(a) = (1 - f_a)^3 f_a + f^3_a (1 - f_a) + 2 (1 - f_a)^2 f^2_a$, $F^\text{AM}(\lambda) = (1 - f_\lambda)^3 f_\lambda + f^3_\lambda (1 - f_\lambda) + (1 - f_\lambda)^2 f^2_\lambda$, and $f_a$ is the probability of a minor allele occurring at locus $a$.

Using the first (Eq.~\ref{eq:mu_DDistr_AM}) and second (Eq.~\ref{eq:mu2_DDistr_AM}) raw moments of the asymptotic AM distance distribution, the variance is given by
%
\begin{equation}\label{eq:var_DDistr_AM}
\begin{aligned}
\text{Var}\left(D^\text{AM}_{ij}\right) &= \text{E}\left[\left(D^\text{AM}_{ij}\right)^2\right] - \left[\text{E}\left(D^\text{AM}_{ij}\right)\right]^2 \\
&= \sum_{a \in \mathcal{A}} G^\text{AM}(a) + 8\sum_{r \in \mathcal{A}} \sum_{s \leq r - 1} \prod_{\lambda \in \{r,s\}} F^\text{AM}(\lambda) - 4\left(\sum_{a \in \mathcal{A}}F^\text{AM}(a)\right)^2 \\
&= \sum_{a \in \mathcal{A}} G^\text{AM}(a) - 4\sum_{a \in \mathcal{A}}\left[F^\text{AM}(a)\right]^2 \\
&= \sum_{a \in \mathcal{A}} \left(G^\text{AM}(a)- 4\left[F^\text{AM}(a)\right]^2\right),
\end{aligned}
\end{equation}
%
where $G^\text{AM}(a) = (1 - f_a)^3 f_a + f^3_a (1 - f_a) + 2 (1 - f_a)^2 f^2_a$, $F^\text{AM}(a) = (1 - f_a)^3 f_a + f^3_a (1 - f_a) + (1 - f_a)^2 f^2_a$, and $f_a$ is the probability of a minor allele occurring at locus $a$. Similar to the mean (Eq.~\ref{eq:mu_DDistr_AM}), the variance just depends on minor allele probabilities $f_a$ for all $a \in \mathcal{A}$.

With the mean (Eq.~\ref{eq:mu_DDistr_AM}) and variance (Eq.~\ref{eq:var_DDistr_AM}) estimates of AM distances, the asymptotic AM distance distribution is given by the following
%
\begin{equation}\label{eq:DDistr_AM}
D^\text{AM}_{ij} \overset{.}{\sim} \mathcal{N}\left(2\sum_{a \in \mathcal{A}} F^\text{AM}(a), \sum_{a \in \mathcal{A}} \left(G^\text{AM}(a)- 4\left[F^\text{AM}(a)\right]^2\right)\right),
\end{equation}
%
where $G^\text{AM}(a) = (1 - f_a)^3 f_a + f^3_a (1 - f_a) + 2 (1 - f_a)^2 f^2_a$, $F(a) = (1 - f_a)^3 f_a + f^3_a (1 - f_a) + (1 - f_a)^2 f^2_a$, and $f_a$ is the probability of a minor allele occurring at locus $a$.

This concludes our analysis of the AM metric in GWAS data when the independence assumption holds for minor allele probabilities $f_a$ and binomial samples $\mathcal{B}(2,f_a)$ for all $a \in \mathcal{A}$. In the next section, we derive more complex asymptotic results for the TiTv distance metric (Eq.~\ref{eq:D_TiTv}).

\subsection{TiTv distance distribution}\label{sec:TiTv_distances}

The TiTv metric allows for one to account for both genotype mismatch, allele mismatch, transition, and transversion. However, this added dimension of information requires knowledge of the nucleotide makeup at a particular locus. A sufficient condition to compute the TiTv metric between instances $i,j \in \mathcal{I}$ is that we know whether the nucleotides associated with a particular locus $a$ are both purines (PuPu), purine and pyrimidine (PuPy), or both pyrimidines (PyPy). We illustrate all possibilities for transitions and transversions in a diagram (Fig.~\ref{fig:TiTv_diagram}). Purines (A and G) and pyrimidines (C and T) are shown at the top and bottom, respectively. Transitions occur in the cases of PuPu and PyPy, while transversion occurs only with PuPy encoding.

\begin{figure}[H]
\begin{minipage}[c]{0.62\textwidth}
	\framebox{\includegraphics[width=0.98\textwidth]{TiTv_diagram2.pdf}}
\end{minipage}\hfill
\begin{minipage}[c]{0.34\textwidth}
	\caption{Purines (A and G) and pyrimidines (C and T) are shown. Transitions occur when a mutation involves purine-to-purine or pyrimidine-to-pyrimidine insertion. Transversions occur when a purine-to-pyrimidine or pyrimidine-to-purine insertion happens, which is a more extreme case. There are visibly more possibilities for transversions to occur than there are transitions, but there are about twice as many transitions in real data.}\label{fig:TiTv_diagram}
\end{minipage}
\end{figure}

This additional encoding is always given in a particular GWAS data set, which leads us to consider the probabilities of PuPu, PuPy, and PyPy. These will be necessary to determine asymptotics for the TiTv distance metric. Let $\gamma_0$, $\gamma_1$, and $\gamma_2$ denote the probabilities of PuPu, PuPy, and PyPy, respectively, for the $p$ loci of data matrix $X$. In real data, there are approximately twice as many transitions as there are transversions. That is, the probability of a transition $\text{P}(\text{Ti})$ is approximately twice the probability of transversion $\text{P}(\text{Tv})$. It is likely that any particular data set will not satisfy this criterion exactly. In this general case, we have $\text{P}(\text{Ti})$ being equal to some multiple $\eta$ times $\text{P}(\text{Tv})$. In order to enforce this general constraint in simulated data, we define the following set of equalities
%
\begin{alignat}{2}\label{eq:TiTv_constraints1}
\gamma_0 + \gamma_1 + \gamma_2 &= 1, \\ \label{eq:TiTv_constraints2}
\text{P}(\text{Ti}) - \eta \text{P}(\text{Tv}) &= 0.
\end{alignat}

The sum-to-one constraint (Eq.~\ref{eq:TiTv_constraints1}) is natural in this context because there are only three possible genotype encodings at a particular locus, which are PuPu, PuPy, and PyPy. Solving the Ti/Tv ratio constraint (Eq.~\ref{eq:TiTv_constraints2}) for $\eta$ gives
%
\[
\eta = \frac{\text{P}(\text{Ti})}{\text{P}(\text{Tv})},
\]
%
which is easily computed in a real data set by dividing the fraction of Ti out of the total $p$ loci by the fraction of Tv out of the total $p$ loci. We will use the simplified notation $\eta=\text{Ti}/\text{Tv}$ to represent this factor for the remainder of this work.

Using this PuPu, PuPy, and PyPy encoding, the probability of a transversion occuring at any fixed locus $a$ is given by the following
%
\begin{equation}\label{eq:prob_Tv}
\begin{aligned}
\text{P}(\text{Tv}) &= \gamma_1.
\end{aligned}
\end{equation}

Using the sum-to-one constraint (Eqs.~\ref{eq:TiTv_constraints1}) and the probability of

\noindent transversion (Eq.~\ref{eq:TiTv_constraints2}), the probability of a transition occurring at locus $a$ is computed as follows
%
\begin{equation}\label{eq:prob_Ti}
\begin{aligned}
\text{P}(\text{Ti}) &= \gamma_0 + \gamma_2.
\end{aligned}
\end{equation}

Also using the sum-to-one constraint (Eq.~\ref{eq:TiTv_constraints1}) and the Ti/Tv ratio constraint (Eq.~\ref{eq:TiTv_constraints2}), it is clear that we have $\text{P}(\text{Tv}) = \frac{1}{\eta + 1}$ and $\text{P}(\text{Ti}) = \frac{\eta}{\eta + 1}$. Without loss of generality, we then sample 
%
\begin{equation}\label{eq:gamma0}
\gamma_0 \sim \mathcal{U}\left(\varepsilon,\frac{\eta}{\eta + 1} - \varepsilon\right),
\end{equation}
%
where $\varepsilon$ is some small positive real number.

Then it immediately follows that we have 
%
\begin{equation}\label{eq:gamma2}
\gamma_2 = \frac{\eta}{\eta + 1} - \gamma_0.
\end{equation}

However, we can derive the mean and variance of the distance distribution induced by the TiTv metric without specifying any relationship between $\gamma_0$, $\gamma_1$, and $\gamma_2$. We proceed by computing $\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = k\right]$ for each $k \in \mathcal{D} = \bigl\{0,\frac{1}{4},\frac{1}{2},\frac{3}{4},1\bigr\}$. Let $y$ represent a random sample of size $p$ from $\{0,1,2\}$, where 
%
\begin{equation}\label{eq:yvec}
y_a = \begin{cases}
0 & \text{if locus } a \text{ is PuPu}, \\
1 & \text{if locus } a \text{ is PuPy}, \\
2 & \text{if locus } a \text{ is PyPy}.
\end{cases}
\end{equation}

We derive $\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = 0\right]$ as follows
%
\begin{equation}\label{eq:prob_TiTv_0}
\begin{aligned}
\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = 0\right] &= \text{P}\left[y_a = 0, X_{ia} = X_{ja}\right] \\
&+ \text{P}\left[y_a = 1, X_{ia} = X_{ja}\right] \\
&+ \text{P}\left[y_a = 2, X_{ia} = X_{ja}\right] \\
&= \gamma_0 \left[(1 - f_a)^2 + 4 f_a (1 - f_a) + f^2_a\right] \\
&+ \gamma_1 \left[(1 - f_a)^2 + 4 f_a (1 - f_a) + f^2_a\right] \\
&+ \gamma_2 \left[(1 - f_a)^2 + 4 f_a (1 - f_a) + f^2_a\right] \\
&= (\gamma_0 + \gamma_1 + \gamma_2)\left[(1 - f_a)^2 + 4 f_a (1 - f_a) + f^2_a\right] \\
&= (1 - f_a)^2 + 4 f_a (1 - f_a) + f^2_a,
\end{aligned}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$.

We derive $\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = \frac{1}{4}\right]$ as follows
%
\begin{equation}\label{eq:prob_TiTv_0.25}
\begin{aligned}
\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = \frac{1}{4}\right] &= 2 \text{P}\left[y_a = 0, X_{ia} = 0, X_{ja} = 1\right] \\
&+ 2 \text{P}\left[y_a = 0, X_{ia} = 1, X_{ja} = 2\right] \\
&+ 2 \text{P}\left[y_a = 2, X_{ia} = 0, X_{ja} = 1\right] \\
&+ 2 \text{P}\left[y_a = 2, X_{ia} = 1, X_{ja} = 2\right] \\
&= 4 \gamma_0 (1 - f_a)^3 f_a + 4 \gamma_0 f^3_a (1 - f_a) + 4 \gamma_2 (1 - f_a)^3 f_a \\
&+ 4 \gamma_2 f^3_a (1 - f_a) \\
&= 4 \gamma_0 \left[(1 - f_a)^3 f_a + f^3_a (1 - f_a)\right] \\
&+ 4 \gamma_2 \left[(1 - f_a)^3 f_a + f^3_a (1 - f_a)\right] \\
&= 4(\gamma_0 + \gamma_2)\left[(1 - f_a)^3 f_a + f^3_a (1 - f_a)\right],
\end{aligned}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$, $\gamma_0$ is the probability of PuPu occurring at any locus $a$,  and $\gamma_2$ is the probability of PyPy occurring at any locus $a$.

We derive $\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = \frac{1}{2}\right]$ as follows
%
\begin{equation}\label{eq:prob_TiTv_0.5}
\begin{aligned}
\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = \frac{1}{2}\right] &= 2 \text{P}\left[y_a = 1, X_{ia} = 0, X_{ja} = 1\right] \\
&+ 2 \text{P}\left[y_a = 1, X_{ia} = 1, X_{ja} = 2\right] \\
&= 4 \gamma_1 (1 - f_a)^3 f_a + 4 \gamma_1 f^3_a (1 - f_a) \\
&= 4 \gamma_1 \left[(1 - f_a)^3 f_a + f^3_a (1 - f_a)\right],
\end{aligned}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$ and $\gamma_1$ is the probability of PuPy occurring at any locus $a$.

We derive $\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = \frac{3}{4}\right]$ as follows
%
\begin{equation}\label{eq:prob_TiTv_0.75}
\begin{aligned}
\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = \frac{3}{4}\right] &= 2 \text{P}\left[y_a = 0, X_{ia} = 0, X_{ja} = 2\right] \\
&+ 2 \text{P}\left[y_a = 2, X_{ia} = 0, X_{ja} = 2\right] \\
&= 2 \gamma_0 (1 - f_a)^2 f^2_a + 2 \gamma_2 (1 - f_a)^2 f^2_a \\
&= 2(\gamma_0 + \gamma_2)(1 - f_a)^2 f^2_a,
\end{aligned}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$, $\gamma_0$ is the probability of PuPu occurring at any locus $a$, and $\gamma_2$ is the probability of PyPy occurring at any locus $a$.

We derive $\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = 1\right]$ as follows
%
\begin{equation}\label{eq:prob_TiTv_1}
\begin{aligned}
\text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = 1\right] &= 2 \text{P}\left[y_a = 1, X_{ia} = 0, X_{ja} = 2\right] \\
&= 2 \gamma_1 (1 - f_a)^2 f^2_a,
\end{aligned}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$ and $\gamma_1$ is the probability of PuPy occurring at any locus $a$.

Using the TiTv diff probabilities (Eqs.~\ref{eq:prob_TiTv_0}-\ref{eq:prob_TiTv_1}), we compute the expected TiTv distance between instances $i,j \in \mathcal{I}$ as follows
%
\begin{equation}\label{eq:mu_DDistr_TiTv}
\begin{aligned}
\text{E}\left(D^\text{TiTv}_{ij}\right) &= \sum_{a \in \mathcal{A}} \left(\sum_{k \in \mathcal{D}} k \cdot \text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = k\right]\right) \\
&= (\gamma_0 + \gamma_2 + 2\gamma_1) \sum_{a \in \mathcal{A}} \left[(1 - f_a)^3 f_a + f^3_a (1 - f_a)\right] \\
&+ \left[\frac{3}{2}(\gamma_0 + \gamma_2) + 2\gamma_1\right] \sum_{a \in \mathcal{A}} (1 - f_a)^2 f^2_a \\
&= (\gamma_0 + \gamma_2 + 2\gamma_1) \sum_{a \in \mathcal{A}} F^\text{TiTv}(a)\\
&\hspace{1.5cm} + \left[\frac{3}{2}(\gamma_0 + \gamma_2) + 2\gamma_1\right] \sum_{a \in \mathcal{A}} G^\text{TiTv}(a),
\end{aligned}
\end{equation}
%
where $F^\text{TiTv}(a) = (1 - f_a)^3 f_a + f^3_a (1 - f_a)$, $G^\text{TiTv}(a) = (1 - f_a)^2 f^2_a$, $f_a$ is the probability of a minor allele occurring at locus $a$, $\gamma_0$ is the probability of PuPu occurring at any locus $a$, $\gamma_1$ is the probability of PuPy occurring at any locus $a$, and $\gamma_2$ is the probability of PyPy occurring at any locus $a$. In contrast to the expected GM and AM distances (Eqs.~\ref{eq:mu_DDistr_GM} and \ref{eq:mu_DDistr_AM}), the expected TiTv distance (Eq.~\ref{eq:mu_DDistr_TiTv}) depends on minor allele probabilities $f_a$ for all $a \in \mathcal{A}$ and the genotype encoding probabilities $\gamma_0, \gamma_1, \text{ and } \gamma_2$. 

The second moment about the origin for the TiTv distance is computed as follows
%
\begin{equation}\label{eq:mu2_DDistr_TiTv}
\begin{split}
\begin{aligned}
\text{E}\left[\left(D^\text{TiTv}_{ij}\right)^2\right] &= \text{E}\left[\left(\sum_{a \in \mathcal{A}} \text{d}^\text{TiTv}_{ij}(a)\right)^2\right] \\
&= \text{E}\left[\sum_{a \in \mathcal{A}} \left(\text{d}^\text{TiTv}_{ij}(a)\right)^2\right] + 2 \text{E}\left[\sum_{r \in \mathcal{A}} \sum_{s \leq r - 1} \text{d}^\text{TiTv}_{ij}(r) \cdot \text{d}^\text{TiTv}_{ij}(s)\right] \\
&= \sum_{a \in \mathcal{A}} \left(\sum_{k \in \mathcal{D}} k^2 \cdot \text{P}\left[\text{d}^\text{TiTv}_{ij}(a) = k\right]\right)
\end{aligned}\\
+ 2\sum_{a \in \mathcal{A}} \sum_{s \leq r - 1} \left(\sum_{k \in \mathcal{D}} k \cdot \text{P}\left[\text{d}^\text{TiTv}_{ij}(r) = k\right]\right) \cdot \left(\sum_{k \in \mathcal{D}} k \cdot \text{P}\left[\text{d}^\text{TiTv}_{ij}(s) = k\right]\right) \\
\begin{aligned}
&= \left[\frac{1}{4}(\gamma_0 + \gamma_2) + \gamma_1\right] \sum_{a \in \mathcal{A}} F^\text{TiTv}(a) + \left[\frac{9}{8}(\gamma_0 + \gamma_2) + 2\gamma_1\right] \sum_{a \in \mathcal{A}} G^\text{TiTv}(a) \\
&+ 2 \sum_{r \in \mathcal{A}} \sum_{s \leq r - 1} \prod_{\lambda \in \{r,s\}} {\Biggl\{}{\biggl(}[\gamma_0 + \gamma_2 + 2\gamma_1] F^\text{TiTv}(\lambda) 
\end{aligned}\\
+ \left[\frac{3}{2}(\gamma_0 + \gamma_2) + 2\gamma_1\right] G^\text{TiTv}(\lambda){\biggr)}{\Biggr\}},
\end{split}
\end{equation}
%
where $F^\text{TiTv}(a) = (1 - f_a)^3 f_a + f^3_a (1 - f_a)$, $G^\text{TiTv}(a) = (1 - f_a)^2 f^2_a$, $f_a$ is the probability of a minor allele occurring at locus $a$, $\gamma_0$ is the probability of PuPu occurring at any locus $a$, $\gamma_1$ is the probability of PuPy occurring at any locus $a$, and $\gamma_2$ is the probability of PyPy occurring at any locus $a$.

Using the first (Eq.~\ref{eq:mu_DDistr_TiTv}) and second (Eq.~\ref{eq:mu2_DDistr_TiTv}) raw moments of the TiTv distance, the variance is given by
%
\begin{equation}\label{eq:var_DDistr_TiTv}
\begin{split}
\begin{aligned}
\text{Var}\left(D^\text{TiTv}_{ij}\right) &= \text{E}\left[\left(D^\text{TiTv}_{ij}\right)^2\right] - \left[\text{E}\left(D^\text{TiTv}_{ij}\right)\right]^2 \\
&=\left[\frac{1}{4}(\gamma_0 + \gamma_2) + \gamma_1\right] \sum_{a \in \mathcal{A}} F^\text{TiTv}(a) \\
&+ \left[\frac{9}{8}(\gamma_0 + \gamma_2) + 2\gamma_1\right] \sum_{a \in \mathcal{A}} G^\text{TiTv}(a)\\
&+ 2 \sum_{r \in \mathcal{A}} \sum_{s \leq r - 1} \prod_{\lambda \in \{r,s\}} {\Biggl\{}{\biggl(}[\gamma_0 + \gamma_2 + 2\gamma_1] F^\text{TiTv}(\lambda)\hphantom{space space spa}
\end{aligned}\\
+ \left[\frac{3}{2}(\gamma_0 + \gamma_2) + 2\gamma_1\right] G^\text{TiTv}(\lambda){\biggr)}{\Biggr\}} \\
\begin{aligned}
&- {\Biggl\{}{\biggl(}[\gamma_0 + \gamma_2 + 2\gamma_1] \sum_{a \in \mathcal{A}} F^\text{TiTv}(a) \\
&\hspace{4cm}+ \left[\frac{3}{2}(\gamma_0 + \gamma_2) + 2\gamma_1\right] \sum_{a \in \mathcal{A}} G^\text{TiTv}(a){\biggr)}{\Biggr\}}^2 \\
&=\left[\frac{1}{4}(\gamma_0 + \gamma_2) + \gamma_1\right] \sum_{a \in \mathcal{A}} F^\text{TiTv}(a) + \left[\frac{9}{8}(\gamma_0 + \gamma_2) + 2\gamma_1\right] \sum_{a \in \mathcal{A}} G^\text{TiTv}(a) \\
&- \sum_{a \in \mathcal{A}} \left([\gamma_0 + \gamma_2 + 2\gamma_1] F^\text{TiTv}(a) + \left[\frac{3}{2}(\gamma_0 + \gamma_2) + 2\gamma_1\right] G^\text{TiTv}(a)\right)^2,
\end{aligned}
\end{split}
\end{equation}
%
where $F^\text{TiTv}(a) = (1 - f_a)^3 f_a + f^3_a (1 - f_a)$, $G^\text{TiTv}(a) = (1 - f_a)^2 f^2_a$, $f_a$ is the probability of a minor allele occurring at locus $a$, $\gamma_0$ is the probability of PuPu occurring at any locus $a$, $\gamma_1$ is the probability of PuPy occurring at any locus $a$, and $\gamma_2$ is the probability of PyPy occurring at any locus $a$.

With the mean (Eq.~\ref{eq:mu_DDistr_TiTv}) and variance (Eq.~\ref{eq:var_DDistr_TiTv}) estimates, the asymptotic TiTv distance distribution is given by the following
%
\begin{equation}\label{eq:DDistr_TiTv}
\begin{aligned}
D^\text{TiTv}_{ij} \overset{.}{\sim} \mathcal{N}{\fontsize{0.5cm}{1pt}\selectfont \Biggl(}& {\Biggl\{}(\gamma_0 + \gamma_2 + 2\gamma_1) \sum_{a \in \mathcal{A}} F^\text{TiTv}(a) \\
&+ \left[\frac{3}{2}(\gamma_0 + \gamma_2) + 2\gamma_1\right] \sum_{a \in \mathcal{A}} G^\text{TiTv}(a){\Biggr\}}, \\
&{\Biggl\{}\left[\frac{1}{4}(\gamma_0 + \gamma_2) + \gamma_1\right] \sum_{a \in \mathcal{A}} F^\text{TiTv}(a) \\
&+ \left[\frac{9}{8}(\gamma_0 + \gamma_2) + 2\gamma_1\right] \sum_{a \in \mathcal{A}} G^\text{TiTv}(a) \\
&- \sum_{a \in \mathcal{A}} {\biggl(}[\gamma_0 + \gamma_2 + 2\gamma_1] F^\text{TiTv}(a) \\
&+ \left[\frac{3}{2}(\gamma_0 + \gamma_2) + 2\gamma_1\right] G^\text{TiTv}(a){\biggr)}^2{\Biggr\}}{\fontsize{0.5cm}{1pt}\selectfont \Biggr)},
\end{aligned}
\end{equation}
%
where $F^\text{TiTv}(a) = (1 - f_a)^3 f_a + f^3_a (1 - f_a)$, $G^\text{TiTv}(a) = (1 - f_a)^2 f^2_a$, $f_a$ is the probability of a minor allele occurring at locus $a$, $\gamma_0$ is the probability of PuPu occurring at any locus $a$, $\gamma_1$ is the probability of PuPy occurring at any locus $a$, and $\gamma_2$ is the probability of PyPy occurring at any locus $a$.

Given upper and lower bounds $l$ and $u$, respectively, of the success probability sampling interval, the average success probability (or average MAF) is computed as follows
%
\begin{equation}\label{eq:avg_maf}
\bar{f}_a = \frac{1}{2}(l + u).
\end{equation}

The maximum TiTv distance occurs at $\bar{f}_a=0.5$ for any fixed Ti/Tv ratio $\eta$ (Eq.~\ref{eq:TiTv_constraints2}), which is the inflection point about which the minor allele changes at locus $a$ (Fig.~\ref{fig:TiTv-vs-maf}). If few minor alleles are present ($\bar{f}_a \to 0$), the predicted TiTv distance approaches 0. The same is true after the minor allele switches ($\bar{f}_a \to 1$). To explore how TiTv distance changes with increased minor allele frequency, we fixed the Ti/Tv ratio $\eta$ and generated simulated TiTv distances for $\bar{f}_a = 0.055, 0.150, 0.250, \text{ and } 0.350$ (Fig.~\ref{fig:TiTv_ridge}A). For fixed $\eta$, TiTv distance increases significantly with increased $\bar{f}_a$. We similarly fixed the average minor allele frequency $\bar{f}_a$ and generated simulated TiTv distances for $\eta = \text{Ti/Tv} = 0.5, 1, 1.5, \text{ and } 2$ (Fig.~\ref{fig:TiTv_ridge}C). The TiTv distance decreases slightly with increased $\eta = \text{Ti/Tv}$. As $\eta \to 0^+$, the data is approaching all Tv and no Ti, which means the TiTv distance is larger by definition. On the other hand, the TiTv distance decreases as $\eta \to 2^-$ because the data is approaching approximately twice as many Ti as there are Tv, which is typical for GWAS data in humans.

% made with simulate_TiTv_distances.R and LaTeX
\begin{figure}[H]
\begin{minipage}[c]{0.65\textwidth}
	\includegraphics[width=0.98\textwidth]{TiTv_distance-vs-maf.pdf}
\end{minipage}\hfill
\begin{minipage}[c]{0.35\textwidth}
	\caption{Predicted average TiTv distance as a function of average minor allele frequency $\bar{f}_a$ (see Eq.~\ref{eq:avg_maf}). Success probabilities $f_a$ are drawn from a sliding window interval from 0.01 to 0.9 in increments of about 0.009 and $m=p=100$. For $\eta=0.1$, where $\eta$ is the Ti/Tv ratio given by Eq.~\ref{eq:TiTv_constraints1}, Tv is ten times more likely than Ti and results in larger distance. Increasing to $\eta=1$, Tv and Ti are equally likely and the distance is lower.  In line with real data for $\eta=2$, Tv is half as likely as Ti so the distances are relatively small.}\label{fig:TiTv-vs-maf}
\end{minipage}
\end{figure}

We also compared theoretical and sample moments as a function of $\eta = \text{Ti/Tv}$ and $\bar{f}_a$ for the TiTv distance metric (Fig.~\ref{fig:TiTv_ridge}B and D). We fixed $\bar{f}_a$ and computed the theoretical and simulated moments as a function of $\eta$ (Fig.~\ref{fig:TiTv_ridge}B). Theoretical average TiTv distance, given by Eq.~\ref{eq:mu_DDistr_TiTv}, and simulated TiTv average distance are approximately equal as $\eta$ increases. Theoretical standard deviation, given by Eq.~\ref{eq:var_DDistr_TiTv}, and simulated TiTv standard deviation differ slightly. We also fixed $\eta$ and computed theoretical and sample moments as a function of $\bar{f}_a$ (Fig.~\ref{fig:TiTv_ridge}D). In this case, there is approximate agreement with simulated and theoretical moments as $\bar{f}_a$ increases.

\begin{figure}[H]
	\centering
	\framebox{\includegraphics[width=0.98\textwidth]{re_fig_5_BDedit.pdf}}
	\caption{Density curves and moments of TiTv distance as a function of average MAF $\bar{f}_a$, given by Eq.~\ref{eq:avg_maf}, and Ti/Tv ratio $\eta$, given by Eq.~\ref{eq:TiTv_constraints2}. We fix $m=p=100$ for all simulated TiTv distances. (\textbf{A}) For fixed $\bar{f}_a=0.055$, TiTv distance density is plotted as a function of increasing $\eta$. TiTv distance decreases as $\eta$ increases. For $\eta=\text{Ti/Tv}=0.5$, there are twice as many transversions as there are transitions. On the other hand, $\eta=\text{Ti/Tv}=2$ indicates that there are half as many transversions as transitions. Since transversions encode a larger magnitude distance than transitions, this behavior is expected. (\textbf{B}) Simulated and predicted mean $\pm$ SD are shown as a function of increasing Ti/Tv ratio $\eta$. Distance decreases as Ti/Tv increases. Theoretical and simulated moments are approximately the same. (\textbf{C}) For fixed $\eta=2$, TiTv distance density is plotted as a function of increasing $\bar{f}_a$. TiTv distance increases as $\bar{f}_a$ approaches maximum of 0.5, which means that there is about the same frequency of minor alleles as major alleles. (\textbf{D}) Simulated and predicted mean $\pm$ SD as a function of increasing average MAF $\bar{f}_a$. Distance increases as the number of minor alleles increases. Theoretical and simulated moments are approximately the same.}\label{fig:TiTv_ridge}
\end{figure}

We conclude our treatment of GWAS distance metrics (Eqs.~\ref{eq:D_GM}-\ref{eq:D_TiTv}) by summarizing moment estimates in a table (Table~\ref{tab:dist_distr_gwas}) that is organized by metric, statistic (mean or variance), and asymptotic formula. In the next section, we focus on the distributions of GWAS attribute diff metrics (Eqs.~\ref{eq:diff_GM}-\ref{eq:diff_TiTv}).


%\begin{figure}[H]
%	\centering
%	\framebox{\includegraphics[width=0.98\textwidth]{TiTv_distance_ridges_maf_eta.pdf}}
%	\caption{Density curves of TiTv distance as a function of average MAF $\bar{f}_a$, given by Eq.~\ref{eq:avg_maf}, and Ti/Tv ratio $\eta$, given by Eq.~\ref{eq:TiTv_constraints2}. (\textbf{A}) For fixed $\eta=2$, TiTv distance density is plotted as a function of increasing $\bar{f}_a = 0.055, 0.150, 0.250, \text{ and } 0.350$. TiTv distance increases as $\bar{f}_a$ approaches a maximum of 0.5, which means that there is about the same frequency of minor alleles as primary alleles at locus $a$. (\textbf{B}) For fixed $\bar{f}_a=0.055$, TiTv distance density is plotted as a function of increasing $\eta = 0.5, 1, 1.5, \text{ and } 2$. TiTv distance decreases as $\eta$, the Ti/Tv ratio, increases. For $\eta=\text{Ti/Tv}=0.5$, there are twice as many transversions as there are transitions. On the other hand, $\eta=\text{Ti/Tv}=2$ indicates that there are half as many transversions as there are transitions. Since transversions encode a larger magnitude distance than transitions in Eq.~\ref{eq:diff_TiTv}, this behavior is expected.}\label{fig:TiTv_hist}
%\end{figure}
%
%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.98\textwidth]{TiTv_distance_meanSD_vs_MAF_eta.pdf}
%	\caption{TiTv distance predicted and simulated moments as a function of Ti/Tv ratio $\eta$ and average MAF $\bar{f}_a$ given by Eqs.~\ref{eq:TiTv_constraints2} and \ref{eq:avg_maf}, respectively. (\textbf{A}) Simulated and predicted mean $\pm$ SD are shown as a function of increasing Ti/Tv ratio $\eta$. Distance decreases as Tv becomes more frequent than Ti. Theoretical standard deviation is slightly larger than simulated, but the means are approximately the same. (\textbf{B}) Simulated and predicted mean $\pm$ SD are shown as a function of increasing average MAF $\bar{f}_a$. Distance increases as the number of minor alleles increases at each locus $a$. Theoretical and simulated moments are approximately the same.}\label{fig:TiTv_meanSD}
%\end{figure}

\begin{table}[H]
	\caption{Asymptotic estimates of means and variances of genotype mismatch (GM) (Eq.~\ref{eq:D_GM}), allele mismatch (AM) (Eq.~\ref{eq:D_AM}), and transition-transversion (TiTv) (Eq.~\ref{eq:D_TiTv}) distance metrics in GWAS data ($p \gg 1$). GWAS data $X_{ia} \sim \mathcal{B}(2,f_a)$, where $f_a$ for all $a \in \mathcal{A}$ are the probabilities of a minor allele occurring at locus $a$. For the TiTv distance metric, we have the additional encoding that uses $\gamma_0=\text{P}(\text{PuPu})$, $\gamma_1=\text{P}(\text{PuPy})$, and $\gamma_2=\text{P}(\text{PyPy})$.}
	\label{tab:dist_distr_gwas}
	\centering
	\begin{adjustbox}{center}
		\begin{tikzpicture}
		% trim=left botm right top
		\node at (0,0) {\includegraphics[clip,trim=0.27cm 0.0cm 0.0cm 0.05cm,width=0.99\textwidth]{updated_distributions_table-gwas(5-23-2019).pdf}};
		
		% GM metric (column 1 - row 1)
		\node[fill=white,xscale=0.81,yscale=0.81] at (-5.1,3.53) {(\textbf{Eq.}~\textbf{\ref{eq:D_GM}})};
		
		% AM metric (column 1 - row 2)
		\node[fill=white,xscale=0.81,yscale=0.81] at (-5.1,0.22) {(\textbf{Eq.}~\textbf{\ref{eq:D_AM}})};
		
		% TiTv metric (column 1 - row 3)
		\node[fill=white,xscale=0.81,yscale=0.81] at (-5.1,-4.08) {(\textbf{Eq.}~\textbf{\ref{eq:D_TiTv}})};
		
		% mean (row 1)
		\node[fill=white,xscale=0.78,yscale=0.78] at (3.24,4.91) {(\textbf{\ref{eq:mu_DDistr_GM}})};
		
		% var (row 2)
		\node[fill=white,xscale=0.78,yscale=0.78] at (4.05,3.35) {(\textbf{\ref{eq:var_DDistr_GM}})};
		
		% mean (row 3)
		\node[fill=white,xscale=0.78,yscale=0.78] at (3.23,1.775) {(\textbf{\ref{eq:mu_DDistr_AM}})};
		
		% var (row 4)
		\node[fill=white,xscale=0.78,yscale=0.78] at (4.1,0.22) {(\textbf{\ref{eq:var_DDistr_AM}})};
		
		% mean (row 5)
		\node[fill=white,xscale=0.78,yscale=0.78] at (5.76,-1.94) {(\textbf{\ref{eq:mu_DDistr_TiTv}})};
		
		% var (row 6)
		\node[fill=white,xscale=0.78,yscale=0.78] at (5.8,-4.1) {(\textbf{\ref{eq:var_DDistr_TiTv}})};
		\end{tikzpicture}
	\end{adjustbox}
\end{table}

\subsection{Distribution of one-dimensional projected GWAS distance onto a SNP}\label{sec:GWAS_diff}

We previously derived the exact distribution of the one-dimensional projected distance onto an attribute in continuous data (Section~\ref{sec:continuous_diff}), which is used as the predictor in NPDR to calculate relative feature importance in the form of standardized beta coefficients. GWAS data and the metrics we have considered are discrete. Therefore, we derive the density function for each diff metric (Eqs.~\ref{eq:diff_GM}-\ref{eq:diff_TiTv}), which also serves as the probability distribution for each metric, respectively.

The support of the GM metric (Eq.~\ref{eq:diff_GM}) is simply $\{0,1\}$, so we derive the probability, $\text{P}\left[\text{d}^\text{GM}_{ij}(a) = k\right]$, of this diff taking on each of these two possible values. First, the probability that the GM diff is equal to zero is given by
%
\begin{equation}\label{eq:probGM0}
\begin{aligned}
f_\text{GM}(0;f_a) &= \text{P}\left[\text{d}^\text{GM}_{ij}(a) = 0\right] \\
&= \text{P}\left(X_{ia}=0,X_{ja}=0\right) + \text{P}\left(X_{ia}=1,X_{ja}=1\right) \\
& \hspace{3.64cm} + \text{P}\left(X_{ia}=2,X_{ja}=2\right) \\
&= \left(1 - f_a\right)^4 + 4 f^2_a \left(1 - f_a\right)^2 + f^4_a,
\end{aligned}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$.

Similarly, the probability that the GM diff is equal to 1 is derived as follows
%
\begin{equation}\label{eq:probGM1}
\begin{aligned}
f_\text{GM}(1;f_a) &= \text{P}\left[\text{d}^\text{GM}_{ij}(a) = 1\right]\\
&= 2\text{P}\left(X_{ia}=0,X_{ja}=1\right) + 2\text{P}\left(X_{ia}=1,X_{ia}=2\right) \\
& \hspace{3.81cm} + 2\text{P}\left(X_{ia}=0,X_{ja}=2\right) \\
&= 4\left(1 - f_a\right)^3 f_a + 4 f^3_a \left(1 - f_a\right) + 2 f^2_a \left(1 - f_a\right)^2,
\end{aligned}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$.

This leads us to the probability distribution of the GM diff metric, which is the distribution of the one-dimensional GM distance projected onto a single SNP. This distribution is given by
%
\begin{equation}\label{eq:GMdiffPDF}
f_\text{GM}(d;f_a) = \begin{cases}
\left(1 - f_a\right)^4 + 4 f^2_a \left(1 - f_a\right)^2 + f^4_a & d=0, \\
4\left(1 - f_a\right)^3 f_a + 4 f^3_a \left(1 - f_a\right) + 2 f^2_a \left(1 - f_a\right)^2 & d=1,
\end{cases}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$. 

The mean and variance of this GM diff distribution can easily be derived using this newly determined density function (Eq.~\ref{eq:GMdiffPDF}). The average GM diff is given by the following
%
\begin{equation}\label{eq:GMdiffMean}
\text{E}\left[\text{d}^\text{GM}_{ij}(a)\right] = 2 F^\text{GM}(a),
\end{equation}
%
where $F^\text{GM} = 2 \left(1 - f_a\right)^3 f_a + 2 f^3_a \left(1 - f_a\right) + f^2_a \left(1 - f_a\right)^2$ and $f_a$ is the probability of a minor allele occurring at locus $a$.

The variance of the GM diff metric is given by
%
\begin{equation}\label{eq:GMdiffVar}
\text{Var}\left[\text{d}^\text{GM}_{ij}(a)\right] = 2 F^\text{GM}(a)\left[1 - 2 F^\text{GM}(a)\right],
\end{equation}
%
where $F^\text{GM} = 2 \left(1 - f_a\right)^3 f_a + 2 f^3_a \left(1 - f_a\right) + f^2_a \left(1 - f_a\right)^2$ and $f_a$ is the probability of a minor allele occurring at locus $a$.

The support of the AM metric (Eq.~\ref{eq:diff_AM}) is $\{0,1/2,1\}$. Beginning with the probability of the AM diff being equal to 0, we have the following probability
%
\begin{equation}\label{eq:probAM0}
\begin{aligned}
f_\text{AM}(0;f_a) &= \text{P}\left[\text{d}^\text{AM}_{ij}(a) = 0\right] \\
&= \text{P}\left(X_{ia}=0,X_{ja}=0\right) + \text{P}\left(X_{ia}=1,X_{ja}=1\right) \\
& \hspace{3.64cm} + \text{P}\left(X_{ia}=2,X_{ja}=2\right) \\
&= \left(1 - f_a\right)^4 + 4 f^2_a \left(1 - f_a\right)^2 + f^4_a,
\end{aligned}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$.

The probability of the AM diff metric being equal to 1/2 is computed similarly as follows
%
\begin{equation}\label{eq:probAM0.5}
\begin{aligned}
f_\text{AM}\left(1/2;f_a\right) &= \text{P}\left[\text{d}^\text{AM}_{ij}(a) = 1/2\right] \\
&= 2\text{P}\left(X_{ia}=0,X_{ja}=1\right) + 2\text{P}\left(X_{ia}=1,X_{ia}=2\right) \\
&= 4 \left(1 - f_a\right)^3 f_a + 4 f^3_a \left(1 - f_a\right),
\end{aligned}
\end{equation}
%
where $f_a$ the probability of a minor allele occurring at locus $a$.

Finally, the probability of the AM diff metric being equal to 1 is given by the following
%
\begin{equation}\label{eq:probAM1}
\begin{aligned}
f_\text{AM}(1;f_a) = \text{P}\left[\text{d}^\text{AM}_{ij}(a) = 1\right] &= 2 \text{P}\left(X_{ia}=0,X_{ja}=2\right) \\
&= 2 f^2_a \left(1 - f_a\right)^2,
\end{aligned}
\end{equation}
%
where $f_a$ is the probability of a minor allele occuring at locus $a$.

As in the case of the GM diff metric, we now have the probability distribution of the AM diff metric. This also serves as the distribution of the one-dimensional AM distance projected onto a single SNP, and is given by the following
%
\begin{equation}\label{eq:AMdiffPDF}
f_\text{AM}(d;f_a) = \begin{cases}
\left(1 - f_a\right)^4 + 4 f^2_a \left(1 - f_a\right)^2 + f^4_a & d=0, \\
4 \left(1 - f_a\right)^3 f_a + 4 f^3_a \left(1 - f_a\right) & d=1/2, \\
2 f^2_a \left(1 - f_a\right)^2 & d=1,
\end{cases}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$.

The mean and variance of this AM diff distribution is derived using the corresponding density function (Eq.~\ref{eq:AMdiffPDF}). The average AM diff is given by
%
\begin{equation}\label{eq:AMdiffMean}
\text{E}\left[\text{d}^\text{AM}_{ij}(a)\right] = 2 F^\text{AM}(a),
\end{equation}
%
where $F^\text{AM}(a) = \left(1 - f_a\right)^3 f_a + f^3_a \left(1 - f_a\right) + f^2_a \left(1 - f_a\right)^2$ and $f_a$ is the probability of a minor allele occurring at locus $a$.

The variance of the AM diff metric is given by
%
\begin{equation}\label{eq:AMdiffVar}
\text{Var}\left[\text{d}^\text{AM}_{ij}(a)\right] = G^\text{AM}(a) - 4 \left[F^\text{AM}(a)\right]^2,
\end{equation}
%
where $G^\text{AM}(a) = \left(1 - f_a\right)^3 f_a + f^3_a \left(1 - f_a\right) + 2\left(1 - f_a\right)^2 f^2_a$,

\noindent $F^\text{AM}(a) = \left(1 - f_a\right)^3 f_a + f^3_a \left(1 - f_a\right) + f^2_a \left(1 - f_a\right)$, $f_a$ is the probability of a minor allele occurring at locus $a$.

For the TiTv diff metric (Eq.~\ref{eq:diff_TiTv}), the support is $\{0,1/4,1/2,3/4,1\}$. We have already derived the probability that the TiTv diff assumes each of the values of its support (Eqs.~\ref{eq:prob_TiTv_0}-\ref{eq:prob_TiTv_1}). Therefore, we have the following distribution of the TiTv diff metric
%
\begin{equation}\label{eq:TiTvdiffPDF}
f_\text{TiTv}(d;f_a,\gamma_0,\gamma_1,\gamma_2) = \begin{cases}
\left(1 - f_a\right)^4 + 4 f^2_a \left(1 - f_a\right)^2 + f^4_a & d=0, \\
4(\gamma_0 + \gamma_2)\left[\left(1 - f_a\right)^3 f_a + f^3_a \left(1 - f_a\right)\right] & d=1/4, \\
4 \gamma_1 \left[\left(1 - f_a\right)^3 f_a + f^3_a \left(1 - f_a\right)\right] & d=1/2, \\
2(\gamma_0 + \gamma_2)\left(1 - f_a\right)^2 f^2_a & d=3/4, \\
2\gamma_1\left(1 - f_a\right)^2 f^2_a & d=1,
\end{cases}
\end{equation}
%
where $f_a$ is the probability of a minor allele occurring at locus $a$, $\gamma_0$ is the probability of PuPu at locus $a$, $\gamma_1$ is the probability of PuPy at locus $a$, $\gamma_2$ is the probability of PyPy at locus $a$, and $\eta$ is the Ti/Tv ratio (Eq.~\ref{eq:TiTv_constraints2}).

The mean and variance of this TiTv diff distribution is derived using the corresponding density function (Eq.~\ref{eq:TiTvdiffPDF}). The average TiTv diff is given by
%
\begin{equation}\label{eq:TiTvdiffMean}
\begin{aligned}
\text{E}\left[\text{d}^\text{TiTv}_{ij}(a)\right] &= (\gamma_0 + \gamma_2 + 2\gamma_1)F^\text{TiTv}(a) + \left[\frac{3}{2}(\gamma_0 + \gamma_2) + 2\gamma_1\right] G^\text{TiTv}(a),
\end{aligned}
\end{equation}
%
where $F^\text{TiTv}(a) = \left(1 - f_a\right)^3 f_a + f^3_a \left(1 - f_a\right)$, $G^\text{TiTv}(a) = f^2_a \left(1 - f_a\right)^2$, $f_a$ is the probability of a minor allele occurring at locus $a$, $\gamma_0$ is the probability of PuPu at locus $a$, $\gamma_1$ is the probability of PuPy at locus $a$, and $\gamma_2$ is the probability of PyPy at locus $a$.

The variance of the TiTv diff metric is given by
%
\begin{equation}\label{eq:TiTvdiffVar}
\begin{aligned}
\text{Var}\left[\text{d}^\text{TiTv}_{ij}(a)\right] &= \left[\frac{1}{4}(\gamma_0 + \gamma_2) + \gamma_1\right] F^\text{TiTv}(a) \\
&\hspace{3cm}+ \left[\frac{9}{8}(\gamma_0 + \gamma_2) + 2\gamma_1\right] G^\text{TiTv}(a) \\
&- {\Biggl\{}{\biggl(}(\gamma_0 + \gamma_2 + 2\gamma_1)F^\text{TiTv}(a) \\
&\hspace{3cm}+ \left[\frac{3}{2}(\gamma_0 + \gamma_2) + 2\gamma_1\right] G^\text{TiTv}(a){\biggr)}{\Biggr\}}^2,
\end{aligned}
\end{equation}
%
where $F^\text{TiTv}(a) = \left(1 - f_a\right)^3 f_a + f^3_a \left(1 - f_a\right)$, $G^\text{TiTv}(a) = f^2_a \left(1 - f_a\right)^2$, $f_a$ is the probability of a minor allele occurring at locus $a$, $\gamma_0$ is the probability of PuPu at locus $a$, $\gamma_1$ is the probability of PuPy at locus $a$, and $\gamma_2$ is the probability of PyPy at locus $a$.

We now have the distribution of the projection of pairwise GWAS distances onto a single SNP. These distributions and their respective moments can inform NPDR, as well as other nearest-neighbor distance-based feature selection algorithms. Similar to the distribution of the TiTv pairwise distance, the TiTv diff distribution we have just derived is a novel result. In the next section, we introduce our new diff metric for time series correlation-based data, with a particular application to resting-state fMRI.

%\subsection{Resting-State fMRI Distance Distribution}
\section{Time series correlation-based distance distribution}\label{sec:rs-fMRI_distances}

For time series correlation-based data, we consider the case where there are $m$ correlation matrices $A^{(p \times p)}$. In particular, we are focusing on resting-state fMRI (rs-fMRI) data, which falls into this category. The derivations that follow, however, are relevant to all correlation-based data fitting the assumptions we have adopted. The features in rs-fMRI are commonly Regions of Interest (ROIs), which are collections of highly correlated and spatially proximal voxels \cite{lee2013}. These correlations are between different ROIs for a particular brain atlas \cite{dickie2017}. Because the features of interest ($a$) are the ROIs themselves, we propose the following projection (diff)
%
\begin{equation}\label{eq:diff_rs-fMRI}
\text{d}^\text{ROI}_{ij}(a) = \sum_{k \neq a}\bigl|A^{(i)}_{ka} - A^{(j)}_{ka}\bigr|,
\end{equation}
%
where $A^{(i)}_{ka}$ and $A^{(j)}_{ka}$ are the correlations between ROI $a$ and ROI $k$ for instances $i,j \in \mathcal{I}$, respectively. With this rs-fMRI diff, we define the pairwise distance between two instances $i,j \in \mathcal{I}$ is computed as follows
%
\begin{equation}\label{eq:D_rs-fMRI}
D^\text{fMRI}_{ij} = \sum_{a \in \mathcal{A}} \text{d}^\text{ROI}_{ij}(a),
\end{equation}
%
which is based on Manhattan ($q=1$). This metric may be expanded to general $q$, but we only consider $q=1$.

In order for comparisons between different correlations to be possible, we first perform a Fisher r-to-z transform on the correlations. This transformation makes the data approximately normally distributed with stabilized variance across different samples. After this transformation, we then load all of the transformed correlations into a $p(p-1) \times m$ matrix $X$ (Fig.~\ref{fig:rs-fMRI_matrix}). Each column of $X$ represents a single instance (or subject) in rs-fMRI data. Contrary to a typical $p \times m$ data set, each row does not represent a single feature. Rather, each feature (or ROI) is represented by $p - 1$ consecutive rows. The first $p - 1$ rows represents $\text{ROI}_1$, the next $p - 1$ rows represents $\text{ROI}_2$, and so on until the last $p - 1$ rows that represents $\text{ROI}_p$. For a given column of $X$, we exclude pairwise correlations between an ROI and itself. Therefore, the matrix does not contain $\hat{A}^{(i)}_{aa}$ for any $i \in \mathcal{I}$ or $a \in \mathcal{A}$. Furthermore, symmetry of correlation matrices means that each column contains exactly two of each element of the upper triangle of an instance's transformed correlation matrix. For example, $\hat{A}^{(i)}_{ka} = \hat{A}^{(i)}_{ak}$ for $k \neq a$ and both will be contained in a given column of $X$ for each $a \in \mathcal{A}$. Based on our rs-fMRI diff (Eq.~\ref{eq:diff_rs-fMRI}), the organization of $X$ makes computation of each value of the diff very simple. In order to compute each value of the rs-fMRI diff, we just need to know the starting and ending row indices for a given ROI. Starting indices are given by
%
\[
\text{start}_k = (k - 1)(p - 1) + 1, \quad \text{ for } k = 1,2,\dots,p
\]
%
and ending indices are given by
%
\[
\text{end}_k = k(p - 1), \quad \text{ for } k = 1,2,\dots,p.
\]

These indices allow use to extract just the rows necessary to compute the rs-fMRI diff for a fixed ROI. 

\begin{figure}[H]
\begin{minipage}[c]{0.7\textwidth}
	\includegraphics[width=0.95\textwidth]{rs_fmri_all_instance_matrix.pdf}
\end{minipage}\hfill
\begin{minipage}[c]{0.29\textwidth}
	\caption{Resting-state fMRI transformed subject correlation matrices. Each column corresponds to an instance (or subject) $I_j$ and each column corresponds to an ROI (or feature). The notation $\hat{A}^{(j)}_{ak}$ represents the r-to-z transformed correlation between ROIs $a$ and $k \neq a$ for instance $j$.}\label{fig:rs-fMRI_matrix}
\end{minipage}
\end{figure}

%
%\begin{equation}\label{eq:rs-fMRI_matrix}
%X = \left[
%\begin{array}{c c c c c}
%\hat{A}^{(1)}_{12} & \hat{A}^{(2)}_{12} & \hat{A}^{(3)}_{12} & \dots & \hat{A}^{(m)}_{12} \\
%\hat{A}^{(1)}_{13} & \hat{A}^{(2)}_{13} & \hat{A}^{(3)}_{13} & \dots & \hat{A}^{(m)}_{13} \\
%\hat{A}^{(1)}_{14} & \hat{A}^{(2)}_{14} & \hat{A}^{(3)}_{14} & \dots & \hat{A}^{(m)}_{14} \\
%\vdots & \vdots & \vdots & \ddots & \vdots \\
%\hat{A}^{(1)}_{1p} & \hat{A}^{(2)}_{1p} & \hat{A}^{(3)}_{1p} & \dots & \hat{A}^{(m)}_{1p} \\
%\hat{A}^{(1)}_{21} & \hat{A}^{(2)}_{21} & \hat{A}^{(3)}_{21} & \dots & \hat{A}^{(m)}_{21} \\
%\hat{A}^{(1)}_{23} & \hat{A}^{(2)}_{23} & \hat{A}^{(3)}_{23} & \dots & \hat{A}^{(m)}_{23} \\
%\hat{A}^{(1)}_{24} & \hat{A}^{(2)}_{24} & \hat{A}^{(3)}_{24} & \dots & \hat{A}^{(m)}_{24} \\
%\vdots & \vdots & \vdots & \ddots & \vdots \\
%\hat{A}^{(1)}_{2p} & \hat{A}^{(2)}_{2p} & \hat{A}^{(3)}_{2p} & \dots & \hat{A}^{(m)}_{2p} \\
%\vdots & \vdots & \vdots & \ddots & \vdots \\
%\hat{A}^{(1)}_{p1} & \hat{A}^{(2)}_{p1} & \hat{A}^{(3)}_{p1} & \dots & \hat{A}^{(m)}_{p1} \\
%\hat{A}^{(1)}_{p2} & \hat{A}^{(2)}_{p2} & \hat{A}^{(3)}_{p2} & \dots & \hat{A}^{(m)}_{p2} \\
%\hat{A}^{(1)}_{p3} & \hat{A}^{(2)}_{p3} & \hat{A}^{(3)}_{p3} & \dots & \hat{A}^{(m)}_{p3} \\
%\vdots & \vdots & \vdots & \ddots & \vdots \\
%\hat{A}^{(1)}_{p,(p-1)} & \hat{A}^{(2)}_{p,(p-1)} & \hat{A}^{(3)}_{p,(p-1)} & \dots & \hat{A}^{(m)}_{p,(p-1)}
%\end{array}
%\right],
%\end{equation}

We further transform the data matrix $X$ by standardizing so that each of the $m$ columns has zero mean and unit variance. Therefore, the data in matrix $X$ are approximately standard normal. Recall that the mean (Eq.~\ref{eq:normalManMean}) and variance (Eq.~\ref{eq:normalManVar}) of the Manhattan ($L_1$) distance distribution for standard normal data are $\frac{2p}{\sqrt{\pi}}$ and $\frac{2(\pi - 2)p}{\pi}$, respectively. This allows us to easily derive the expected pairwise distance between instances $i,j \in \mathcal{I}$ in rs-fMRI data as follows
%
\begin{equation}\label{eq:mu_DDistr_rs-fMRI}
\begin{aligned}
\text{E}(D^\text{fMRI}_{ij}) &= \text{E}\left(\sum_{a \in \mathcal{A}} \text{d}^\text{ROI}_{ij}(a)\right) \\
&= \text{E}\left(\sum_{a \in \mathcal{A}} \sum_{k \neq a} \left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&= \sum_{a \in \mathcal{A}} \sum_{k \neq a} \text{E}\left(\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&= \sum_{a \in \mathcal{A}} \sum_{k \neq a} \frac{2}{\sqrt{\pi}} \\
&= \frac{2p(p-1)}{\sqrt{\pi}}.
\end{aligned}
\end{equation}

The expected pairwise rs-fMRI distance (Eq.~\ref{eq:mu_DDistr_rs-fMRI}) grows on the order of $p(p-1)$, which is the total number of transformed pairwise correlations in each column of $X$ (Fig.~\ref{fig:rs-fMRI_matrix}). This is similar to the case of a typical $m \times p$ data matrix in which the data is standard normal and Manhattan distances are computed between instances. 

We first derive the variance of the rs-fMRI distance by making an independence assumption with respect to the magnitude differences $|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}|$ for all $k \neq a \in \mathcal{A}$. We observe empirically that this assumption gives us a reasonable estimate of the actual variance of rs-fMRI distances in simulated data, but there is a consistent discrepancy between predicted and simulated variances. We begin our derivation of the variance of rs-fMRI distances by assuming that cross-covariances between the diffs of different pairs of ROIs are negligible. This will allow us to determine the relationship between the predicted variance under the independence assumption and the simulated variance. We proceed by applying the variance operator linearly as follows
%
\begin{equation}\label{eq:var_DDistr_rs-fMRI_standard}
\begin{aligned}
\text{Var}(D^\text{fMRI}_{ij}) &= \text{Var}\left(\sum_{a \in \mathcal{A}} \text{d}^\text{ROI}_{ij}(a)\right) \\
&= \text{Var}\left(\sum_{a \in \mathcal{A}} \sum_{k \neq a} \left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&= \sum_{a \in \mathcal{A}} \sum_{k \neq a} \text{Var}\left(\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&= \sum_{a \in \mathcal{A}} \sum_{k \neq a} \frac{2(\pi - 2)}{\pi} \\
&= \frac{2(\pi - 2)(p-1)p}{\pi}.
\end{aligned}
\end{equation}

Similar to the case of a standard $m \times p$ data matrix containing standard normal data, we have a rs-fMRI distance variance that grows on the order of $p(p - 1)$, which is the total number of pairwise associations in a column of data matrix $X$ (Fig.~\ref{fig:rs-fMRI_matrix}). Therefore, the expected rs-fMRI distance (Eq.~\ref{eq:mu_DDistr_rs-fMRI}) and the variance of the rs-fMRI distance (Eq.~\ref{eq:var_DDistr_rs-fMRI_standard}) increase on the same order.

The independence assumption we made to derive the variance of our rs-fMRI distance metric (Eq.~\ref{eq:var_DDistr_rs-fMRI_standard}) is not actually satisfied due to the fact that a single value of the diff (Eq.~\ref{eq:diff_rs-fMRI}) includes the same fixed ROI $a$ for each term in the sum for all $k \neq a$. Therefore, the linear application of the variance operator we have previously employed does not account for the additional cross-covariance that exists. Although we have seen empirically that the theoretical variance of the distance we computed for the rs-fMRI distance metric (Eq.~\ref{eq:var_DDistr_rs-fMRI_standard}) still reasonably approximates the sample variance, there is a slight discrepancy between our theoretical rs-fMRI distance metric variance (Eq.~\ref{eq:var_DDistr_rs-fMRI_standard}) and the sample variance. More precisely, the formula we have given for the variance (Eq.~\ref{eq:var_DDistr_rs-fMRI_standard}) consistently under-approximates the sample variance of the rs-fMRI distance. Because of this discrepancy, we determine a corrected formula by assuming that there is dependence between the terms of the rs-fMRI diff and estimate the cross-covariance between rs-fMRI diffs of different pairs of ROIs. 

We begin the derivation of our corrected formula by writing the variance as a two-part sum, where the first term in the sum involves the variance of the magnitude difference $|\hat{A}^{(i)}_{ka} - \hat{A}^{(j)}_{ka}|$ and then second term involves the cross-covariance of the rs-fMRI diff for distinct pairwise ROI-ROI associations. This formulation is implied in our previous derivation of the variance, but our independence assumption allowed us to assume that all terms in the second part of the two-part sum were zero. Our formulation of the variance is given by the following
%
\begin{equation}\label{eq:var_DDistr_rs-fMRI}
\begin{aligned}
\text{Var}(D^\text{fMRI}_{ij}) &= \text{Var}\left(\sum_{a \in \mathcal{A}} \sum_{k \neq a} \left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&= \sum_{a = 1}^{p-1} \text{Var}\left(\sum_{k=a+1}^{p} 2\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&+ 2\sum_{a = 1}^{p-1} \sum_{r=a+1}^{p-1} \text{Cov}\left(\sum_{k=a+1}^{p} 2\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|, \sum_{s=r+1}^{p} 2\left|\hat{A}^{(i)}_{rs} - \hat{A}^{(j)}_{rs}\right|\right) \\
&= \sum_{a=1}^{p-1} \sum_{k=a+1}^{p} \text{Var}\left(2\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&+ 2\sum_{a = 1}^{p-1} \sum_{r=a+1}^{p-1} \text{Cov}\left(\sum_{k=a+1}^{p} 2\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|, \sum_{s=r+1}^{p} 2\left|\hat{A}^{(i)}_{rs} - \hat{A}^{(j)}_{rs}\right|\right) \\
&= \sum_{a = 1}^{p-1} \sum_{k=a+1}^{p-1}\frac{4(\pi-2)}{\pi} \\
&+ 2\sum_{a = 1}^{p-1} \sum_{r=a+1}^{p-1} \text{Cov}\left(\sum_{k=a+1}^{p} 2\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|, \sum_{s=r+1}^{p} 2\left|\hat{A}^{(i)}_{rs} - \hat{A}^{(j)}_{rs}\right|\right) \\
&= \frac{2p(\pi-2)(p-1)}{\pi} \\
&+ 2\sum_{a = 1}^{p-1} \sum_{r=a+1}^{p-1} \text{Cov}\left(\sum_{k=a+1}^{p} 2\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|, \sum_{s=r+1}^{p} 2\left|\hat{A}^{(i)}_{rs} - \hat{A}^{(j)}_{rs}\right|\right).
\end{aligned}
\end{equation}

In order to have a formula in terms of the number of ROIs $p$ only, we must estimate the double sum on the right-hand side of the equation of rs-fMRI distance variance (Eq.~\ref{eq:var_DDistr_rs-fMRI}). Through simulation, it can be seen that the difference between sample variance $S^2_{D_{ij}}$ and $\frac{2p(\pi-2)(p-1)}{\pi}$ has a quadratic relationship with $p$. More explicitly, we have the following relationship
%
\begin{equation}\label{eq:estimate_cov}
S^2_{D^\text{fMRI}_{ij}} - \frac{2p(\pi-2)(p-1)}{\pi} = \beta_1 p^2 + \beta_0 p.
\end{equation}
%
where $\beta_0$ and $\beta_1$ are the coefficients we must estimate in order to approximate the cross-covariance term in the right-hand side of the rs-fMRI distance variance equation (Eq.~\ref{eq:var_DDistr_rs-fMRI}).

The coefficient estimates found through least squares fitting are $\beta_0 = - \beta_1 \approx 0.08$. These estimates allow us to arrive at a functional form for the double sum in the right-hand side of the rs-fMRI distance variance equation (Eq.~\ref{eq:var_DDistr_rs-fMRI}) that is proportional to $\frac{2p(\pi-2)(p-1)}{\pi}$. That is, we have the following formula for approximating the double sum
%
\begin{equation}\label{eq:estimate_cov_form}
\begin{aligned}
&2\sum_{a = 1}^{p-1} \sum_{r=a+1}^{p-1} \text{Cov}\left(\sum_{k=a+1}^{p} 2\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|, \sum_{s=r+1}^{p} 2\left|\hat{A}^{(i)}_{rs} - \hat{A}^{(j)}_{rs}\right|\right) \\ &\hspace{3.5cm}\approx \frac{p(\pi - 2)(p - 1)}{4\pi}.
\end{aligned}
\end{equation}

Therefore, the variance of the rs-fMRI distances is approximated well by the following
%
\begin{equation}\label{eq:var_DDistr_rs-fMRI2}
\text{Var}(D^\text{fMRI}_{ij}) \approx \frac{9p(\pi - 2)(p-1)}{4\pi}.
\end{equation}

With the mean (Eq.~\ref{eq:mu_DDistr_rs-fMRI}) and variance (Eq.~\ref{eq:var_DDistr_rs-fMRI2}) estimates, we have the following asymptotic distribution for rs-fMRI distances
%
\begin{equation}\label{eq:DDistr_rs-fMRI}
D^\text{fMRI}_{ij} \overset{.}{\sim} \mathcal{N}\left(\frac{2p(p-1)}{\sqrt{\pi}}, \frac{9p(\pi - 2)(p-1)}{4\pi}\right).
\end{equation}

\subsection{Max-min normalized time series correlation-based distance distribution}

We have determined in a previous section (Section~\ref{sec:extremes}) the asymptotic distribution of the sample maximum of size $m$ from a standard normal distribution. We can naturally extend these results to our transformed rs-fMRI data because $X$ (Fig.~\ref{fig:rs-fMRI_matrix}) is approximately standard normal. We proceed with the definition of the max-min normalized rs-fMRI pairwise distance.

Consider the max-min normalized rs-fMRI distance given by the following equation
%
\begin{equation}\label{eq:max-min_diff_rs-fMRI}
D^\text{fMRI*}_{ij} = \sum_{a \in \mathcal{A}} \sum_{k \neq a} \frac{\left|A^{(i)}_{ak} - A^{(j)}_{ak}\right|}{\max(a) - \min(a)}.
\end{equation}

Assuming that the data $X$ has been r-to-z transformed and standardized, we can easily compute the expected attribute range and variance of the attribute range. The expected maximum of a given attribute in data matrix $X$ is estimated by the following
%
\begin{equation}\label{eq:mean_max_rs-fMRI}
\begin{aligned}
\text{E}\left(X^\text{max}_a - X^\text{min}_a\right) &= 2\mu^{(1)}_\text{max}(m,p) \\
&= 2 \left[\frac{\text{log}(\text{log}(2))}{\Phi^{-1}\left(\frac{1}{m(p-1)}\right)} - \Phi^{-1}\left(\frac{1}{m(p-1)}\right)\right].
\end{aligned}
\end{equation}

The variance can be esimated with the following
%
\begin{equation}\label{eq:var_max_rs-fMRI}
\text{Var}\left(X^\text{max}_a - X^\text{min}_a\right) = \frac{\pi^2}{6\text{log}[m(p-1)]}.
\end{equation}

Let $\mu_{D^\text{fMRI}_{ij}}$ and $\sigma^2_{D^\text{fMRI}_{ij}}$ denote the mean and variance of the rs-fMRI distance distribution given by Eqs. \ref{eq:mu_DDistr_rs-fMRI} and \ref{eq:var_DDistr_rs-fMRI2}. Using the formulas for the mean and variance of the max-min normalized distance distribution given in Eq. \ref{eq:max-min_DDistr_normal}, we have the following asymptotic distribution for the max-min normalized rs-fMRI distances
%
\begin{equation}\label{eq:max-min_DDistr_normal_rs-fMRI}
D^\text{fMRI*}_{ij} \overset{.}{\sim} \mathcal{N}\left(\frac{\mu_{D^\text{fMRI}_{ij}}}{2\mu^{(1)}_\text{max}(m,p)}, \frac{6\sigma^2_{D^\text{fMRI}_{ij}}\text{log}[m(p-1)]}{\pi^2 + 24\left[\mu^{(1)}_\text{max}(m,p)\right]^2\text{log}[m(p-1)]}\right).
\end{equation}

\subsection{One-dimensional projection of rs-fMRI distance onto a single ROI}\label{sec:rs-fMRI_diff}

Just as in previous sections (Sections.~\ref{sec:continuous_diff} and \ref{sec:GWAS_diff}), we now derive the distribution of our rs-fMRI diff metric (Eq.~\ref{eq:diff_rs-fMRI}). Unlike what we have seen in previous sections, we do not derive the exact distribution for this diff metric. We have determined empirically that the rs-fMRI diff is approximately normal. Although the rs-fMRI diff is a sum of $p-1$ magnitude differences, the Classical Central Limit Theorem does not apply because of the dependencies that exist between the terms of the sum. Examination of histograms and quantile-quantile plots of simulated values of the rs-fMRI diff easily indicate that the normality assumption is safe. Therefore, we derive the mean and variance of the approximately normal distribution of the rs-fMRI diff. As we have seen previously, this normality assumption is reasonable even for small values of $p$. 

The mean of the rs-fMRI diff is derived by fixing a single ROI $a$ and considering all pairwise associations with other ROIs $k \neq \mathcal{A}$. This is done as follows
%
\begin{equation}\label{eq:rs-fMRI_diffMean}
\begin{aligned}
\text{E}\left[\text{d}^\text{ROI}_{ij}(a)\right] &= \text{E}\left(\sum_{k \neq a} \left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&= \sum_{k \neq a} \text{E}\left(\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&= \sum_{k \neq a} \frac{2}{\sqrt{\pi}} \\
&= \frac{2(p-1)}{\sqrt{\pi}},
\end{aligned}
\end{equation}
% 
where $a$ is a single fixed ROI.

Considering the variance of the rs-fMRI diff metric, we have two estimates. The first estimate uses the variance operator in a linear fashion, while the second will simply be a direct implication of the corrected formula of the variance of rs-fMRI pairwise distances (Eq.~\ref{eq:var_DDistr_rs-fMRI2}). Our first estimate is derived as follows
%
\begin{equation}\label{eq:rs-fMRI_diffVar1}
\begin{aligned}
\text{Var}\left[\text{d}^\text{ROI}_{ij}(a)\right] &= \text{Var}\left(\sum_{k \neq a} \left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&= \sum_{k \neq a} \text{Var}\left(\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|\right) \\
&= \sum_{k \neq a} \frac{2(\pi - 2)}{\pi} \\
&= \frac{2(\pi - 2)(p-1)}{\pi},
\end{aligned}
\end{equation}
%
where $a$ is a single fixed ROI.

Using the corrected rs-fMRI distance variance formula (Eq.~\ref{eq:var_DDistr_rs-fMRI2}), our second estimate of the rs-fMRI diff variance is given directly by the following
%
\begin{equation}\label{eq:rs-fMRI_diffVar2}
\text{Var}\left[\text{d}^\text{ROI}_{ij}(a)\right] = \frac{9(\pi - 2)(p - 1)}{4\pi},
\end{equation}
%
where $a$ is a single fixed ROI.

Empirically, the first estimate (Eq.~\ref{eq:rs-fMRI_diffVar1}) of the variance of our rs-fMRI diff is closer to the sample variance than the second estimate (Eq.~\ref{eq:rs-fMRI_diffVar2}). This is due to fact that we are considering only a fixed ROI $a \in mathcal{A}$, so the cross-covariance between the magnitude differences $\left|\hat{A}^{(i)}_{ak} - \hat{A}^{(j)}_{ak}\right|$ for different pairs of ROIs ($a$ and $k \neq a$) is negligible here. When considering all ROIs $a \in \mathcal{A}$, these cross-covariances are no longer negligible. Using the first variance estimate (Eq.~\ref{eq:rs-fMRI_diffVar1}) and the estimate of the mean (Eq.~\ref{eq:rs-fMRI_diffMean}), we have the following asymptotic distribution of the rs-fMRI diff
%
\begin{equation}
\text{d}^\text{ROI}_{ij}(a) \overset{.}{\sim} \mathcal{N}\left(\frac{2(p-1)}{\sqrt{\pi}},\frac{2(\pi - 2)(p-1)}{\pi}\right),
\end{equation}
%
where $a$ is a single fixed ROI.

\subsection{Normalized Manhattan \texorpdfstring{($q=1$)}{} for rs-fMRI}

Substituting the non-normalized mean (Eq.~\ref{eq:mu_DDistr_rs-fMRI}) into the equation for the mean of the max-min normalized rs-fMRI metric (Eq.~\ref{eq:max-min_DDistr_normal_rs-fMRI}), we have the following
%
\begin{equation}\label{eq:mu_max-min_rs-fMRI}
\begin{aligned}
\text{E}\left(D^\text{fMRI*}_{ij}\right) &= \frac{\mu_{D^\text{fMRI}_{ij}}}{2\mu^{(1)}_\text{max}(m,p)} \\
&= \frac{p(p-1)}{\sqrt{\pi}\mu^{(1)}_\text{max}(m,p)},
\end{aligned}
\end{equation}
%
where $\mu^{(1)}_\text{max}(m,p)$ (Eq.~\ref{eq:mean_max_rs-fMRI}) is the expected maximum of a single ROI in a data set with $m$ instances and $p$ ROIs.

Similarly, the variance of $D^\text{fMRI*}_{ij}$ is given by
%
\begin{equation}\label{eq:var_max-min_rs-fMRI}
\begin{aligned}
\text{Var}\left(D^\text{fMRI*}_{ij}\right) &= \frac{6\sigma^2_{D^\text{fMRI}_{ij}}\text{log}[m(p-1)]}{\pi^2 + 24\left[\mu^{(1)}_\text{max}(m,p)\right]^2\text{log}[m(p-1)]} \\
&= \frac{27(\pi-2)\text{log}[m(p-1)](p-1)p}{2\pi\left(\pi^2 + 24\left[\mu^{(1)}_\text{max}(m,p)\right]^2\text{log}[m(p-1)]\right)},
\end{aligned}
\end{equation}
%
where $\mu^{(1)}_\text{max}(m,p)$ (Eq.~\ref{eq:mean_max_rs-fMRI}) is the expected maximum of a single ROI in a data set with $m$ instances and $p$ ROIs.

This concludes our analysis of rs-fMRI distances in random time series correlation-based data. We summarize the moment estimates for the standard and max-min normalized rs-fMRI distance metrics in a table (Table~\ref{tab:dist_distr_rs-fMRI}) that is organized by metric, statistic (mean or variance), and asymptotic formula.

\begin{table}[H]
	\caption{Aymptotic means and variances for the new standard (Eq.~\ref{eq:D_rs-fMRI}) and max-min normalized (Eq.~\ref{eq:max-min_diff_rs-fMRI}) rs-fMRI distance metrics.}
	\label{tab:dist_distr_rs-fMRI}
	\centering
	\begin{adjustbox}{center}
		\begin{tikzpicture}
		% trim=left botm right top
		\node at (0,0) {\includegraphics[clip,trim=0.27cm 0.0cm 0.0cm 0.05cm,width=\textwidth]{updated_distributions_table-rs-fMRI.pdf}};
		
		% standard rs-fMRI metric (column 1 - row 2)
		\node[fill=white,xscale=0.91,yscale=0.91] at (-4.78,1.42) {(\textbf{Eq.}~\textbf{\ref{eq:D_rs-fMRI}})};
		
		% max-min normalized rs-fMRI metric (column 1 - row 3)
		\node[fill=white,xscale=0.91,yscale=0.91] at (-4.78,-2.42) {(\textbf{Eq.}~\textbf{\ref{eq:max-min_diff_rs-fMRI}})};
		
		% mean (row 1)
		\node[fill=white,xscale=0.83,yscale=0.83] at (3.66,2.25) {(\textbf{\ref{eq:mu_DDistr_rs-fMRI}})};
		
		% var (row 2)
		\node[fill=white,xscale=0.83,yscale=0.83] at (4.12,0.9) {(\textbf{\ref{eq:var_DDistr_rs-fMRI}})};
		
		% mean (row 3)
		\node[fill=white,xscale=0.83,yscale=0.83] at (3.8,-0.33) {(\textbf{\ref{eq:mu_max-min_rs-fMRI}})};
		
		% numerator equation (row 3)
		\node[fill=white,xscale=0.94,yscale=0.94] at (4.66,-1.26) {\ref{eq:mu_DDistr_rs-fMRI}};
		
		% denominator equation (row 3)
		\node[fill=white,xscale=0.94,yscale=0.94] at (5.83,-1.26) {\ref{eq:mean_max_rs-fMRI}};
		
		% var (row 4)
		\node[fill=white,xscale=0.83,yscale=0.83] at (5.35,-2.445) {(\textbf{\ref{eq:var_max-min_rs-fMRI}})};
		
		% numerator equation (row 4)
		\node[fill=white,xscale=0.94,yscale=0.94] at (4.68,-3.575) {\ref{eq:mu_DDistr_rs-fMRI}};
		
		% denominator equation (row 5)
		\node[fill=white,xscale=0.94,yscale=0.94] at (5.83,-3.575) {\ref{eq:mean_max_rs-fMRI}};
		\end{tikzpicture}
	\end{adjustbox}
\end{table}

\section{Comparison of theoretical and sample moments}

It is important that our asymptotic estimates of sample moments for distances in high-dimensional data are accurate and precise enough to be useful when considering either simulated or real data that satisfy the appropriate assumptions. We compared our asymptotic estimates of sample moments of pairwise distances by generating random data for various dimensions $m$ and $p$ (Fig.~\ref{fig:compare_theoretical_sample_moments}). We fixed $m=100$ samples and generated random Manhattan (Eq.~\ref{eq:D}) distance matrices on standard normal data for $p=1000,2000,3000,4000,$ and $5000$ attributes. For each value of $p$, we generated 20 random distance matrices, computed the average distance, and computed the standard deviation of distances. We then computed the average of these 20 different means and standard deviations. The theoretical moments were simply computed for each value of $p$ and fixed $m=100$ in order to determine how closely our formulas approximate simulated moments. We generated scatter plots of theoretical versus simulated mean (Fig.~\ref{fig:compare_theoretical_sample_moments}A) and theoretical vs simulated standard deviation (Fig.~\ref{fig:compare_theoretical_sample_moments}B). All points lie approximately on the dashed identity lines, indicating that our theoretical asymptotic formulas for sample moments are reliable for both large and relatively small numbers of features. Although we do not show these results for every combination of data type, distance metric, sample size $m$, and number of features $p$, all theoretical formulas of sample moments we have derived similarly approximate their corresponding simulated moments.

\begin{figure}[H]
	\centering
	\includegraphics[scale = 0.4]{compared_moments_normal_manhattan5.pdf}
	\caption{Comparison of theoretical and sample moments of Manhattan (Eq.~\ref{eq:D}) distances in standard normal data. (\textbf{A}) Scatter plot of theoretical vs simulated mean Manhattan distance (Eq.~\ref{eq:normalManMean}). Each point represents a different number of attributes $p$. For each value of $p$ we fixed $m=100$ and generated 20 distance matrices from standard normal data and computed the average simulated pairwise distance from the 20 iterations. The corresponding theoretical mean was then computed for each value of $p$ for comparison. The dashed line represents the identity (or $y=x$) line for reference. (\textbf{B}) Scatter plot of theoretical vs simulated standard deviation of Manhattan (Eq.~\ref{eq:D}) distance (Eq.~\ref{eq:normalManVar}). These standard deviations come from the same random distance matrices for which mean distance was computed for \textbf{A}. Both theoretical mean and standard deviation approximate the simulated moments quite well.}\label{fig:compare_theoretical_sample_moments}
\end{figure}

\section{Effects of correlation on distances}\label{sec:correlation}

All of the derivations presented in previous sections are for the cases where there is no correlation between instances or features. We assumed that any pair $(X_{ia},X_{ja})$ of data points for instances $i$ and $j$ and fixed feature $a$ were independent and identically distributed. This was done in order to determine asymptotic estimates in null data. That is, data with no main effects, interaction effects, or pairwise correlations between features. Within this highly simplified context, our asymptotic formulas for distributional moments are reliable. However, correlations do exist between features and instances in real data. There are a multitude of different statistical effects that impact distance distributional properties. Ultimately, divergence from normality is caused primarily by large magnitude pairwise correlation between features. Pairwise feature correlation can be the result of main effects, where features have different within-group means. On the other hand, there could be an underlying interaction network in which there are strong associations between features. If features are differentially correlated between phenotype groups, then interactions exist that change affect the distance distribution. In the following few sections, we consider particular cases of the $L_q$ metric for continuous and discrete data under the effects of pairwise feature correlation.

\subsection{Continuous data}

Without loss of generality, suppose that we have $X^{(m \times p)}$ where $X_{ia} \sim \mathcal{N}(0,1)$ for all $i=1,2,\dots,m$ and $a=1,2,\dots,p$, and let $m=p=100$ and consider only the $L_2$ (Euclidean) metric (Eq.~\ref{eq:D}, $q=2$). We explore the effects of correlation on these distances by generating simulated data sets with increasing degree of pairwise feature correlation and then plotting the density curve of the induced distances (Fig.~\ref{fig:null_vs_correlated_ridge}A). Divergence from normality in distances is directly related to the average absolute pairwise correlation that exists in the simulated data. This measure is given by
%
\begin{equation}\label{eq:abs_corr}
\bar{r}_\text{abs} = \frac{2}{p(p-1)}\sum^{p-1}_{i=1} \sum_{j > i} r_{ij}
\end{equation}
%
where $r_{ij}$ is the correlation between features $i,j \in \mathcal{I}$ across all instances $m$. Distances generated on data without correlation closely approximate a Gaussian. The mean (Eq.~\ref{eq:normalEucMeanImproved}) and variance (Eq.~\ref{eq:normalEucVar}) of the uncorrelated distance distribution are given by substituting $p=100$ for the mean. As $\bar{r}_\text{abs}$ increases, we very quickly see positive skewness and increased variability in distances. The predicted and sample means, however, are approximately the same between correlated and uncorrelated distances due to linearity of the expectation operator. Because of the dependencies between features, the predicted variance of 1 for $L_2$ on standard normal data obviously no longer holds. 

In order to introduce a controlled level of correlation between features, we created correlation matrices based on a random graph with specified connection probability, where features correspond to the vertices in each graph. We assigned high correlations to connected features from the random graph and low correlations to all non-connections. Using the upper-triangular Cholesky factor $U$ for uncorrelated data matrix $X$, we computed the following product to create correlated data matrix $X^\text{corr}$
%
\begin{equation}\label{eq:cholesky}
X^\text{corr} = X U^\text{T}.
\end{equation}

The new data matrix $X^\text{corr}$ has approximately the same correlation structure as the randomly generated correlation matrix created from a random graph. The Cholesky method is a standard approach in creating correlated data sets.
%
%\begin{figure}[H]
%	\centering
%	\framebox{\includegraphics[width=0.98\textwidth]{null_vs_correlated_normal_euclidean_standard_TrangEdit.pdf}}
%	\caption{Density curves of Euclidean distances computed on data with correlated vs uncorrelated features. The average absolute pairwise correlation, given by $r$ (Eq.~\ref{eq:abs_corr}), is a measure of the deviation from normality in distances. When $r=0.103$, correlated distances closely approximate uncorrelated distances. With $r=0.246$, the increased correlation causes significant positive skewness in distances. In the cases of $r=0.436$ and $r=0.597$, the positive skewness becomes more extreme and correlated distances diverge maximally from the uncorrelated distance distribution. The average correlated and uncorrelated distances are approximately the same for each value of $r$, however, the standard deviation of correlated distances are far larger than that of uncorrelated distances.}\label{fig:null-vs-corr-normal}
%\end{figure}

\subsection{GWAS data}

In analogy to the previous section, we explore the effects of pairwise feature correlation in the context of GWAS data. Without loss of generality, we let $m=p=100$ and consider only the TiTv metric (Eq.~\ref{eq:D_TiTv}). To create correlated GWAS data, we first generated standard normal data with random correlation structure, just as in the previous section. We then applied the standard normal cumulative distribution function (CDF) to this correlated data in order transform the correlated standard normal variates into uniform data with preserved correlation structure. We then subsequently applied the inverse binomial CDF to the correlated uniform data with random success probabilities $f_a \text{ for all } a \in \mathcal{A}$. Each feature $a \in \mathcal{A}$ corresponds to an individual SNP in the data matrix. The resulting GWAS data set is binomial with $n=2$ trials and has roughly the same correlation matrix as the original correlated standard normal data with which we started. Average absolute pairwise correlation $\bar{r}_\text{abs}$ induces positive skewness in GWAS data at lower levels than in correlated standard normal data (Fig.~\ref{fig:null_vs_correlated_ridge}B). This could have important implications in nearest neighborhoods in NPDR and other similar methods.
%
%\begin{figure}[H]
%	\centering
%	\framebox{\includegraphics[width=0.98\textwidth]{null_vs_correlated_gwas_TiTv_TrangEdit.pdf}}
%	\caption{Density curves of TiTv distances computed on data with correlated vs uncorrelated features. The average absolute pairwise correlation, given by $r$ (Eq.~\ref{eq:abs_corr}), is a measure of the deviation from normality in distances. There is very little difference between correlated vs uncorrelated distances when $r=0.086$. When $r=0.147$, correlated distances begin to show positive skewness. Increasing to $r=0.227$ and $r=0.299$, correlated distances show extreme skewness. Compared to Fig.~\ref{fig:null-vs-corr-normal}, it appears that correlation more drastically affects distances in discrete GWAS data than $L_q$ distances in continuous data. This could have important implications for the choice of neighborhood parameters in nearest-neighbor distance-based feature selection. As in continuous data, the average correlated and uncorrelated TiTv distances are approximately the same with clear differences in standard deviations.}\label{fig:null-vs-corr-titv}
%\end{figure}

\subsection{Correlation-based data}

For our correlation data-based metric

\noindent (Eq.~\ref{eq:D_rs-fMRI}), we consider additional effects of correlation between features. Without loss of generality, we let $m=100$ and $p=30$. We show an illustration of the effects of correlated features in this context (Fig.~\ref{fig:null_vs_correlated_ridge}C). Based on the density estimates, it appears that correlation between features introduces positive skewness at low values of $\bar{r}_\text{abs}$. We introduced correlation to the transformed data matrix (Fig.~\ref{fig:rs-fMRI_matrix}) with the Cholesky method used previously.
%
%\begin{figure}[H]
%	\centering
%	\framebox{\includegraphics[width=0.98\textwidth]{null_vs_correlated_rsfMRI_TrangEdit.pdf}}
%	\caption{Density curves of rs-fMRI distances on data with correlated vs uncorrelated features. The average absolute pairwise correlation, given by $r$ (Eq.~\ref{eq:abs_corr}), is a measure of the deviation from normality in distances. Even with the relatively small $r=0.09$, there is significant deviation from uncorrelated distances with increased variance and some positive skewness. A small increase to $r=0.118$ causes rather extreme positive skewness to develop in correlated distances. As we increase to $r=0.163$ and $r=0.218$, the differences between correlated and uncorrelated rs-fMRI distances become much more pronounced. It appears that the feature-feature dependencies have the largest impact on time series correlation-based data like rs-fMRI. The data already consists of pairwise correlations between ROIs, which are transformed into a single $m \times p(p-1)$ data set. Correlation is then added on top of these transformed ROI-ROI correlations to give what is shown in this figure. The average distances in uncorrelated and correlated distances are still approximately the same for this data type, with obvious differences in variance.}\label{fig:null-vs-corr-rsfMRI}
%\end{figure}

%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.98\textwidth]{TiTv_distance_histograms_MAFs.pdf}
%	\caption{Histograms of simulated TiTv distance distributions for different average MAFs. The Ti/Tv ratio was fixed to be 2 in all simulations. Average MAF is computed as the expected value of the uniform distribution from which minor allele success probabilities ($f_a$) are drawn. The upper bounds for each success probability uniform distribution are $\{0.1,0.2,0.3,0.4\}$, which are the maximum possible MAF for a given locus $a$. The corresponding lower bounds were $\{0.01,0.1,0.2,0.3\}$. Sample and predicted means, as well as standard deviations, are overlaid on each histogram. Each distance distribution comes from a simulated data set with $m=100$ instances and $p=100$ features.}\label{fig:TiTv_hist}
%\end{figure}

%\begin{figure}[H]
%	\centering
%	\includegraphics[width=0.98\textwidth]{TiTv_distance_histogram_TiTvs.pdf}
%	\caption{Histograms of simulated TiTv distance distributions for different Ti/Tv ratios. Average MAF was fixed to be 0.055. The Ti/Tv ratio was taken to be 2, 1.5, 1, and 0.5. The average distance increases as the Ti/Tv ratio decreases, which is intuitive because the TiTv distance is greater for transversions than transitions. Sample and predicted means, as well as standard deviations, are overlaid on each histogram. Each distance distribution comes from a simulated data set with $m=100$ instances and $p=100$ features.}\label{fig:TiTv_hist2}
%\end{figure}

\begin{figure}[H]
	\centering
	\framebox{\includegraphics[width=0.98\textwidth]{null_corr_BryanEdit.pdf}}
	\caption{Distance densities from uncorrelated vs correlated bioinformatics data. (\textbf{A}) Euclidean distance densities for random normal data with and without correlation. Correlated data was created by multiplying random normal data by upper-triangular Cholesky factor from randomly generated correlation matrix. We created correlated data for average absolute pairwise correlation (Eq.~\ref{eq:abs_corr}) $\bar{r}_\text{abs} = 0.105, 0.263, 0.458, \text{ and } 0.612$. (\textbf{B}) TiTv distance densities for random binomial data with and without correlation. Correlated data was created by first generating correlated standard normal data using the Cholesky method from (A). Then we applied the standard normal CDF to create correlated uniformly distributed data, which was then transformed by the inverse binomial CDF with $n=2$ trials and success probabilites $f_a \text{ for all } a \in \mathcal{A}$. (\textbf{C}) Time series correlation-based distance densities for random rs-fMRI data (Fig.~\ref{fig:rs-fMRI_matrix}) with and without additional pairwise feature correlation. Correlation was added to the transformed rs-fMRI data matrix (Fig.~\ref{fig:rs-fMRI_matrix}) using the Cholesky algorithm from (A).}\label{fig:null_vs_correlated_ridge}
\end{figure}

\section{Discussion}\label{sec:discussion}

Nearest-neighbor distance-based feature selection is a class of methods that are relatively simple to implement, intuitive in nature, and perform surprisingly well in detecting interaction effects in high dimensional data. However, there has been a lack of theory on the limiting behavior of distance distributions to improve hyperparameter estimates of these feature selection methods. Furthermore, little has been done in the way of optimizing the choice of distance metric. Most often, distance-based feature selection methods use the $L_q$ metric (Eq.~\ref{eq:D}) with $q=1$ or $q=2$. However, these two realizations of the $L_q$ metric have considerably different expressions for the mean and variance of their respective limiting distributions. For instance, the expected distance for $L_1$ and $L_2$ on standard normal data is on the order of $p$ (Eq.~\ref{eq:normalManMean} and Table~\ref{tab:dist_distr_standardL1L2}) and $\sqrt{p}$ (Eq.~\ref{eq:normalEucMean} and Table~\ref{tab:dist_distr_standardL1L2}), respectively. In addition, $L_1$ and $L_2$ on standard normal data have asymptotic variances on the order of $p$ and 1, respectively (Eqs.~\ref{eq:normalManVar} and \ref{eq:normalEucVar}). These results can inform the choice of $L_1$ or $L_2$ depending context. For instance, distances become harder to distinguish from one another in high dimensions, which is one of the curses of dimensionality. In the case of $L_2$, the asymptotic distribution ($\mathcal{N}(\sqrt{2p - 1},1)$) indicates that the limiting $L_2$ distribution can be thought of simply as a positive translation of the standard normal distribution ($\mathcal{N}(0,1)$). The $L_2$ distribution ($\mathcal{N}(\sqrt{2p - 1},1)$) also indicates that most neighbors are contained in a thin shell far from the instance in high dimension ($p \gg 1$). On the other hand, the $L_1$ distances become more dispersed due to the fact that the variance of the limiting distribution is proportional to the feature dimension $p$. This $L_1$ dispersion could be more desirable when determining nearest neighbors because instances may be easier to distinguish with this metric. If using $L_1$, then it may be best to use a fixed-k algorithm instead of fixed-radius. This is because fixed-radius neighborhood order could vary quite a bit considering the $L_1$ variance is proportional to feature dimension $p$, which in turn could affect the quality of selected features. If $L_2$ is being used, then perhaps either fixed-k or fixed-radius may perform equally well because most distances will be within 1 standard deviation away from the mean. 

In any neighborhood selection algorithm, it is important to know what the average distance is and how dispersed these distances become as the feature dimension $p$ grows. In our analysis, we derived distance asymptotics for some of the most commonly used metrics in nearest-neighbor distance-based feature selection, as well as two new metrics for GWAS (Eq.~\ref{eq:D_TiTv}) and time series correlation-based data (Eqs.~\ref{eq:D_rs-fMRI} and \ref{eq:max-min_diff_rs-fMRI}) like resting-state fMRI. Using extreme value theory, we have derived limiting distributions for the sample maximum and minimum of a fixed feature $a$. This allowed us to determine the expected value and variance of the max-min normalized $L_q$ distance in standard normal (Eq.~\ref{eq:max-min_DDistr_normal}) and standard uniform (Eq.~\ref{eq:max-min_DDistr_uniform}) data, which is a new result to the best of our knowledge. Our derivations provide an important reference for individuals that are using nearest-neighbor feature selection methods in typical bioinformatics data. 

In this work, we expanded nearest-neighbor distance-based feature selection into the context of time series correlation-based data. Our motivation for this is partly based on the fact that these methods have not yet been applied to resting-state fMRI data. In order for this to be possible, we had to create a metric (Eq.~\ref{eq:diff_rs-fMRI}) that could allow us to have regions of interest (ROIs) as features. Not all ROIs will be relevant to a particular phenotype in case-control studies, so it could be important to use a nearest-neighbor feature selection method to determine which ROIs are important. This could allow us to detect interactions to help elucidate the network structure of the brain as it relates to the phenotype of interest.

The recently introduced transition-transversion metric (Eq.~\ref{eq:diff_TiTv}) provides an additional dimension to the commonly used discrete metrics in GWAS nearest-neighbor distance-based feature selection. In this work, we have provided the asymptotic mean and variance of the limiting TiTv distance distribution. This novel result, as well as asymptotic estimates for the GM (Eq.~\ref{eq:diff_GM}) and AM (Eq.~\ref{eq:diff_AM}) metrics, provides an important reference to aid in neighborhood parameter selection in this context. We have also shown how the Ti/TV ratio $\eta$ (Eq.~\ref{eq:TiTv_constraints2}) and minor allele frequency (or success probability) $f_a$ affects these discrete distances. For the GM and AM metrics, the distance is solely determined by the minor allele frequencies because the genotype encoding is not taken into account. We showed how both minor allele frequency and Ti/Tv ratio uniquely affects the TiTv distance (Figs.~\ref{fig:TiTv_ridge}A and \ref{fig:TiTv_ridge}C). Because transversions are more drastic forms of mutation than transitions, this additional dimension of information is important to consider, which is why we have provided asymptotic results for this metric.

In addition to asymptotic $L_q$ distance distributions, we have also provided the exact distributions for the one-dimensional projection of the $L_q$ distance onto individual attributes (Sections.~\ref{sec:continuous_diff}, \ref{sec:GWAS_diff}, and \ref{sec:rs-fMRI_diff}). These distributions are important for all nearest-neighbor distance-based feature selection algorithms because the $L_q$ distance is a function of the one-dimensional attribute diff. In particular, these projected distance distributions are important for understanding the predictors in NPDR, which are simply one-dimensional attribute diffs.

Correlations exist between features and instances in real data. Because of this, there can be rather drastic divergence from the asymptotic results for uncorrelated data we have derived in this work. To illustrate this behavior, we showed how strong correlations lead to positive skewness in the distance distribution of random normal, binomial, and rs-fMRI data (Figs.~\ref{fig:null_vs_correlated_ridge}A, \ref{fig:null_vs_correlated_ridge}B, and \ref{fig:null_vs_correlated_ridge}C). Pairwise correlation between features does not change the average distance, so our asymptotic results for uncorrelated data also apply when features are not independent. In contrast, the sample variance of distances diverges from the uncorrelated case substantially as the average absolute pairwise feature-feature correlation increases (Eq.~\ref{eq:abs_corr}). For fixed-radius neighborhood methods, this increases the probability of neighborhood inclusion for a fixed instance. The increased variability in distances on correlated data may provide further motivation for optimizing the choice of metric in nearest-neighbor feature selection. This most certainly motivates a discussion on optimal choices of neighborhood selection parameters, which we will address in future work.

There are many different distance metrics that can be used in place of those we have considered for bioinformatics data, but we have derived results for those that are the most commonly used in practice. Our work brings together many important aspects of nearest-neighbor distance-based feature selection, which also serves as a guide to other researchers that may be interested in a different choice of metric for a similar analysis. In future work, we will consider how pairwise feature correlation, as well as a mixture of main and interaction effects, changes the optimal choice of neighborhood selection parameters like fixed-k and fixed-radius.

% mean/variance tables

%\begin{table}[H]
%\caption{Summary of asymptotic distance distributions for common data types. Metrics with subscripts M and E represent Manhattan and Euclidean, respectively. Metrics with superscript $^*$ represent a deviation from the standard metric by attribute range normalization. The function $\Phi^{-1}(x)$ denotes the standard normal quantile function, where $x \in (0,1)$.}
%\label{tab:dist_distr_common}
%\centering
% trim=left botm right top
%\includegraphics[width=\textwidth]{typical_data-metric_tab.pdf}
%\end{table}

%\begin{table}[H]
%\caption{Summary of asymptotic distance distributions for rs-fMRI and GWAS data. Metrics with superscript $^*$ represent a deviation from the standard metric by attribute range normalization. The function $\Phi^{-1}(x)$ denotes the standard normal quantile function, where $x \in (0,1)$.}
%\label{tab:dist_distr_bio}
%\centering
% trim=left botm right top
%\includegraphics[width=\textwidth]{bioinformaticsy_tab.pdf}
%\end{table}



% AOS,AOAS: If there are supplements please fill:
%\begin{supplement}[id=suppA]
%  \sname{Supplement A}
%  \stitle{Title}
%  \slink[doi]{10.1214/00-AOASXXXXSUPP}
%  \sdatatype{.pdf}" 
%  \sdescription{Some text}
%\end{supplement}

\bibliographystyle{unsrt}
\bibliography{BoD}

\end{document}
